[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H2 Data Science Practicals",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#practical-sessions",
    "href": "index.html#practical-sessions",
    "title": "H2 Data Science Practicals",
    "section": "Practical sessions",
    "text": "Practical sessions\nIn these practicals, you will apply concepts you learn throughout the H2 Data Science course. The questions throughout are useful for learning and revision, while producing appropriate graphics and correctly describing results are essential parts of the scientific process. Expertise in these core skills is essential to do well in future courses and scientific projects. H2 Data Science is truly one of the most important courses you’ll take!\nYou will use datasets provided on Brightspace, datasets included in R, and datasets you simulate yourself.\nPlease read through the practical before the class. Each session is scheduled for 3.5h with a recommended break in the middle. You will work through a series of coding exercises and questions which are not assessed or marked, but may appear in the assessments. You should complete all the material in each practical.\nThe practical booklet is provided online and as a pdf on Brightspace. The web version has additional features that are not possible with pdfs and is the recommended interface. The underlying files (lightly edited) are available on Brightspace.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "P0_R_setup.html",
    "href": "P0_R_setup.html",
    "title": "R setup",
    "section": "",
    "text": "Getting started in R\nR is a statistical language and computing platform that is widely used in the sciences. It is free and open source. We will be using it extensively. There are many resources available, including:\nUse these resources as needed to complement, revise, and reinforce the concepts you’ll learn during this course.\nDuring the practicals, use ‘copy-and-paste’ thoughtfully. R is best learned through your fingers, and working through errors, though frustrating, is an essential skill.",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P0_R_setup.html#getting-started-in-r",
    "href": "P0_R_setup.html#getting-started-in-r",
    "title": "R setup",
    "section": "",
    "text": "Your notes and course material from H1 Maths and Data Science (also on Brightspace &gt; Practicals)\nCourses such as those from Software Carpentry or Swirl\n\nOnline videos such as IQUIT R",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P0_R_setup.html#sec-RStudio_projects",
    "href": "P0_R_setup.html#sec-RStudio_projects",
    "title": "R setup",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nWorking in a ‘Project’ within RStudio is the best way to avoid working directory complications. This is a very common source of frustration and errors. It also helps organize work to ensure that necessary scripts, documents, data, and output are all contained within the same folder. Other benefits include tracking your history of R commands and integrating cleanly with more sophisticated version control (e.g., git – more to come later).\nCreate an R Project\nWe will create an R Project for the semester. During each practical, you should work within this project. To set up a new project as we need it:\n\nSign into OneDrive, then open RStudio\nSelect File &gt; New Project…\n\nChoose New Directory &gt; New Project\n\nSet the Directory name as H2_DataScience and use the Browse button by Create project as a subdirectory of: to set a convenient location in your OneDrive folder. Click ‘Create Project’.\nIn the Files panel in RStudio, click the New Folder button and create a folder called data.\nOn Brightspace, download the file in Practicals &gt; data and move it into your newly created data folder.\nOn Brightspace, download the files in Practicals &gt; code and move them into the H2_DataScience folder.\nClose RStudio (to learn how to open appropriately)\n\n\n\n\n\n\nFigure 1: Setting up a new RStudio project.\n\n\n\n\n\n\n\nFigure 2: H2_DataScience directory structure.\n\n\nThe code in the practicals assumes this organization. If you choose to put your data files elsewhere, you will need to update the scripts accordingly. You’re now prepared and organized for a semester in R!\nOpening R\nWhen using a project, you should open R via the .proj file – not the script you plan to work on.\n\n\n\n\n\n\nI repeat: Open H2_DataScience.Rproj instead of the .R or .qmd file you plan to work on!\n\n\n\nThis is the start-up process you should use for the practicals:\n\nOpen Windows Explorer (or Finder on Mac) and find your H2_DataScience folder.\n\nDouble click on H2_DataScience.Rproj. This opens your R project with the working directory set to that folder. The working directory is shown at the top of the Console pane, and you can check it with getwd(). This is where R is ‘situated’ when loading or saving files.\nIn the Files panel, open the .qmd file for the week (or your .R file if you prefer).",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P0_R_setup.html#rstudio-settings",
    "href": "P0_R_setup.html#rstudio-settings",
    "title": "R setup",
    "section": "RStudio Settings",
    "text": "RStudio Settings\nYou can adjust many settings in RStudio via Tools &gt; Global options. In the Appearance tab in the popup box, you can set the theme (e.g., if you prefer a dark theme), font size, etc. The Code tab has many nice features as well (e.g., rainbow parentheses under Display).",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P0_R_setup.html#sec-R_packages",
    "href": "P0_R_setup.html#sec-R_packages",
    "title": "R setup",
    "section": "R packages and libraries",
    "text": "R packages and libraries\nR packages are collections of functions, custom data structures, and datasets that are developed by the user base. A new installation of R includes many useful packages, visible on the ‘Packages’ tab in RStudio. There are many additional packages available from the official CRAN repository or less officially from GitHub. If you find yourself re-using custom functions across projects, you can even create your own personal package.\nTo install a package from CRAN, use the function install.packages(\"packageName\"). This downloads the package files to your computer. Each time you open R, you will need to load that package to use it with library(packageName).\nInstalling from other package sources is slightly more complicated, so see me if you have a need.\nView an overview of a package with ?packageName, and then see a list of all of the functions by scrolling to the bottom of the help page and clicking the “index” link.\nThe help for each function is available with ?functionName, and you can see the underlying code by running functionName without parentheses.",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P0_R_setup.html#sec-quarto",
    "href": "P0_R_setup.html#sec-quarto",
    "title": "R setup",
    "section": "R scripts (.R) vs Quarto documents (.qmd)",
    "text": "R scripts (.R) vs Quarto documents (.qmd)\nIn H1, you used R scripts (.R). These are just text files. The extension tells your computer to associate them with R, and also lets you run lines of code with ‘ctrl + enter’ and other convenient things in R and RStudio.\nQuarto documents (.qmd) are also text files. However, RStudio interprets the text differently, allowing you to intersperse written prose, figures, references, R code, and output in a single document (similar to a jupyter notebook or a live script in Matlab). Code is marked as “chunks” and you can specify options for how the code and output are displayed. The text uses markdown formatting, which allows all sorts of formatting (headers, bold, italic, equations, hyperlinks, tables, images…). RStudio can render a .qmd file into many other formats (e.g., pdf, docx, html, epub…).\nCode chunks can be added with the green button with a ‘(+)c’ on the top right of a .qmd document in RStudio. Code chunks look like this:\n\n```{r}\n# This is a code chunk\na &lt;- 1:3\na\n```\n\n[1] 1 2 3\n\n\nRun the full block with the green ‘play’ button at the top right of the block. The output from the code is shown just below the block.\nFor code-heavy work, Quarto documents are a handy way to produce nicely formatted output without the hassle of copying and pasting code, output, and figures into, e.g., a word document. There are many guides and tutorials online.\nThis manual is written as a Quarto book. Versions of the .qmd files for each practical are available on Brightspace to make things easier for you. You should now have these downloaded and saved in your project directory.\nNote that the project will be submitted as a .qmd file with a version rendered to html.",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P0_R_setup.html#writing-r-code",
    "href": "P0_R_setup.html#writing-r-code",
    "title": "R setup",
    "section": "Writing R code",
    "text": "Writing R code\nR has established best practices to make your meaning clear. Just like any language, you|can|write|with|your|own|system, but it’s easier for everyone to use standard conventions. See the full style guide for more.\nA few key points:\n\nUse &lt;- to assign a value to an object. You may see =, which works, but is not preferred.\nUse # to write a comment which R will ignore.\nUse spaces to make your code legible: a &lt;- c(1, 2, 3).\nAvoid spaces in column names or file names as these are a pain to work with.\nUse names for objects that are short, but descriptive.\nLimit the length of a line of code to about 80 characters.\nUsually, variables should be nouns and functions should be verbs.\nRun the line of code where your cursor is (or everything you’ve selected) with ctrl + enter",
    "crumbs": [
      "R setup"
    ]
  },
  {
    "objectID": "P1_R_recap.html",
    "href": "P1_R_recap.html",
    "title": "1  R recap",
    "section": "",
    "text": "1.1 Basic data exploration\nStatistics can be divided into two broad categories: descriptive statistics, which describe or summarise data, and inferential statistics, which allow us to infer something about a population from a sample.\nThis session focuses on how to appropriately display and summarise data (i.e., descriptive stats). You should already be aware of several techniques for displaying data (e.g., bar charts, histograms, scatter plots). When and how to use these techniques is one focus of today.\nThere are two purposes for visualizing data.\nFirst, the best way to get a ‘gut’ feel for your dataset is to look at it graphically. Examining data graphically enables you to identify any outliers (suspicious observations which could be errors). It will also help you to select the most appropriate inferential statistical model (more on this through the course).\nSecond, visualizations are used to impart information as clearly as possible to ‘the reader’, drawing attention to the most interesting aspects of your data. Graphics that are confusing, either through a lack of detail (e.g. no labels) or that contain too much information will fail in this central objective.\nAs you create graphics, keep in mind that they may be viewed on different machines, in grey scale, or by colour-blind or visually impaired readers. Colour scales such as those available from ColorBrewer or viridis are designed with this in mind.\nIt’s best practice to load necessary packages at the top of your document. Today we’ll use the tidyverse package, which is actually a collection of packages. First, you’ll need to install it with install.packages(\"tidyverse\"). Installation needs to be done once per machine, but loading is needed each time you re-open R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R recap</span>"
    ]
  },
  {
    "objectID": "P1_R_recap.html#basic-data-exploration",
    "href": "P1_R_recap.html#basic-data-exploration",
    "title": "1  R recap",
    "section": "",
    "text": "1.1.1 Object structure\nWe will mostly work with dataframes. A data.frame is a 2D rectangular object with columns and rows. In a tidy dataset, each row represents an ‘observation’ and each column represents a ‘variable’. R (and often packages) contains several built-in dataframes.\nThe data.frame cars gives the max speed and stopping distance for cars built in the early 20th century. It is already available in your R session. We will use cars to demonstrate a few basic programming and statistical concepts.\n\n# functions for basic details of objects\nstr(cars) # structure overview\nclass(cars) # object class\nnames(cars) # column names\nhead(cars) # first few rows\n\n\nhead(cars, 2)\n\n  speed dist\n1     4    2\n2     4   10\n\ntail(cars, 2)\n\n   speed dist\n49    24  120\n50    25   85\n\n# what are the last 10 rows?\n\n\n1.1.2 Subsetting, renaming, and rearranging\nThere are several ways to access subsets of a data.frame:\n\nUse data_df$columnName or data_df[[\"columnName\"]] to extract a single column\nUse data_df[rows,columns] to extract a block\n\n\ncars$speed # whole column\ncars[[\"speed\"]] # whole column\n\n\ncars[1, 1] # row 1, column 1\n\n[1] 4\n\ncars[1:5, 1] # rows 1-5, column 1\n\n[1] 4 4 7 7 8\n\ncars[1:3, ] # leaving the 'columns' space blank returns all columns\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n\n\nWe can also change column names. For illustration, let’s make a copy of the data.frame to do that.\n\ncars2 &lt;- cars \nnames(cars2)\n\n[1] \"speed\" \"dist\" \n\nnames(cars2)[1] &lt;- \"speed_mph\" # change first column name\nnames(cars2)\n\n[1] \"speed_mph\" \"dist\"     \n\nnames(cars2) &lt;- c(\"speed_mph\", \"dist_ft\") # change both column names\n\nRearranging and duplicating columns is also easy.\n\nhead(cars2, 2)  \n\n  speed_mph dist_ft\n1         4       2\n2         4      10\n\nhead(cars2[, 2:1], 2) # rearrange columns \n\n  dist_ft speed_mph\n1       2         4\n2      10         4\n\ncars3 &lt;- cars2[, c(2, 1, 1)] # duplicate a column\nhead(cars3, 2)\n\n  dist_ft speed_mph speed_mph.1\n1       2         4           4\n2      10         4           4\n\ncars3 &lt;- cars3[, 1:2] # remove the duplicated column\nhead(cars3, 2)\n\n  dist_ft speed_mph\n1       2         4\n2      10         4\n\ncars3$dist_x_speed &lt;- cars3$dist_ft * cars3$speed_mph # create a new column\nhead(cars3, 2)\n\n  dist_ft speed_mph dist_x_speed\n1       2         4            8\n2      10         4           40\n\nrm(cars3) # remove the dataframe 'cars3' from your R environment\n\n\n\n\n\n\n\nRecall that you must assign the results of an operation (&lt;-) to save it. For example, running cars2[, 2:1] will display the results, but cars2 &lt;- cars2[, 2:1] will overwrite cars2 in your R environment.\n\n\n\nYou can also subset based on criteria. Say we only want rows where the speed is \\(&gt;\\) 20 mph:\n\ncars_fast &lt;- cars2[cars2$speed_mph &gt; 20, ]\nclass(cars_fast) \n\n[1] \"data.frame\"\n\nncol(cars_fast)  # and how many *rows* are there?\n\n[1] 2\n\nhead(cars_fast, 2)\n\n   speed_mph dist_ft\n44        22      66\n45        23      54\n\n\n\n1.1.3 NAs and summary\nWhen you import data, you should check for missing values. These are represented as NA.\nWe can check each element of a vector using is.na(), which will return TRUE if an element is NA, and FALSE if an element is not NA.\n\nis.na(cars2$speed_mph)\n\nR converts logical values (i.e., TRUE/FALSE) to numeric (i.e., 1/0) automatically. This is handy, but can be dangerous if you don’t realize it.\n\nsum(is.na(cars2$speed_mph)) # how many are NA?\n\n[1] 0\n\ncarsNA &lt;- cars2\ncarsNA[c(2, 4, 5, 10), 1] &lt;- NA\nsum(is.na(carsNA$dist_ft))\n\n[1] 0\n\n\nAnother very useful check is summary():\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\nsummary(carsNA)\n\nOnce you are confident that your data.frame looks sensible, that it contains the data you expect, and that you know what the data-types are, you can start to explore and summarise your data.\nThere are many graphical methods for data exploration. The appropriate method depends on the nature of the data and what you wish to communicate to the reader.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R recap</span>"
    ]
  },
  {
    "objectID": "P1_R_recap.html#graphical-methods-for-displaying-data",
    "href": "P1_R_recap.html#graphical-methods-for-displaying-data",
    "title": "1  R recap",
    "section": "\n1.2 Graphical methods for displaying data",
    "text": "1.2 Graphical methods for displaying data\nAlways keep in mind that the primary reason for data visualization is to impart information concisely and accurately to your reader.\nGraphics must be clear, concise and easy to understand. Brightspace contains some examples of bad graphics (‘Learning resources&gt;Lecture support material&gt;Introduction (Lectures 1-3)&gt;Graphics’).\n\n\n\n\n\nFigure 1.1: An example of a terrible graphic, as published in a Scottish government report.\n\n\nIn addition to poor design choices for effective communication (Figure 1.1), graphics can also be deliberately misleading (Figure 1.2).\n\n\n\n\n\nFigure 1.2: A misleading graphic. What type of plot is this and how is it misleading?\n\n\n\n1.2.1 Scatter plots\nThe scatter plot is used to plot two continuous variables against each other. It is commonly used for analyses like correlation or linear regression. The plot() function in R will create a scatter plot if given two numeric variables. There are two options for specifying the variables.\nUsing data_df &lt;- data.frame(x=1:3, y=4:6), the same plot can be created with either syntax:\n\nplot(data_df$x, data_df$y) # two vectors: x, y\nplot(y ~ x, data=data_df) # columns in dataframe: formula y ~ x\n\nThere are many options for modifying the output of plot().\n\npar(mfrow=c(1,3)) # set the plot window to show 1 row, 3 columns\n\n# plot(response ~ predictor, data=dataframe)\nplot(dist_ft ~ speed_mph,\n  data=cars2, xlab=\"Speed (mph)\", ylab=\"Distance (ft)\",\n  main=\"Default symbol\")\nplot(dist_ft ~ speed_mph,\n  data=cars2, xlab=\"Speed (mph)\", ylab=\"Distance (ft)\",\n  pch=2, main=\"Setting 'pch=2'\")\n# you can check out more symbols and their respective numbers using this plot:\nplot(1:20, pch=1:20, main=\"'pch' symbols 1 to 20\")\n\n\n\n\n\n\nFigure 1.3: Symbol options.\n\n\n\n\n\npar(mfrow=c(1,1)) # reset to a single panel\n\n\nQ. 1.1With the cars dataset, plot stopping distance by speed for only those cars with a speed less than or equal to 15 mph.\n\n\n\nQ. 1.2Use the plot help page to add an appropriate title to your plot.\n\n\n\nQ. 1.3\n?points opens the help page for points. Search it for ‘pch’ and change the symbol in your plot.\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nplot(dist ~ speed, data=cars[cars$speed &lt;= 15, ], pch=2,\n     xlab=\"Speed (mph)\", ylab=\"Distance (ft)\", \n     main=\"Cars with max speed &lt;= 15 mph\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: Stopping distance by speed.\n\n\n\n\n\n1.2.2 Boxplots\nBox plots are used to summarise a continuous variable by levels of a factor. We will use the mtcars dataset to illustrate this. You can learn about these data with ?mtcars.\nExplore the data.frame using the strategies covered above. Which variables are categorical? Which are continuous?\n\nhead(mtcars, 2)\n\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\n\n\n\n\n\n\n\n\nFigure 1.5: Boxplot showing miles per litre vs. number of carburetors\n\n\n\n\n\n?boxplot\n# See examples at bottom of the help page\n\n\nQ. 1.4Reproduce the plot shown in Figure 1.5 (assume 1 gallon = 4.5 L). You will need to generate a new variable (miles per litre) and label your box plot appropriately. You can limit the extent of the y-axis by adding the argument ylim=c(a, b) where a and b are the limits you want (e.g., ylim=c(0, 100)).\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nmtcars2 &lt;- mtcars\nmtcars2$mpl &lt;- mtcars2$mpg / 4.5\n\nboxplot(mpl ~ cyl, data=mtcars2, xlab=\"Cylinders\", ylab=\"Miles per litre\")\n\n\n\n\n\nQ. 1.5Use ?boxplot to investigate what the box and whiskers actually represent.\n\n\nNote that box plots are not the most visually intuitive. Packages like ggplot2 (and extensions) make alternatives like those in Figure 1.6 simple to produce. We will cover some of these later.\n\n\n\n\n\n\n\nFigure 1.6: Alternatives to boxplots\n\n\n\n\n\n1.2.3 Line plots\nLine plots are most often seen in timeseries plots with time on the x-axis and the response on the y-axis. Line plots involve joining points with a line, which indicates that you have made assumptions about the value of the response variable between successive measurements.\nWe will examine these plots using the dataset lynx, which consists of the number of Canadian lynx pelts sold per year between 1821 - 1934. It is a ‘classic’ dataset as it shows a cyclical ‘boom-and-bust’ lynx population (demonstrating predator-prey interactions).\nFirst, we will create a variable Year.\n\nstr(lynx)\n\n Time-Series [1:114] from 1821 to 1934: 269 321 585 871 1475 ...\n\n\n\nstr(lynx)\nlynx2 &lt;- as.data.frame(lynx) \nstr(lynx2) \nlynx2$Year &lt;- seq(from=1821, to=1934, by=1)\nlynx2$Trappings &lt;- as.numeric(lynx2$x) # Time-Series is complicated.\nstr(lynx2)\nhead(lynx2, 2) \n\nIn R, we use functions to perform actions on objects. Functions have arguments, taking the form functionName(arg1=..., arg2=...). If you do not name the arguments, the function will assume that you are listing the arguments in order. See the help file for a function with ? to see the argument order (e.g., ?seq).\n\nQ. 1.6Using seq(), write a piece of code which generates odd numbers between 1 and 20. Try with and without naming the arguments.\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nseq(from=1, to=20, by=2)\nseq(1, 20, 2)\n\n\n\n\nUse ?plot to investigate options for plotting. Find the type= argument for plotting both the points and a connecting line. Why might this be the best option here?\n\nQ. 1.7Using plot(), produce a line plot similar to Figure 1.7.\n\n\n\n\n\n\n\n\n\nFigure 1.7: The number of lynx trapped in Canada (1820-1934)\n\n\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nplot(Trappings ~ Year, data=lynx2, pch=19,\n     main=\"Lynx trapping in Canada (1820-1934)\", type=\"b\")\n\n\n\n\n\nQ. 1.8Create a plot that shows the log number of trappings from 1850 to 1900.\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nplot(log(Trappings) ~ Year, \n     data=lynx2[lynx2$Year &gt;= 1850 & lynx2$Year &lt;= 1900, ], \n     pch=19,\n     main=\"Lynx trapping in Canada (1820-1934)\", type=\"b\")\n\n\n\n\n\n1.2.4 Histograms\nHistograms illustrate the distribution of continuous data. In histograms the bars are adjacent (no gap). This indicates that the underlying values are continuous rather than discrete.\n\nhist(lynx2$Trappings, main=\"Lynx trapping\", xlab=\"Trapped lynx per year\")\n\n\n\n\n\n\nFigure 1.8: Lynx pelts per year with default settings.\n\n\n\n\n\nQ. 1.9What conclusions do you draw from this plot? Which range of values was most common across years?\n\n\nBe aware that histograms can be quite sensitive to the number of bins, and you should explore different options. You can set the number or values of break points with breaks=....\n\npar(mfrow=c(1,2)) # panels for the plotting window\n# R takes the number of breaks as a suggestion\nhist(lynx2$Trappings, xlab=\"Trapped lynx per year\",\n     breaks=5)\n# this forces R to plot according to the defined breaks\nhist(lynx2$Trappings, xlab=\"Trapped lynx per year\",\n     breaks=c(0, 500, 1000, 2000, 5000, 10000))\n\n\n\n\n\n\nFigure 1.9: Lynx pelts per year with breaks=5 (left) and a vector of breaks (right).\n\n\n\n\n\nCodepar(mfrow=c(2, 2)) # plot panels (2 rows x 2 columns)\npar(mar=rep(2, 4)) # change the plot margins\nhist(lynx2$Trappings, main=\"bin width: 100\", xlab=\"Trapped lynx per year\", \n     breaks=seq(0, 10000, by=100))\nhist(lynx2$Trappings, main=\"bin width: 500\", xlab=\"Trapped lynx per year\", \n     breaks=seq(0, 10000, by=500))\nhist(lynx2$Trappings, main=\"bin width: 1000\", xlab=\"Trapped lynx per year\", \n     breaks=seq(0, 10000, by=1000))\nhist(lynx2$Trappings, main=\"bin width: 2000\", xlab=\"Trapped lynx per year\", \n     breaks=seq(0, 10000, by=2000))\n\n\n\n\n\n\nFigure 1.10: Histograms of lynx pelts per year with different breaks\n\n\n\n\n\npar(mfrow=c(1, 1)) # reset the par setting.\n\nWhich of these plots is the most useful? There is no definitive answer, but the first is very busy and the last fails to show relevant detail near 0. Bin widths of 500-1000 communicate the patterns most clearly.\nGenerally, 5-15 breaks usually work well.\n\n1.2.5 Bar graphs\nBar graphs are used to plot counts of categorical or discrete variables. We’ll be using the islands dataset which is a named vector of island areas.\n\n\n\n\n\n\nMany objects in R can have row names. However, converting between data types may lose this information. Consequently, it is better practice to store relevant information in a column. Nevertheless, there are occasions where this is useful and you may come across datasets with data stored as row names.\n\n\n\nWorking with data involves a lot of time spent tidying the datasets: cleaning, checking, and reshaping into useful formats. We will cover a more modern set of methods for this later in the course using the tidyverse package. For now, we’ll stay with base R. First, we need to tidy the islands data.\n\nstr(islands) \nclass(islands) # this is a named numeric vector\nhead(islands)\n\n# convert to a dataframe\nislands_df &lt;- as.data.frame(islands) \nhead(islands_df, 2)\nstr(islands_df) # rownames are not shown!\n\n\n# put the row names into a new column\nislands_df$LandMass &lt;- row.names(islands_df) \nhead(islands_df, 2)\n\n           islands   LandMass\nAfrica       11506     Africa\nAntarctica    5500 Antarctica\n\n# set row names to the row number\nrow.names(islands_df) &lt;- 1:nrow(islands_df) \nnames(islands_df)[1] &lt;- \"Area\" \nhead(islands_df, 2) \n\n   Area   LandMass\n1 11506     Africa\n2  5500 Antarctica\n\n# reorder by area\nislands_df &lt;- islands_df[order(islands_df$Area, decreasing=TRUE), ]\nhead(islands_df, 3)\n\n    Area      LandMass\n3  16988          Asia\n1  11506        Africa\n35  9390 North America\n\n\nWe can use the function barplot() to plot the vector of island areas.\n\npar(mar=c(4, 0, 0, 0)) # change the margin sizes\nbarplot(islands_df$Area)\n\n\n\n\n\n\nFigure 1.11: Island areas with barplot defaults\n\n\n\n\nThe whole dataset includes a lot of very small areas, so let’s cut it down to just the 10 largest. Since the dataset is already sorted, we can take rows 1:10.\n\nbarplot(islands_df$Area[1:10])\n\n\n\n\n\n\nFigure 1.12: Top 10 island areas\n\n\n\n\nAnd the next step is to add some names to the x-axis…\n\nbarplot(islands_df$Area[1:10], names=islands_df$LandMass[1:10])\n\n\n\n\n\n\nFigure 1.13: Top 10 island areas with names\n\n\n\n\nWhich of course are unreadable. There are many options here (e.g., see las in ?par) but we will rotate the plot. To do this, we need to re-adjust the margins, set horiz=TRUE and las=1, and use [10:1] so the largest is on top.\n\npar(mar=c(4, 10, 0, 0))\nbarplot(islands_df$Area[10:1], names=islands_df$LandMass[10:1], \n        horiz=TRUE, las=1, xlab=\"Area (km2)\")\n\n\n\n\n\n\nFigure 1.14: Finally! Did you know Antarctica is bigger than Europe?\n\n\n\n\nData visualization is an iterative process with lots of trial and error to find a plot that communicates the message within the data well. There are several packages (e.g., ggplot2) that make these sort of adjustments and explorations less opaque than all of the options in par().\n\n\n\n\n\n\nWe will cover ggplot2 and other the tidyverse packages in more detail soon. You are welcome to use whichever system you prefer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R recap</span>"
    ]
  },
  {
    "objectID": "P1_R_recap.html#summary-statistics",
    "href": "P1_R_recap.html#summary-statistics",
    "title": "1  R recap",
    "section": "\n1.3 Summary statistics",
    "text": "1.3 Summary statistics\nYou will often need to summarise your data before you present it. Data summaries are usually contained in tables and they can sometimes replace graphics (e.g., where the data is relatively simple or where individual precise values are important). There are many types of summary statistics. Here we are concerned with central tendency and variability.\n\nQ. 1.10What are the three main measures of central tendency?\n\n\n\nQ. 1.11What are three measures of variability?\n\n\nThe most appropriate metrics of central tendency or variability will depend on your data. Another summary statistic that you might include is sample size. R is very good at producing summary statistics, and there are myriad ways to produce them. We’ll return to the cars2 dataset.\n\nsummary(cars2) \n\n   speed_mph       dist_ft      \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\nsummary(cars2[cars2$speed_mph &gt; 20, ]) \n\n\n# Recall the options to access a column in a dataframe\nsummary(cars2$speed_mph) \nsummary(cars2[, 1])\nsummary(cars2[, \"speed_mph\"])\n\nOften you’ll wish to summarise your data across levels of a certain factor, such as levels of a certain treatment. More complex summaries can be made using the dplyr package. We’ll go into more detail later on some of the very powerful ways this package (and others in the tidyverse) can be used.\nWe’ll use the built-in dataset InsectSprays. Viewing your raw data can be an important check as well. You can open a spreadsheet-style viewer in R using View(YourDataFrame).\n\nstr(InsectSprays)\n\n'data.frame':   72 obs. of  2 variables:\n $ count: num  10 7 20 14 14 12 10 23 17 20 ...\n $ spray: Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(InsectSprays) # glimpse() is loaded with tidyverse\n\nRows: 72\nColumns: 2\n$ count &lt;dbl&gt; 10, 7, 20, 14, 14, 12, 10, 23, 17, 20, 14, 13, 11, 17, 21, 11, 1…\n$ spray &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B, B, B…\n\n\n\n# spray is the categorical predictor; count is the response\nView(InsectSprays)\n\nTo do more complex summaries, we will string together a series of functions. This can be done in a nested format (e.g., fun3(fun2(fun1(dataset)))), but this gets unwieldy very quickly.\nSo, let’s use the pipe operator |&gt;. This takes the output from one function and feeds it as the first input of the next (e.g., dataset |&gt; fun1() |&gt; fun2() |&gt; fun3()), making code much more legible. Many functions in the tidyverse are built for piping.\n\n?`|&gt;`\n\n\n# use group_by() with the grouping column name(s)\nspray_summaries &lt;- InsectSprays |&gt;\n  group_by(spray) |&gt;\n  summarise(count_mean=mean(count))\nspray_summaries\n\n\n# it is very easy to calculate any number of summary statistics\nInsectSprays |&gt;\n  group_by(spray) |&gt;\n  summarise(mean=mean(count) |&gt; signif(3),\n            median=median(count),\n            max=max(count),\n            sd=sd(count) |&gt; signif(3),\n            N=n(),\n            N_over_10=sum(count &gt; 10),\n            Pr_over_5=mean(count &gt; 5))\n\n# A tibble: 6 × 8\n  spray  mean median   max    sd     N N_over_10 Pr_over_5\n  &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 A     14.5    14      23  4.72    12         9    1     \n2 B     15.3    16.5    21  4.27    12        11    1     \n3 C      2.08    1.5     7  1.98    12         0    0.0833\n4 D      4.92    5      12  2.5     12         1    0.167 \n5 E      3.5     3       6  1.73    12         0    0.167 \n6 F     16.7    15      26  6.21    12        10    1     \n\n\n\n1.3.1 Choosing a central tendency metric\nThe choice of central tendency metric depends on the nature of the data and objectives of your research. We will use datasets that you downloaded from Brightspace (Practicals &gt; data). Remember to put these into the data folder in your working directory (or modify the file paths in the code accordingly).\n\n# this will load the 'Scallop %fat' data sheet from the xlsx spreadsheet.\nscallop_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet=\"Scallop %fat\")\nstr(scallop_df)\n\ntibble [49 × 1] (S3: tbl_df/tbl/data.frame)\n $ Scallop % fat: num [1:49] 22.5 24.1 18.2 32.5 17.4 23.6 21.5 22.2 27.6 22.2 ...\n\n# avoid spaces and symbols in column names. It's a pain.\nnames(scallop_df) &lt;- \"fat_pct\"\n\n\nQ. 1.12Check the data using the methods above. Does it look OK to you?\n\n\n\nQ. 1.13Are these data likely to be continuous or discontinuous?\n\n\n\nQ. 1.14Create a plot to visualize the distribution of these data.\n\n\n\nQ. 1.15Do you spot any issues?\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nhist(scallop_df$fat_pct, main=NULL) # (what does 'main=NULL' do?)\n\n\n\n\n\n\nFigure 1.15: Histogram of fat percentage.\n\n\n\n\n\n\n\nYou should have spotted a potential outlier. Data entry errors are common, and a check against the original data sheet shows that the decimal was typed in the wrong place. The following code helps you locate the error.\n\nwhich(scallop_df$fat_pct &gt; 50) # which() returns the indexes\n\n[1] 36\n\nscallop_df$fat_pct[35:37] # row 36 is 99.0, but should be 9.90\n\n[1] 22.8 99.0 12.9\n\nscallop_df &lt;- scallop_df[, c(1, 1)] # duplicate column\nnames(scallop_df) &lt;- c(\"fat_pct_orig\", \"fat_pct_corr\")\nhead(scallop_df, 2)\n\n# A tibble: 2 × 2\n  fat_pct_orig fat_pct_corr\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         22.5         22.5\n2         24.1         24.1\n\n\n\n# there are many ways to 'fix' the outlier in R.\n# You need to correct the outlier in row 36 of column 'fat_pct_corr'\nscallop_df$fat_pct_corr[36] &lt;- 9.9\nwhich(scallop_df$fat_pct_corr &gt; 90) \n# integer(0) - this means that no elements in fat_pct_corr contain values &gt;90\n\nNow summarise scallop_df using some of the methods above.\n\nQ. 1.16Create a histogram for the corrected column. How does it differ from the original column with the error?\n\n\n\nQ. 1.17Calculate mean, variance, median, interquartile range, minimum, maximum and range for both fat_pct_orig and fat_pct_corr.\n\n\n\nQ. 1.18Suppose the outlier was even bigger (i.e. your typo was even worse). Adjust your data, multiplying the erroneous data item by 10; copy the fat_pct_orig column and change row 36 to 999.\n\n\n\nQ. 1.19Calculate the same summary statistics.\n\n\n\nQ. 1.20Which measures of central tendency and variability are most ‘robust’ against this outlier?\n\n\nOr look individually instead of calculating many metrics at once with dplyr functions:\n\nsummary(scallop_df$fat_pct_corr)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   8.50   16.90   20.60   19.56   22.50   32.50 \n\nvar(scallop_df$fat_pct_corr)\n\n[1] 22.79836\n\nIQR(scallop_df$fat_pct_corr)\n\n[1] 5.6\n\n\nR is excellent at generating well formatted tables such as shown in Table 1.1. What is missing from from this table?\n\n\n\nTable 1.1: Summary statistics with and without an outlier. Note which summary stats are most influenced by the outlier.\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nMean\nMedian\nStandard deviation\nRange\nInterquartile range\n\n\n\nfat_pct_corr\n19.6\n20.6\n4.77\n24.0\n5.6\n\n\nfat_pct_orig\n21.4\n20.6\n12.20\n90.5\n5.3\n\n\n\n\n\n\n\n\n\nQ. 1.21How would the patterns seen in Table 1.1 influence your choice if you were required to summarise data that you thought might contain values that could be erroneous? Consider how each metric is influenced by the data distribution and by outliers.\n\n\nLet’s load a dataset that gives the length of hake across three years of sampling.\n\nhake_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet=\"Hake\")\nstr(hake_df) # once again, column names made for excel rather than R\n\ntibble [499 × 2] (S3: tbl_df/tbl/data.frame)\n $ Year            : num [1:499] 1 1 1 1 1 1 1 1 1 1 ...\n $ Hake length (mm): num [1:499] 190 219 181 148 206 204 168 197 178 211 ...\n\n\n\nQ. 1.22What type of variable is length?\n\n\n\nQ. 1.23Select an appropriate graphical method and display these data.\n\n\n\nQ. 1.24In your own time, use the dplyr functions to summarise the hake data by year.\n\n\n\nhake_df$Year &lt;- as.factor(hake_df$Year) # Treat as categorical, not numeric\nnames(hake_df) &lt;- c(\"Year\", \"Length\") # simplify the column names\n\n\n\n\nTable 1.2: Summary of hake data.\n\n\n\n\nYear\nMean length (cm)\n\n\n\n1\n201.8\n\n\n2\n497.0\n\n\n3\n988.9\n\n\n\n\n\n\n\n\nThe following ‘settling velocity’ data relates to the settling velocity of salmon faecal material. Shona Magill generated these data.\n\nfishPoo_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet=\"Settling velocity\")\nstr(fishPoo_df)\n\ntibble [200 × 1] (S3: tbl_df/tbl/data.frame)\n $ Settling velocity (mm s-1): num [1:200] 2.06 1.03 1.56 1.88 1.16 0.76 1.26 1.13 1.23 1.31 ...\n\n\n\nQ. 1.25Produce a histogram of the settling velocity. Is it left or right skewed?\n\n\n\nQ. 1.26Which measures of central tendency and variability are most appropriate?\n\n\n\nQ. 1.27Sketch the distribution and indicate the relative positions of the mean and median.\n\n\n\nQ. 1.28Generate a new column of the log-transformed settling velocity data and plot these data.\n\n\n\nQ. 1.29What measures of central tendency and variability could be applied to the log-transformed data? Selecting the preferable metrics for a dataset is not necessarily straightforward.\n\n\nTable 1.3 gives some indication of what issues you might consider.\n\n\n\nTable 1.3: Appropriate measures of central tendency and variability according to the underlying data distribution.\n\n\n\n\n\n\n\n\n\nData distribution\nCentral tendency metric\nVariability metric\n\n\n\nContinuous, unimodal, symmetric\nMean\nVariance or sd\n\n\nContinuous, skewed\nMedian\nInterquartile range\n\n\nContinuous, multimodal\nNone; state modes\nNone; summarise by group\n\n\nDiscontinuous\nNone; data-dependent\nRange?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R recap</span>"
    ]
  },
  {
    "objectID": "P1_R_recap.html#conclusions",
    "href": "P1_R_recap.html#conclusions",
    "title": "1  R recap",
    "section": "\n1.4 Conclusions",
    "text": "1.4 Conclusions\nVisualizing and summarising data are the critical first steps in the data analysis and reporting workflow. We use graphical methods to firstly explore our own data. Once we have made sense of it we select the most appropriate method to convey that understanding to our readers. We may help that communication by summarising data in the most appropriate way taking into account the distribution of the data and the presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R recap</span>"
    ]
  },
  {
    "objectID": "P2_binomial_poisson.html",
    "href": "P2_binomial_poisson.html",
    "title": "2  Binomial and Poisson Distributions",
    "section": "",
    "text": "2.1 The Binomial distribution\nWe often wish to determine the probability of events occurring given our current understanding of the processes that are involved. This requires a mathematical description of a theoretical relationship. We refer to this as a statistical model. Models are simplified representations of reality. As George Box said, ‘All models are wrong but some are useful’.\nTwo models, the binomial and Poisson distributions, often provide excellent approximations of real-world events. This means that they can be used to determine the likelihood of events or series of events given certain ‘reasonable’ assumptions. In terms of planning, e.g. in the insurance industry, this is extremely useful.\nFor example, the binomial or Poisson distributions can be used to answer:\nThis practical gives you the opportunity to practice using these distributions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "P2_binomial_poisson.html#the-binomial-distribution",
    "href": "P2_binomial_poisson.html#the-binomial-distribution",
    "title": "2  Binomial and Poisson Distributions",
    "section": "",
    "text": "2.1.1 Bernoulli trials\nA Bernoulli trial is a single event with a binary outcome (i.e., two categories that are mutually exclusive). Binary outcomes include:\n\nAlive vs. dead\nReproductive vs. not reproductive\nPresent vs. absent\n\nOther variables can be re-coded into a binary outcome. For example:\n\nFlower colour (blue vs. not blue)\nIncome (\\(\\geq\\) £100,000 vs. \\(&lt;\\) £100,000)\n\nBy definition, each Bernoulli trial is independent of all previous trials.\n\nQ. 2.1A fair coin has been heads in 99 consecutive flips. What is the probability of heads on the next flip?\n\n\nThis is different to asking whether (100 heads) or (99 heads + 1 tails) is more likely in a throw of 100 coins. This is different because each flip is independent.\n\n2.1.2 The binomial distribution\nThe binomial distribution is a discrete probability distribution that applies to a series of Bernoulli trials.\nFor example, if a chicken laid 4 eggs and they were all female, you might wonder how likely this was by chance. Here, the outcome can be female or male, and the number of trials is 4: each egg is a ‘trial’ or ‘event’ with a binary outcome. You may question the assumption that the ratio of female:male was 50:50 and favour an alternative hypothesis that females are more likely. The binomial distribution allows you to quantify the probability of getting \\(x\\) females from \\(n\\) eggs for any given probability \\(p=P(female)\\). That is, we are not restricted to 50:50.\nYou need to know two things to use the binomial distribution. These are:\n\nThe number of trials (\\(n\\), or sometimes \\(k\\))\nThe probability of success (\\(p\\))\n\nFrom p, we can calculate the probability of failure as \\(q = (1-p)\\), since the two probabilities must sum to 1.\nThe distribution of a binomially distributed variable \\(y\\) is specified as \\(y \\sim Binom(n,p)\\). We denote \\(P(x)\\) as the probability of getting \\(x\\) successes where \\(x\\) is an integer from \\(0\\) to \\(n\\).\nThe mean of a binomial distribution is \\(n*p\\). This gives you the expected outcome. For example, if \\(P(female)=0.5\\) and \\(n=4\\) eggs the expected number of females is \\(4*0.5=2\\).\n\n2.1.3 Binomial distributions by hand\nWongles always lay two eggs in a clutch but 50% of the eggs are infertile and don’t hatch. We are interested in the proportion that we expect to hatch from one clutch (two eggs).\n\nQ. 2.2What is the event?\n\n\n\nQ. 2.3What is \\(p\\)?\n\n\n\nQ. 2.4What is the number of trials?\n\n\n\nQ. 2.5How many possible outcomes are there for a clutch? What are they?\n\n\n\nQ. 2.6Write down the model specification (with parameters).\n\n\n\nQ. 2.7Calculate the expected proportions of clutches that contain (a) two fertile, (b) two infertile, and (c) one of each. Use a probability tree if needed.\n\n\n\nQ. 2.8Using the binomial probability mass function from Chapter 7, calculate the expected proportions for two eggs.\n\n\nUnlike Wongles, Oozles always have broods of eight offspring and all of them hatch. We are interested in modelling the probabilities of the number of male and female offspring in these broods of eight eggs.\n\nQ. 2.9What is the Bernoulli event?\n\n\n\nQ. 2.10What are the theoretical limits to your outcomes (i.e, max numbers of each)?\n\n\n\nQ. 2.11What are the possible outcomes? What is the number of possible outcomes?\n\n\n\nQ. 2.12Write down the model for Oozle egg sex: \\(y \\sim Binom(n,p)\\).\n\n\nWe will assume that the probability \\(p\\) of any offspring being female is 0.5 and being male \\(q\\) is 0.5. For the extreme cases where all offspring are one sex, we can use simple probability theory: the probability of getting \\(n\\) females in a brood of size \\(n\\) is equal to \\(p^n\\).\n\nQ. 2.13Calculate the probability of obtaining eight male offspring.\n\n\n\nQ. 2.14What is the mean number of females you would expect in Oozle broods?\n\n\nIt gets more complicated when you want to know the probability of getting, say, 1 male and 7 females from your clutch of eight eggs.\n\nQ. 2.15Given that \\(p=q\\), what shape would expect the distribution to be?\n\n\n\nQ. 2.16Use the binomial expression to calculate the probability of obtaining 0, 1, 2, 3, 4, 5, 6, 7 and 8 male offspring (note that the distribution is symmetric).\n\n\nIt is much easier, of course, to do this using R.\n\n2.1.4 Binomial distributions in R\nR can calculate probabilities for specific outcomes from a massive array of theoretical probability distributions. The binomial is just one of them.\n\nnum_female &lt;- 4 # note that 4 is assigned to the variable called num_female\nnum_trials &lt;- 8\np_female &lt;- 0.5\n\n\n# for a single probability: y~Binom(n=8, p=0.5) determine P(y_i=4)\ndbinom(num_female, num_trials, p_female) # dbinom(4, 8, 0.5)\n\n\n# formatted output just because:\npaste0(\"P(\", num_female, \" female | \", num_trials, \" eggs) = \",\n       dbinom(num_female, num_trials, p_female))\n\n[1] \"P(4 female | 8 eggs) = 0.2734375\"\n\n\nThe function dbinom() gives the probability of a single outcome.\nOften we want to know cumulative probabilities instead. This allows us to answer questions like “What is the probability of obtaining &lt; 4 females in a brood of 8 eggs?” Here, &lt;4 equates to the cumulative probability P(0) + P(1) + P(2) + P(3).\nFor these calculations, we can use pbinom(...) instead of sum(dbinom(...)).\n\n# pbinom gives the cumulative probability\npaste(\"The cumulative probability is\", \n      max(pbinom(0:num_female - 1, num_trials, p_female)))\n\n[1] \"The cumulative probability is 0.36328125\"\n\n\n\nQ. 2.17Why do we parameterise pbinom() with num_female-1 rather than num_female?\n\n\n\nQ. 2.18Would this change if the question was \\(P( \\leq 4)\\)?\n\n\n\nQ. 2.19Take max out of the above line and run again. You should see 5 cumulative probabilities. Why is the first cumulative probability zero?\n\n\nNote that R has vectorized the calculation, returning the probability for each value in vector from 0:num_female - 1.\n\nQ. 2.20Calculate P(&lt; 4 females | 8 eggs) using dbinom() instead of pbinom().\n\n\nThe above code has a bug in it. You can check what R is doing by running parts of the code:\n\n0:num_female - 1 # Oops! Is this what you expected?\n\n[1] -1  0  1  2  3\n\n0:(num_female - 1) # this is actually what we want.\n\n[1] 0 1 2 3\n\n\n\nQ. 2.21Correct the code above. Why did this bug have no effect?\n\n\n\nQ. 2.22What is the probability of getting 3 females?\n\n\n\nQ. 2.23What is the probability of getting 8 females?\n\n\n\nQ. 2.24What cumulative probabilities would you need to consider to answer the question “What is the probability of getting fewer than three females?”\n\n\n\nQ. 2.25Review your model that describes this random process (number of females per eight eggs) that you wrote above: \\(y \\sim Binom(n, p)\\).\n\n\n\nQ. 2.26What is the probability of getting &lt; 4 females?\n\n\n\nQ. 2.27What is the probability of getting \\(\\leq\\) 4 females?\n\n\n\nQ. 2.28What is the probability of getting &gt; than 8 females?\n\n\n\nQ. 2.29What is the probability of getting \\(\\geq\\) than 2 females?\n\n\nLet’s visualize these distributions in order to better understand them.\n\n# Run this, then explore values of p_female\n# Note: 'success' and 'failure' is arbitrary. Just make sure you're calculating\n# what you think. How would you calculate the probabilities for males instead?\nnum_female &lt;- 0:8 \np_female &lt;- 0.5 # what are the limits of p_female?\nprFemale_df &lt;- data.frame(num_female=num_female, \n                          prob=dbinom(num_female, max(num_female), p_female)) \nprFemale_df \n\n  num_female       prob\n1          0 0.00390625\n2          1 0.03125000\n3          2 0.10937500\n4          3 0.21875000\n5          4 0.27343750\n6          5 0.21875000\n7          6 0.10937500\n8          7 0.03125000\n9          8 0.00390625\n\nbarplot(prFemale_df$prob, names=prFemale_df$num_female, \n        xlab=\"Number of females\", ylab=\"Probability\")\n\n\n\n\n\n\nFigure 2.1: Binomial probability distribution\n\n\n\n\n\nprFemale_df$cumul_prob &lt;- cumsum(prFemale_df$prob)\nbarplot(prFemale_df$cumul_prob, names=prFemale_df$num_female, \n        xlab=\"Enter the correct label!\", ylab=\"Enter the correct label!\")\n\n\n\n\n\n\nFigure 2.2: Cumulative binomial probability distribution\n\n\n\n\nTry re-plotting so that the two panels appear side by side (hint: par(mfrow=c(...))).\nNote \\(P(y_i=x)\\) is read as ‘the probability that a random observation y sub i equals x’. Sometimes this includes conditions: \\(P(y_i=x|n,p)\\), which is read as the probability that \\(y_i\\) equals \\(x\\) given \\(n\\) and \\(p\\). You may see \\(P()\\), \\(Pr()\\), \\(p()\\), or \\(Prob()\\), which all mean the same thing.\nThe interpretation of \\(P(y_i=8\\ |\\ n=8,\\ p=0.5) = 0.00391\\) is that the probability of 8 female offspring in a clutch of 8 eggs where each egg has 50% probability of being female is 0.00391.\nIn other words, if we have 500 broods, each with 8 eggs, we expect \\(500 * 0.00391 = 1.95 \\approx 2\\) broods to be all female. Is there something strange about our Oozle or is it just one of the 2/500 by chance?\n\nQ. 2.30Why is a bar graph the appropriate plot here?\n\n\n\nQ. 2.31What do you notice about the shape of the distribution when \\(p=q=0.5\\)\n\n\n\nQ. 2.32Re-run the analysis with the probability of female as 0.8 and plot the results. How has the shape of the distribution changed?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "P2_binomial_poisson.html#the-poisson-distribution",
    "href": "P2_binomial_poisson.html#the-poisson-distribution",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.2 The Poisson distribution",
    "text": "2.2 The Poisson distribution\nThe Poisson distribution is another probability distribution that describes discrete events that occur in space and/or time. The Poisson distribution is used to model (predict) the distribution of events that are rare, random, and independent. This can include events like earthquakes, storms, or the number of whales spotted on a cruise.\nThe Poisson distribution takes a single parameter: the mean. If a variable is Poisson distributed, its variance will equal its mean. This is a diagnostic feature of the distribution. The Poisson distribution is a discrete probability distribution, but its parameter, the mean, is continuous (similar to continuous \\(p\\) for the discrete binomial distribution).\nFind the formula for the Poisson distribution in Chapter 7.\nHere, \\(\\bar{y}\\) is the mean, \\(x\\) is the outcome of interest, \\(e\\) is Euler’s number, and \\(!\\) is factorial. Note that here, we use \\(\\bar{y}\\) as our stand-in for the population parameter \\(\\lambda = \\mu = \\sigma\\) which defines the Poisson distribution.\n\nQ. 2.33Translate the Poisson formula into an R function by completing the following code.\n\n\n\n# hint: e^2 = exp(2)\n# hint: 3! = factorial(3)\ncalc_poisson_prob &lt;- function(x, y_bar) {\n  # Translate the formula here using x and y_bar\n}\n\n\n2.2.1 Poisson distributions by hand\nThe first step is to calculate the mean number of observations per unit. This is the Poisson parameter, and is referred to as the rate or as lambda (\\(\\lambda\\)). The unit could be spatial (e.g., per \\(m^2\\)) or temporal (e.g., per hour).\nLast weekend, I randomly threw a 1\\(m^2\\) quadrat repeatedly on a sandy beach covered in worm casts Figure 2.3. In each quadrat, I counted the number of casts.\n\n\n\n\n\nFigure 2.3: Worm casts on a sandy beach with 1x1 m quadrats.\n\n\nFrom these data you can calculate the mean number of observations per unit.\nThe code below generates a dataframe from which you can determine that the mean number of worms per quadrat is 1.41. We wish to predict the proportion of quadrats that would contain 0, 1, 2, 3, 4, and 5 worms, assuming that the worms are independently distributed across space (i.e., random: one worm’s location has no effect on another worm’s location, and there is no relevant environmental variation).\nWe will then compare our observations to the expectations from the theoretical Poisson distribution. To do this, we need to know the probability of observing each count, given the mean count per quadrat.\n\n# num_worms is the number of worms per quadrat\n# num_quadrats is number of quadrats that contained each number of worms\n# This dataset is summarised. Raw data might have columns: quadrat_id, num_worms\nworm_df &lt;- data.frame(num_worms=c(0, 1, 2, 3, 4, 5, 6),\n                      num_quadrats=c(35, 28, 15, 10, 7, 5, 0))\nworm_df\n\n  num_worms num_quadrats\n1         0           35\n2         1           28\n3         2           15\n4         3           10\n5         4            7\n6         5            5\n7         6            0\n\n\nAs the number of worms per quadrat is relatively small, the occurrences are rare enough to be reasonably described by a Poisson distribution if worms occur independently.\nFrom the mean, we can calculate the expected frequency of observing different numbers of worms in any quadrat (assuming the model assumptions are met: What are the assumptions?). The number of worms per quadrat (\\(y\\)) is discrete; it can only take integers greater or equal to zero.\nWe are often interested in probabilities such as \\(P(y_i \\leq a)\\). That is, the probability that an observation \\(i\\) of the random variable \\(y\\) is less than or equal to \\(a\\). For example, you may need to calculate \\(P(y_i \\leq 1)\\), which is the probability that a random quadrat (\\(y_i\\)) contains 1 or fewer worm casts (\\(a\\), an integer value). To be fully complete, we might even write \\(P(y_i \\leq 1 | \\bar{y})\\), which acknowledges that we know the (sample) mean.\nSo, to calculate the probability of obtaining 1 or fewer worms per quadrat, you could start by writing \\(P(y_i = 0) + P(y_i=1) = \\dots\\).\n\nQ. 2.34Use your calc_poisson_prob() function to calculate the expected frequency of 0, 1 & 2 worms per quadrat.\n\n\n\nQ. 2.35What calculation would you need to conduct to determine \\(P(y_i \\geq 1)\\)? What is the theoretical upper limit of the Poisson distribution?\n\n\nR has built-in functions for calculating these, but it is important to know what you are asking them to calculate.\n\n2.2.2 Poisson distributions using *pois() functions\nYou can determine the expected probabilities for each worm count per quadrat once you have determined the mean count of worms per quadrat. Since we have a summarised dataset (i.e., the number of observations num_quadrats for each number of worms num_worms, rather than the raw data with a row for each quadrat), we need to do some calculations. The mean number of worms per quadrat = (total number of worms) / (total number of quadrats).\n\nsum(worm_df$num_quadrats) # total number of quadrats\n\n[1] 100\n\nsum(worm_df$num_quadrats * worm_df$num_worms) # total number of worms\n\n[1] 141\n\nlambda_worms &lt;- with(worm_df, sum(num_worms * num_quadrats) / sum(num_quadrats))\nlambda_worms # the mean number of worms per quadrat\n\n[1] 1.41\n\n\nGiven this, we can find \\(P(y_i \\leq 5)\\): the probability of a random quadrat containing five or fewer worms. With a mean of 1.41 worms per quadrat, \\(P(y_i \\leq 5) = 0.997\\) (3 sf). This means that if your data are Poisson distributed with \\(\\lambda = 1.41\\), it is highly unlikely to find more than five worms in a quadrat.\n\nQ. 2.36If the mean number of worms was 3 per quadrat, would you be more or less likely to get five worms in your quadrat?\n\n\nTo do these calculation in R, we can use dpois() and ppois():\n\n# ?dpois \n# for questions like 'determine P(y_i=a | lambda)'\na &lt;- 5 \nlambda &lt;- 1.41 \ndpois(a, lambda) # probability of observing 'a' worms per quadrat: dpois()\n\n[1] 0.01133859\n\n# ?ppois\n# for questions like 'determine P(y_i &gt;= a | lambda)\n1-ppois(a-1, lambda) # probability of observing &gt;= 'a' worms: ppois()\n\n[1] 0.01465169\n\n\n\nQ. 2.37Why is a-1 used in the ppois() function above? When would you use a instead?\n\n\n\nppois(0:a, lambda) # what does 0:a mean? What's another way to make this vector? \n\n[1] 0.2441433 0.5883853 0.8310759 0.9451405 0.9853483 0.9966869\n\nsignif(ppois(0:a, lambda), 3) # round with ?signif\n\n[1] 0.244 0.588 0.831 0.945 0.985 0.997\n\nbarplot(dpois(0:a, lambda),\n        ylab = \"Probability\", xlab = \"Number of worms\",\n        space = 0.2, ylim = c(0, 0.5), names.arg = 0:a)\n\n\n\n\n\n\nFigure 2.4: Poisson probability distribution.\n\n\n\n\n\nQ. 2.38Change the code to plot the cumulative probabilities. You will need to use ppois() instead of dpois() and adjust the y-axis limits (ylim).\n\n\n\n# create new columns in worm_df for the probabilities\nworm_df$prob &lt;- dpois(worm_df$num_worms, lambda)\nworm_df$cumul_prob &lt;- cumsum(worm_df$prob)\n\n\n# Make Table 2.1. Use packageName::function() instead of loading with library()\nknitr::kable(worm_df, digits=5) \n\n\nTable 2.1: Worm cast observations and expected probabilities.\n\n\n\n\nnum_worms\nnum_quadrats\nprob\ncumul_prob\n\n\n\n0\n35\n0.24414\n0.24414\n\n\n1\n28\n0.34424\n0.58839\n\n\n2\n15\n0.24269\n0.83108\n\n\n3\n10\n0.11406\n0.94514\n\n\n4\n7\n0.04021\n0.98535\n\n\n5\n5\n0.01134\n0.99669\n\n\n6\n0\n0.00266\n0.99935\n\n\n\n\n\n\n\n\n\nQ. 2.39Format the probabilities in Table 2.1 to 3 decimal places.\n\n\nIf individual probability values (not cumulative probability) are multiplied by the total number of quadrats thrown (n=100), we generate the expected frequency distribution for comparison with the observed results above.\n\nQ. 2.40Write the appropriate code to add a column (called num_quadrats_expected) to worm_df that is the expected number of quadrats (given 100 quadrats total).\n\n\nThis is the number of quadrats that you would expect to contain 0, 1, …, 5 worms, given that the mean density of worms is 1.41 per m\\(^2\\). Note that the number of worms per quadrat is a discrete variable, but you can have non-integer ‘expectations’ (i.e. means).\n\nQ. 2.41Add another column that is the difference in the observed number of quadrats and num_quadrats_expected.\n\n\n\nQ. 2.42Produce a bar graph of this difference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "P2_binomial_poisson.html#the-poisson-approximation-of-the-binomial-model",
    "href": "P2_binomial_poisson.html#the-poisson-approximation-of-the-binomial-model",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.3 The Poisson approximation of the binomial model",
    "text": "2.3 The Poisson approximation of the binomial model\nWhen the number of trials \\(n\\) is large and the probability of success \\(p\\) is small, the Poisson distribution can be used as an approximation of the binomial distribution. Under these circumstances you can calculate the mean of a variable that has a binomial distribution (\\(n*p\\)) and use that to approximate \\(y \\sim Pois(\\lambda = np)\\). Using the Poisson distribution is computationally more efficient in these cases. There is no settled threshold, but as \\(n\\) increases and \\(p\\) decreases, the approximation gets better.\nIf we set the mean \\(\\lambda = np = 5\\), we can visualize the distributions with different combinations of \\(n\\) and \\(p\\) (e.g., \\(p=0.5, n=10\\); \\(p=0.05, n=100\\), etc). We can use this to illustrate how the binomial distribution converges to the Poisson distribution as \\(n\\) gets larger.\n\nCode# generate dataframe with probability for 0:16 'successes' from different \n# distributions but where mean is 5.\n# note that in y ~ Binom(10,0.5), probability of &gt;10 successes is zero.\ny_seq &lt;- 0:13\nbinom_df &lt;- tibble(y=rep(y_seq, times=4), # ?rep\n                   n=rep(c(10, 20, 100, 500), each=length(y_seq)),\n                   p=rep(c(0.5, 0.25, 0.05, 0.01), each=length(y_seq))) |&gt;\n    mutate(mean=n*p,\n           prob=dbinom(y_seq, n, p),\n           label=paste0(\"y ~ Binom(\", n, \", \", p, \")\"))\npois_df &lt;- tibble(y=y_seq,\n                  n=NA, \n                  p=NA,\n                  mean=5) |&gt;\n    mutate(prob=dpois(y_seq, mean),\n           label=paste0(\"y ~ Pois(\", mean, \")\"))\ndistr_df &lt;- bind_rows(binom_df, pois_df) |&gt;\n  mutate(label=factor(label, levels=unique(label)))\n\n\n\nCodeggplot(distr_df, aes(y, prob, fill=label)) +  # ggplot(data, aes(xVar, yVar))\n  geom_bar(stat=\"identity\", position=\"dodge\", colour=\"grey30\") +  # ?geom_bar\n  scale_fill_brewer(\"Distribution\", palette=\"PuBu\") + # from colorbrewer2.org\n  labs(x=\"Number of successes with mean = 5\", y=\"Probability\") +\n  scale_x_continuous(breaks=y_seq) +\n  theme_classic() + \n  theme(legend.position=c(0.85, 0.85))\n\n\n\n\n\n\nFigure 2.5: Binomial and Poisson distributions converge with larger numbers of events, depending on p.\n\n\n\n\n\nQ. 2.43What is the modal value in each of these distributions?\n\n\n\nQ. 2.44Generate some random numbers from the distributions in Figure 2.5 and calculate their mean and variance.\n\n\nHere is \\(y \\sim Pois(\\lambda=5)\\):\n\ny &lt;- rpois(10000, 5)\npaste0(\"Mean: \", signif(mean(y), 3), \", variance: \", signif(var(y), 3))\n\n[1] \"Mean: 4.99, variance: 4.98\"\n\n\n\nQ. 2.45What do you notice about the mean and variance in the Poisson model?\n\n\n\nQ. 2.46What do you notice about the mean and variance in the binomial models as \\(n\\) increases and \\(p\\) decreases? When is it more similar to the Poisson? Is this what you expected?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "P2_binomial_poisson.html#conclusions",
    "href": "P2_binomial_poisson.html#conclusions",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.4 Conclusions",
    "text": "2.4 Conclusions\nThe binomial distribution is a discrete probability distribution that models situations where the outcome of an observation or experiment is binary (i.e., two possibilities) or is coded as such. The binomial model enables us to predict the probability of making our observation or series of independent observations for any given probability of a success (\\(p\\)) in a known number of trials. This enables us to quantify how likely our observation is to have occurred by chance. If the chance of our observation is very low, we can challenge the hypothesis with regard to the probability of success (\\(p\\)) and suggest a different value.\nThe Poisson distribution is another discrete probability distribution that is used to predict the probability of counts that are rare, independent and randomly distributed with mean = variance = \\(\\lambda\\). The Poisson distribution can be used as an approximation of the binomial distribution where the number of trials (\\(n\\)) is large and the probability of success (\\(p\\)) is small. This approximation is useful as, unlike the Poisson distribution, the binomial calculation requires the handling of massive numbers (from large factorials).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "P3_normal.html",
    "href": "P3_normal.html",
    "title": "3  Normal distribution",
    "section": "",
    "text": "3.1 Using the normal distribution\nStatistical inference is the process by which we infer from a sample statistic to the population. We need to infer from samples to populations because it is usually impossible to measure the entire population. Inference is the process of estimating population parameters from a sample.\nIn order to infer from samples to populations we need to understand how the statistics we generate from our samples are likely to ‘behave’. To do this we use theoretical distributions.\nThe normal (a.k.a., Gaussian) distribution is a theoretical distribution that is central to inferential statistics. If your data (or, more accurately, statistics derived from you data) are reasonably approximated by the normal distribution then you will be able to use a wide range of techniques to deal with it.\nIn this practical we will examine the normal distribution, calculate Z scores, and interpret those Z scores. We’ll assess whether data are reasonably assumed to be normally distributed, transforming the data where they are not. We’ll apply the CLT and evaluate how well other distributions are approximated by the normal distribution.\nThe length of a catch of herring was measured. Five hundred individuals were studied. We will consider this group to be the entire population of interest. The population parameters are: \\(\\mu = 37.6 cm\\) and \\(\\sigma = 1.20 cm\\).\nIf we know the population parameters, we can calculate Z scores for individuals (or groups of individuals) from that population and calculate how unusual they are. For the moment, we are interested in determining what proportion of fish from this population are expected to be &lt; 38 cm.\nCodemu &lt;- 37.6\nsigma &lt;- 1.2\ny &lt;- 38\n\ny_lt38_df &lt;- tibble(len=seq(mu-3*sigma, mu+3*sigma, length.out=1e3),\n                     density=dnorm(len, mu, sigma),\n                     shade=len &lt; 38)\nggplot(y_lt38_df, aes(len, ymin=0, ymax=density, fill=shade)) + \n  geom_ribbon(colour=\"grey30\") + \n  scale_fill_manual(values=c(\"white\", \"red3\"), guide=\"none\") +\n  labs(x=\"Herring length (cm)\", y=\"Probability density\") +\n  theme_classic()\n\n\n\n\n\n\nFigure 3.1: Normal distribution of herring lengths.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#using-the-normal-distribution",
    "href": "P3_normal.html#using-the-normal-distribution",
    "title": "3  Normal distribution",
    "section": "",
    "text": "Q. 3.2What are the theoretical limits of the normal distribution?\n\n\n\nQ. 3.3Do you think normality a fair assumption for these data?\n\n\n\nQ. 3.4Assume herring length is approximately normally distributed and write down the model which describes the fish length distribution: \\(y_i \\sim Norm(\\mu, \\sigma)\\).\n\n\n\n\n\n3.1.1 Using R to calculate areas under the normal curve\nTo solve this problem using R we use the cumulative probability. Observations at the extreme low end are extremely unlikely, but the probability of observing data increases and reaches a maximum at the mean, after which it declines again. The normal distribution is symmetrical.\n\nQ. 3.5What shape is the cumulative probability curve of a normally distributed variable?\n\n\nThe R functions for the normal distribution take the same form as those for the binomial and Poisson distributions. To calculate the probability is of observing a fish less than 38 cm, you need to specify the model and select the appropriate distribution function. Use ?dnorm or ?pnorm and look at your options (Figure 3.2).\n\nCodelibrary(cowplot)\nnorm_df &lt;- tibble(x=seq(-4, 4, length.out=1e3),\n                  x2=seq(0, 1, length.out=1e3),\n                  lo=0) |&gt;\n  mutate(d=dnorm(x), \n         q=qnorm(x2),\n         p=pnorm(x),\n         above_m1=x &gt; -1) \nlabels_df &lt;- tribble(~x, ~y, ~label,\n                     -1, dnorm(-1), \"dnorm(-1) = 0.242  \",\n                     -1, 0, \" qnorm(0.158) = -1\")\np1 &lt;- ggplot(norm_df, aes(x)) + \n  geom_ribbon(aes(ymin=lo, ymax=d, fill=above_m1)) +\n  geom_line(aes(y=d)) + \n  geom_point(data=labels_df, aes(y=y), size=2) +\n  geom_text(data=labels_df[1,], aes(y=y, label=label), hjust=1, size=3) + \n  geom_text(data=labels_df[2,], aes(y=y, label=label), size=3,\n            hjust=0, vjust=0, nudge_y=0.01) + \n  scale_fill_manual(\"\", values=c(\"red3\", \"white\"), \n                    labels=c(\"pnorm(-1) = 0.158\", \"\")) +\n  labs(x=\"Value\", y=\"Density\") +\n  theme_classic() +\n  theme(legend.position=c(0.8, 0.8),\n        legend.text=element_text(size=8),\n        legend.key.size=unit(0.3, \"cm\"))\np2 &lt;- ggplot(norm_df, aes(x, d)) + geom_line() + \n  labs(x=\"Value\", y=\"dnorm(Value, 0, 1)\") + \n  theme_classic() \np3 &lt;- ggplot(norm_df, aes(x, p)) + geom_line() + \n  labs(x=\"Value\", y=\"pnorm(Value, 0, 1)\") + \n  theme_classic() \np4 &lt;- ggplot(norm_df, aes(x2, q)) + geom_line() + \n  labs(x=\"Probability\", y=\"qnorm(Probability, 0, 1)\") + \n  theme_classic() \n\nplot_grid(p1, p2, p3, p4, align=\"hv\", axis=\"tblr\")\n\n\n\n\n\n\nFigure 3.2: Functions for distributions in R illustrated with a standard normal.\n\n\n\n\nTo calculate \\(P(y_i &lt; 38 | \\mu = 37.6, \\sigma = 1.2)\\), we use pnorm(), which gives the cumulative probability from -Inf to the value we choose.\n\n# cumulative probability: which bit of the curve does this relate to?\npnorm(q = 38, mean = 37.6, sd = 1.2) \n\n\nQ. 3.6What is the probability of a randomly selected fish from this population being less than 35 cm?\n\n\n\nQ. 3.7What proportion of individuals are greater than 39 cm?\n\n\nWe can plot any normal distribution we like. Play with the values for mu and sigma below, adjusting the values for from and to as needed to see the distribution.\n\nmu &lt;- 37.6  # population mean\nsigma &lt;- 1.2  # population sd\ncurve(dnorm(x, mean = mu, sd = sigma), from = 30, to = 45,\n      main = \"Normal density\", ylab = \"Density\", xlab = \"Fish length, cm\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#normal-model-adequacy",
    "href": "P3_normal.html#normal-model-adequacy",
    "title": "3  Normal distribution",
    "section": "\n3.2 Normal model adequacy",
    "text": "3.2 Normal model adequacy\nIn a population of long-eared wrasse, we calculate \\(\\mu\\) and \\(\\sigma\\) and find \\(y \\sim Norm(15.2 cm, 25.1 cm)\\).\n\nQ. 3.8What is the mean length and standard deviation of long-eared wrasse?\n\n\n\nQ. 3.9Calculate the proportion of fish that are expected to be less than zero cm in length.\n\n\n\nQ. 3.10What do your results indicate about the adequacy of the normal model to describe the length distribution of long-eared wrasse?\n\n\n\nQ. 3.11Would your conclusions change if \\(\\sigma\\) were smaller? For example: \\(y \\sim Norm(15.2, 2.51)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#quantiles",
    "href": "P3_normal.html#quantiles",
    "title": "3  Normal distribution",
    "section": "\n3.3 Quantiles",
    "text": "3.3 Quantiles\nA quantile is a value that divides a frequency distribution (i.e. a set of numbers) into equally represented groups (i.e., the same number of observations per group). For example, there are three values (Q1, Q2, Q3) that split a normal distribution into four groups (negative infinity to Q1, Q1 to Q2 (median), Q2 to Q3, and Q3 to positive infinity). Q3 - Q1 is the middle 50% of the data: the interquartile range.\nThere are 99 quantiles, called percentiles, that split your data into 100 groups. The 2.5 percentile is the value that splits your data into two groups corresponding to 2.5% along the distribution (from negative infinity for the normal distribution). Just as we can ask what proportion of a distribution is above or below a set value, we can also ask between which values will a given percentage of my data lie (e.g. what values correspond to the middle 95%?).\nTo solve this problem using R we use the quantile function qnorm(), speciying the cumulative probability as an argument. To find the value that splits the upper 2.5%:\n\nqnorm(p = 0.975, mean = 37.6, sd = 1.2) # p is the cumulative probability \n\n[1] 39.95196\n\n\n\nprobs &lt;- seq(from = 0.1, to = 0.9, by = 0.2) # vectorize for multiple values\nrbind(probs, quantiles=qnorm(p = probs, mean = 37.6, sd = 1.2))\n\n              [,1]     [,2] [,3]     [,4]     [,5]\nprobs      0.10000  0.30000  0.5  0.70000  0.90000\nquantiles 36.06214 36.97072 37.6 38.22928 39.13786\n\nqnorm(p = c(0.025, 0.975), mean = 37.6, sd = 1.2) # middle 95%\n\n[1] 35.24804 39.95196\n\n\nThese values (c(0.025, 0.975)) identify proportions of the cumulative curve. That is, they identify the bottom 2.5% and the bottom 97.5% of the curve. Values between these two parameters constitute the central 95% of your data.\nWe quote our mean and interval like this:\n\nThe mean fish length and 95% interval was 37.6 cm (35.2 – 40.0 cm).\n\nRemember to use to same degree of precision (significant figures) for confidence intervals as was used to gather the data (or as specified in the question, defaulting to three).\n\nQ. 3.12Find the values that capture the middle 90% of the herring data, where \\(y \\sim Norm(37.6, 1.2)\\).\n\n\n\nQ. 3.13What value would you expect to correspond to p=0.5?\n\n\n\nQ. 3.14Check your answer above using qnorm().\n\n\nTo visualize regions of the normal distribution, let’s use ggplot.\n\nmu &lt;- 37.6\nsigma &lt;- 1.2\nlb &lt;- 36 # lower boundary \nub &lt;- 40 # upper boundary\n\nnorm_df &lt;- tibble(z=seq(-4, 4, length.out=100),\n                  x=z*sigma + mu,\n                  densNorm=dnorm(x, mu, sigma))\nggplot(norm_df, aes(x, densNorm)) + \n  geom_line() + \n  geom_ribbon(data=norm_df |&gt; filter(x &gt; lb & x &lt; ub),\n              aes(ymin=0, ymax=densNorm), fill=\"steelblue\") +\n  labs(x=\"Herring length (cm)\", \n       y=\"Probability density\",\n       subtitle=paste0(\"P(\", lb, \"&lt; y &lt;\", ub, \") = \", \n                       signif(pnorm(ub, mu, sigma) - pnorm(lb, mu, sigma), digits=3))) +\n  theme_classic() # changes how the plot looks: ?theme\n\n\n\n\n\n\nFigure 3.3: The normal distribution illustrating probability of a random herring from your population having a length between 36 and 40 cm.\n\n\n\n\n\nQ. 3.15Change the parameters in code the above to check that the values you generated for the 95% interval correspond when plugged into lb and ub in the code.\n\n\n\nQ. 3.16What is the difference between \\(&lt; x\\) and \\(\\leq x\\) when applied to continuous data?\n\n\n\nQ. 3.17Does this also apply to discontinuous data?\n\n\nHow would you expect these intervals to change when the population standard deviation \\(\\sigma\\) changes?\n\nQ. 3.18With everything else equal, try doubling, quadrupling, and halving the standard deviation on the herring data then re-running the same code.\n\n\n\nQ. 3.19Does the change in the 95% interval match your expectations?\n\n\nA z score gives the number of standard deviations away from the mean for any value. The z score is the value from the standard normal distribution (\\(\\mu = 0, \\sigma = 1\\)) that corresponds with the same quantile of the original distribution. The values for any normal distribution can be converted to Z scores by subtracting \\(\\mu\\) and dividing by \\(\\sigma\\), such that \\(z_i = \\frac{y_i - \\mu}{\\sigma}\\)\n\nQ. 3.20With \\(y \\sim Norm(37.6, 1.2)\\), what is the z score for an individual of length 34?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#testing-for-normality",
    "href": "P3_normal.html#testing-for-normality",
    "title": "3  Normal distribution",
    "section": "\n3.4 Testing for normality",
    "text": "3.4 Testing for normality\nMany statistical tests assume that data are reasonably approximated by a normal distribution and have homogeneous variance. R can be used to formally test the assumption that data are normally distributed, though you should have some idea of whether this is likely through consideration of the data source. This applies particularly where you have a small sample size which makes evaluating the distribution challenging.\nThe data you collect will be part of a population. The normality check assesses the viability of the assumption that the data you collected were drawn from a population that was normally distributed. Note that populations are, in practice, never actually normally distributed. Your test is to assess how reasonable the assumption of normality is.\n\nMS &lt;- read_xlsx(\"data/H2DS_practicalData.xlsx\", sheet = \"Mood shrimp\")\n# check the data using head(), str() etc.\n\nWe’ll learn more advanced methods in Chapter 5 for the analyses we cover, but we can plot how well our observations follow the expectations of a normal distribution with qqnorm() and add a line of perfect fit with qqline().\n\npar(mfrow = c(1, 2))\nqqnorm(MS$Shrimp1)\nqqline(MS$Shrimp1, col = 2) # the data fall near the line\nqqnorm(MS$Shrimp2)\nqqline(MS$Shrimp2, col = 2) # the data deviate widely from the line\n\n\n\nNormal (QQ) plots. The left indicates that the normality assumption might be reasonable, not so on the right.\n\n\n\nThe axes are automatically scaled so that perfectly normally distributed data would fall on a diagonal line. Any deviation from the red straight line indicates a lack of normality. What we need to assess is how serious any deviation is and whether it is sufficient to indicate that the assumption of normality is not ‘reasonable’. This is a subjective decision, and two different statisticians may tell you different answers about the same data.\nAs sample size decreases, it becomes increasingly difficult to see if your data are reasonably approximated by a normal distribution. There are many formal statistical methods for assessing normality, but as we already know, it is impossible for a population to be distributed perfectly normally and this means such tests are largely redundant. You must assess the assumptions of your model, but you should be aware that all data fails the assumptions. The question is whether the assumptions are reasonably well met such that the model results can be useful. There is sadly no hard rule as to what constitutes ‘reasonable’!\n\nQ. 3.21Make histograms of both Shrimp1 and Shrimp2. Comment on their apparent distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#transformations",
    "href": "P3_normal.html#transformations",
    "title": "3  Normal distribution",
    "section": "\n3.5 Data transformations",
    "text": "3.5 Data transformations\nThe assumption that our sample data are drawn from a normally distributed population is central to the use of many important inferential statistical techniques. However, frequently it is not reasonable to assume that data are approximately normally distributed.\nA common issue is that our measurements are logically bounded. For example, chemical concentrations (e.g., zinc in sediments) cannot be negative. Similarly, you cannot have negative lengths, time, mass, etc. Proportions must be between 0 and 1. Where data is collected ‘near’ a logical boundary they are often not normally distributed, since the normal distribution predicts ‘tails’ which are impossible.\nOne solution is to use a mathematical transformation to convert your data to something that is reasonably approximated by a normal distribution even if it is not well approximated in the original measurement units. Often transformations will also correct unequal variances (see Chapter 5) in addition to non-normality so they are very useful. The appropriate transformation depends on the data.\nCommon transformations include:\n\nLog (log(x), inverse: exp(x)): When the distribution is skewed right and all values are &gt;0. Often ‘cures’ heteroscedasticity. Commonly used for observations spanning orders of magnitude such as body size.\n\nSquare-root (sqrt(x), inverse: x^2): When the measurements are areas (e.g., leaf areas). Often used to ‘down-weight’ common species (e.g., Chapter 6), which is unrelated to model assumptions. Values must be &gt;0.\n\nArcsine (asin(x), inverse: sin(x)^2): When the measurements are proportions. Tends to stretch out the tails (e.g., near 0 or 1 for proportions) and squash the middle (e.g., near 0.5).\n\nLogit (boot::logit(x), inverse: boot::inv.logit(x)): Used for proportions excluding 0 and 1. Also commonly used in models with a binary (Bernoulli) response variable (e.g., survival probability predicted by temperature, where the response variable is ‘alive’/‘dead’).\nReciprocal (1/x, inverse: 1/x): When the distribution is skewed right. Generally ‘stronger’ than a log transformation and often has logically interpretable units (\\(m~s^{-1}\\), \\(s~m^{-1}\\)).\n\n\nIdentifying the correct transformation can be led by an underlying comprehension of the nature of the data. However, this often doesn’t work so expect some trial and error.\nWe can visualize the relationship between data on the original scale and the transformed values. For values bounded by 0:\n\nCodepositive_df &lt;- tibble(orig=seq(0.01, 10, length.out=1e3)) |&gt;\n  mutate(sqrt=sqrt(orig),\n         ln=log(orig),\n         reciprocal=1/orig,\n         squared=orig^2) |&gt;\n  pivot_longer(cols=2:5, names_to=\"transformation\", values_to=\"new_value\")\n\nggplot(positive_df, aes(orig, new_value)) + \n  geom_line() + \n  facet_wrap(~transformation, scales=\"free\", nrow=1) + \n  labs(main=\"Positive values\", x=\"Original value\", y=\"Transformed value\")\n\n\n\n\n\n\nFigure 3.4: Effect of common transformations on positive data.\n\n\n\n\nAnd for proportions:\n\nCodeproportion_df &lt;- tibble(orig=seq(0.01, 0.99, length.out=1e3)) |&gt;\n  mutate(sqrt=sqrt(orig),\n         ln=log(orig),\n         asin=asin(orig),\n         logit=boot::logit(orig)) |&gt;\n  pivot_longer(cols=2:5, names_to=\"transformation\", values_to=\"new_value\")\n\nggplot(proportion_df, aes(orig, new_value)) + \n  geom_line() + \n  facet_wrap(~transformation, scales=\"free\", nrow=1) + \n  labs(main=\"Proportions\", x=\"Original value\", y=\"Transformed value\")\n\n\n\n\n\n\nFigure 3.5: Effect of common transformations on proportions.\n\n\n\n\n\nQ. 3.22Using the Radon concentration worksheet, plot the data. Do they look normally distributed?\n\n\n\n# get the Radon data into R; the units are parts per billion\nreadxl::excel_sheets(\"data/H2DS_practicalData.xlsx\")\n\n\nQ. 3.23In your own time, use R to determine the log, square root, and reciprocals (and combinations of all of them: at least one converts the data to approximate normality).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#the-central-limit-theorem",
    "href": "P3_normal.html#the-central-limit-theorem",
    "title": "3  Normal distribution",
    "section": "\n3.6 The Central Limit Theorem",
    "text": "3.6 The Central Limit Theorem\nThe central limit theorem (CLT) is about how sample means are distributed.\nThe CLT states that the means of normally distributed data will, themselves, be normally distributed.\nIn addition, the theorem states that the means of data which are NOT normally distributed will be normally distributed if the sample size is sufficiently large. In this practical we are going to demonstrate this theorem using random data generated from various probability distributions.\n\n3.6.1 The distribution of means from non-normal data\nThe following code chunk:\n\nGenerates a non-normally distributed dataset (obs_data)\nRepeatedly samples from it num_samples times, each with sample_size observations\nCalculates and stores the sample mean for each num_samples repeat\nPlots histograms and QQ-plots for the raw data and for the sample means\n\nRun the following code and then experiment with the sample_size.\n\n# Non-normal data: a mixture of several distributions\nobs_data &lt;- c(rnorm(2000, 200, 20), \n              rnorm(1500, 100, 50),\n              rnorm(1000, 400, 80), \n              rnorm(800, 300, 100))\nxlims &lt;- range(obs_data)\n\n# Define size of each sample and the number of sampling repeats\nsample_size &lt;- 30\nnum_samples &lt;- 3000\n\n# Sample num_samples times from obs_data, with n = sample_size for each sample\nsample_means &lt;- numeric(length=num_samples) # initialize an empty vector\nfor(i in 1:num_samples) {\n  sample_means[i] &lt;- sample(x=obs_data, size=sample_size, replace=T) |&gt; mean()\n}\n\n# plot observed distribution\npar(mfrow = c(2, 2),\n    mar = c(5, 2, 2, 2)) \nhist(obs_data,\n     main = \"Raw data: obs_data\",\n     sub = paste0(\"mean: \", signif(mean(obs_data), 3), \n                  \", sd: \", signif(sd(obs_data), 3)),\n     breaks = 30, xlab = \"Observations\", xlim = xlims\n) \nqqnorm(obs_data, main = \"Raw data QQ\")\nqqline(obs_data)\n\n# plot distribution of sample means\nhist(sample_means,\n     main = paste0(\"Sample means, N: \", sample_size),\n     sub = paste0(\"mean: \", signif(mean(sample_means), 3), \n                  \", sd: \", signif(sd(sample_means), 3)),\n     breaks = 30, xlab = \"Sample means\", xlim = xlims\n) \nqqnorm(sample_means, main=\"Sample mean QQ\")\nqqline(sample_means)\n\n\n\n\n\n\nFigure 3.6: The central limit theorem in action.\n\n\n\n\n\nQ. 3.24What do you notice about the location of the mean as a function of sample_size (\\(n\\))?\n\n\n\nQ. 3.25What do you notice about the spread around the mean of sample means as a function of sample_size? Why did the pattern you have observed occur?\n\n\nThe CLT states that sample_means will be normally distributed if (a) the underlying data are normally distributed, OR (b) the sample_size is large enough. With R, we can see this in action.\nNote also the relationship between the sample size and the range of values for the sample mean. How does the standard deviation of sample_means change with changes in sample_size?.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#the-standard-error-of-the-mean",
    "href": "P3_normal.html#the-standard-error-of-the-mean",
    "title": "3  Normal distribution",
    "section": "\n3.7 The standard error of the mean",
    "text": "3.7 The standard error of the mean\nThe standard error of the mean is the standard deviation of sample means, of a given sample size, taken from a population. It describes the dispersion of sample means (\\(\\bar{y}\\)’s) we would expect if we were to repeatedly sample the same population over and over again. It is the standard deviation of the distribution shown in the lower histogram in Figure 3.6 : sd(sample_means).\nUsually we only have a single sample rather than 10,000 (=num_samples) as above. In that case, we estimate it from a single sample as \\({SE}_{\\bar{y}} = \\frac{sd(y)}{\\sqrt{n}}\\), where \\(y\\) is a vector of \\(n\\) observations.\nWe can explore this similarly to the simulated sampling we did for the CLT.\n\n# Simulate a normally distributed population with mean mu and sd sigma\nmu &lt;- 10\nsigma &lt;- 2\nsim_obs &lt;- rnorm(10000, mu, sigma)\n\n# Define sample size and number\nsample_size &lt;- c(3, 10, 30, 100) # size of each sample\nnum_samples &lt;- 3000 # number of samples (=repeats) for each sample size\n\n# Create a dataframe to store results\nsample_mean_df &lt;- tibble(N=rep(sample_size, each = num_samples),\n                         id=rep(1:num_samples, times = length(sample_size))) |&gt;\n  rowwise() |&gt; # enforces new sample() call for each row\n  mutate(sample_mean=mean(sample(x=sim_obs, size=N))) |&gt;\n  ungroup()\n\n\nsample_mean_df |&gt;\n  mutate(N=factor(N, levels=unique(N), labels=paste(\"n:\", unique(N)))) |&gt;\n  ggplot(aes(sample_mean, colour=N)) +\n  geom_vline(xintercept=mu, linetype=3) +\n  geom_density(linewidth=0.9) +\n  scale_colour_viridis_d(\"Size of each sample\", option=\"mako\", end=0.85) +\n  labs(x=paste(\"Means of\", num_samples, \"simulated samples\")) +\n  theme_classic()\n\n\n\n\n\n\nFigure 3.7: Simulated sample means. Each curve shows the sampling distribution for means of samples with size n. The standard deviation of each curve is the standard error of the mean.\n\n\n\n\n\nQ. 3.26Calculate the theoretical standard error of the mean for each sample_size given sigma (i.e., from the equation) and compare this with the standard error from the simulated sample means in sample_mean_df.\n\n\nHint:\n\nsample_mean_df |&gt;\n  group_by(N) |&gt;\n  summarise() # what goes here?\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nsample_mean_df |&gt;\n  group_by(N) |&gt;\n  summarise(se_bySim=sd(sample_mean)) |&gt;\n  ungroup() |&gt;\n  mutate(se_byFormula=sigma/sqrt(N))\n\n# A tibble: 4 × 3\n      N se_bySim se_byFormula\n  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1     3    1.16         1.15 \n2    10    0.631        0.632\n3    30    0.371        0.365\n4   100    0.194        0.2  \n\n\n\n\n\n\nQ. 3.27Given what you know about the CLT, how does this change with non-normally distributed data? Try creating sim_obs using rpois().\n\n\n\nQ. 3.28Change sigma and sample_size and check that the standard error estimates are in line with their true values (i.e., as determined by using the standard error formula).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#normal-approximations",
    "href": "P3_normal.html#normal-approximations",
    "title": "3  Normal distribution",
    "section": "\n3.8 Normal approximations",
    "text": "3.8 Normal approximations\nIn Chapter 2 you saw that the Poisson distribution could be used to approximate the binomial distribution. In a conceptually similar way, the normal distribution can be used to approximate both the Poisson distribution and the binomial distribution under certain conditions.\nWhile the Poisson and binomial distributions are discrete and the normal distribution continuous, in practice all of our observations are discontinuous, subject to our measurement precision. A recorded mass of 437 g really indicates a mass between 436.5 and 437.5 g. The assumption that a variable is continuous is typically reasonable if there at least ~30 possible values between the smallest and largest observation. This constraint similarly applies to the approximation of binomial and Poisson distributions with the normal distribution.\n\n\n\n\n\n\nAll models are wrong, but some are useful! Normality is always approximate in practice.\n\n\n\n\n3.8.1 The Normal approximation of the Poisson distribution\nRecall that the Poisson model takes only one parameter, \\(\\lambda\\) = mean = variance. So if we have a variable \\(y \\sim Pois(10)\\), the mean (e.g., density per quadrat) is 10 and so is the variance. We now have the two parameters that define the normal distribution.\n\nQ. 3.29For \\(y \\sim Pois(10)\\) write the equivalent normal distribution: \\(y \\sim Norm(...)\\).\n\n\nA normal approximation may be reasonable if \\(\\lambda \\geq 30\\). So \\(y \\sim Pois(10)\\) should not be approximated by the normal distribution while \\(y \\sim Pois(30)\\) could be. As \\(\\lambda\\) gets larger, the Poisson distribution becomes more and more computationally demanding compared to the normal distribution. Many standard statistical analyses also assume normal distributions.\nHowever, when feasible, it is generally preferable to use the natural distribution for your data (e.g., a Poisson distribution for counts) through the appropriate GLM. The normal distribution is beneficial in some cases, but this is increasingly less so with modern methods.\n\nQ. 3.30Calculate \\(P(y_i \\leq 35)\\) from \\(y \\sim Pois(40)\\) and compare it to the same probability under the normal approximation of this distribution.\n\n\n\nQ. 3.31Evaluate \\(P(y_i&lt;3\\ |\\ \\lambda=5)\\) using the Poisson model and its normal approximation.\n\n\n\n3.8.2 The Normal approximation of the binomial distribution\nWhen \\(n\\) is large, the binomial distribution tends toward a normal distribution, particularly if \\(p\\) is near 0.5. Roughly speaking, if \\(np &gt; 5\\) and \\(n(1-p) &gt; 5\\), then the normal distribution may be a reasonable approximation. The mean of the binomial distribution is \\(np\\) and the variance is \\(npq\\).\nWe might be interested in predicting the number of male and female offspring in turtle clutches. Assume the proportion that are male is 0.5 and clutch size is 40. We’ve observed several clutches of eggs on a particular island with only 10 males. We might wonder how unlikely this was by chance, assuming \\(P(male)=0.5\\).\n\nQ. 3.32What is the probability of observing 10 or fewer males in this scenario?\n\n\n\nQ. 3.33Plot the probability (i.e. from 0 – 40 males) as a bar graph and comment on its shape.\n\n\n\nQ. 3.34Calculate the mean and variance of this population.\n\n\n\nQ. 3.35Are \\(np\\) and \\(n(1-p)\\) both &gt; 5?\n\n\n\nQ. 3.36Specify the normal distribution that approximates this binomial distribution.\n\n\n\nQ. 3.37What is \\(P(y_i \\leq 10)\\) for the normal approximation?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P3_normal.html#conclusions",
    "href": "P3_normal.html#conclusions",
    "title": "3  Normal distribution",
    "section": "\n3.9 Conclusions",
    "text": "3.9 Conclusions\nThe normal distribution is central to statistics. A huge variety of observations are reasonably approximated by the normal distribution (or can be made to be normally distributed through transformation).\nThe normal distribution can be used to assess the likelihood of a given observation, if we know the population mean and standard deviation from which it came. Furthermore, given our knowledge of the population parameters (\\(\\mu\\), \\(\\sigma\\)) we can determine values which define intervals on that population. Frequently scientists determine the values that bound 95% of their data.\nThe central limit theorem says that sample means taken from a normally distributed population will themselves be normally distributed and, in addition, means of sufficiently large samples will also be normally distributed even where the original data are not normally distributed (the sample size required depends on the extent of the skew in the original data).\nThe normal distribution is a good approximation of the Poisson distribution when lambda is large and the binomial model where the number of trials is large and the probability of success around 0.5 (i.e. the distribution of values is not too skewed).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "P4_t_CIs.html",
    "href": "P4_t_CIs.html",
    "title": "4  The t-distribution and confidence intervals",
    "section": "",
    "text": "4.1 Single sample t-tests\nThe goal of science is to understand the world (universe!). To do that, we want to know the values of population parameters (e.g., the mean size of barnacles on the back-beach, the variance in fail-rate of a machine component, the maximum satellite signal strength per satellite transect, the mean size of a fisher’s catch, …). However, we usually cannot measure entire populations due to logistical/time/money constraints. Instead, we take a (random) sample, and infer from that sample to our population of interest based on our statistical models of the world. This is statistical inference.\nUnfortunately, we can never know how well our sample reflects the population.\nFor example, our random sample of barnacles might contain (by chance alone) mostly big ones, or barnacles that varied considerably (or negligibly) in size. The t-distribution is similar to the normal distribution, but it accounts for this added uncertainty. This enables us to estimate the probability that a given sample came from a population with any given mean (with caveats). The t-distribution also enables us to put confidence intervals around the mean of our sample, and gives us some idea of the values of the mean that are likely.\nThe t-distribution models the probability of making a given observation from a population whose parameters are estimated from your sample. The t statistic is calculated in the same way as the z score for samples, but the expected scores follow the t-distribution (instead of the normal distribution) which accounts for sampling uncertainty. For small sample sizes, we are less confident about the population parameter estimates, and the t-distribution consequently becomes shorter than the normal distribution and with fatter tails (Figure 4.1). The shape of the t-distribution depends on the degrees of freedom (\\(df = \\nu = N-1\\)). There is more guidance on the t-distribution, and links to other sources on Brightspace.\nThe single sample t-test is analogous to the calculation of z scores. It enables us to determine how unlikely our sample mean is, given any hypothesized mean. However, before using any parametric tests such as t-tests, we need to assure ourselves that the model assumptions are reasonably met.\nImagine we are fisheries inspectors and have sampled the cod landed by a fishing boat. We know that the mean size of the landed cod should be greater than 36.6 cm. We need to assess how likely it is that our sampled cod come from a population where \\(\\mu \\geq 36.6 cm\\). We are testing the hypothesis that there is one ‘population’ of legally landed cod, and these cod are a part of that population. We use the t-distribution to assess the probability that our sample was drawn from a legally-landed cod population. If this probability is low then we might speculate that the cod are, in fact, drawn from a different population (i.e., that the boat is using illegal gear).\nWe do not know the mean \\(\\mu\\) or standard deviation \\(\\sigma\\) of the population and hence cannot use a normal distribution to model the likelihood of observing any particular value.\nFirst, we should clearly state our hypotheses:\nH0 (the null hypothesis): The population mean of the cod on this boat is greater than or equal to 36.6 cm (\\(\\mu \\geq 36.6 cm\\)).\nH1 (the alternative hypothesis): The population mean of the cod on this boat is less than 36.6 cm (\\(\\mu &lt; 36.6 cm\\)).\nWe use a t-test to calculate the probability of drawing our sample from a population where the mean is 36.6 cm or greater.\nWe collect a sample of 20 fish (found in the worksheet ‘Cod lengths’). The sample size is \\(&lt;\\) 30 so we can’t assume that the means will be normally distributed under the CLT. We can check the normality assumption by plotting the data using a ‘normality’ plot or ‘QQ-plot’ (Figure 4.2).\ncod_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"Cod lengths\")\nstr(cod_df)\n\ntibble [20 × 1] (S3: tbl_df/tbl/data.frame)\n $ CodLength_cm: num [1:20] 34.1 35.6 36.1 34.6 38.3 35 36.8 38 34.4 35.5 ...\nqqnorm(cod_df$CodLength_cm, main = NULL)\nqqline(cod_df$CodLength_cm)\n\n\n\n\n\n\nFigure 4.2: QQ-plot for the sample of cod.\nWe wish to assess how likely our sample is to have been drawn from a population where \\(\\mu\\) is at least 36.6 cm. If our sample mean \\(\\bar{y}\\) is less than the ‘legal’ mean and it is ‘unlikely’ to have been drawn from a legal population, we might wonder if the mean of the landed cod on this boat is &lt;36.6 cm and recommend legal action.\nHopefully your manually calculated t-statistic and the one generated by R match. The p-value given by R is exact: there is a probability of 0.014819 that a sample of 20 cod with mean of 35.785 cm would be drawn from a legally landed cod population where the true mean was 36.6 cm or more (assuming model assumptions are met).\nEvaluating evidence is a central part of statistical analysis. In this example, your conclusion about the population mean \\(\\mu\\) based on your sample determines whether legal action is taken. When a binary decision is necessary, it is best to set the decision threshold before collecting data. This is called the \\(\\alpha\\) value. It is often set to 0.05, but this is a subjective choice. If \\(P &lt; \\alpha\\), the null hypothesis is rejected; the sample is considered too unlikely if \\(\\mu \\geq 36.6cm\\). In this scenario, we have two clear competing hypotheses, so we are in the realm of ‘Neyman-Pearson’s’ decision theory (not Fisher’s hypothesis significance testing approach).\nNow let’s use simulation to explore variability among samples. We will repeatedly sample from \\(y \\sim Norm(100, 10)\\).\nWe will repeat our sampling num_samples times. Each sample will be sample_size values drawn from a normally distributed population with mean mu and standard deviation sigma. For each sample, we will calculate the sample mean \\(\\bar{y}\\) and sample standard deviation \\(s\\). Note that in this case, we know the population parameters.\n# set simulation details and population parameters\nnum_samples &lt;- 5\nsample_size &lt;- 3\nmu &lt;- 100\nsigma &lt;- 10\n\n# initialize plot\nset.seed(1)\nxlims &lt;- c(mu-4.5*sigma, mu+3*sigma)\nplot(NA, NA, xlim=xlims, ylim=c(0,num_samples),\n     xlab=\"Observations\", ylab=\"Sample number\")\nabline(v=mu, lty=3)\npoints(mu, 0, pch=4, col=\"steelblue\")\nsegments(mu-sigma, 0, mu+sigma, 0, col=\"steelblue\")\ntext(x=xlims[1], y=0, adj=c(0,0.5), col=\"steelblue\", \n     labels=paste0(\"mu: \", mu, \", sigma: \", sigma))\n\n# repeatedly sample\nfor (i in 1:num_samples) {\n  sample_i &lt;- rnorm(n = sample_size, mean = mu, sd = sigma)\n  sample_mean &lt;- signif(mean(sample_i), 3)\n  sample_sd &lt;- signif(sd(sample_i), 3)\n  # add to plot\n  points(sample_i, rep(i, sample_size))\n  points(sample_mean, i, pch=4)\n  segments(sample_mean-sample_sd, i, sample_mean+sample_sd, i)\n  text(x=xlims[1], y=i, adj=c(0,0.5),\n       paste0(\"ybar: \", sample_mean, \", s: \", sample_sd))\n}\n\n\n\n\n\n\nFigure 4.3: Repeated samples from a normal distribution. Open points are observations in the sample, with ‘x’ giving the mean and lines showing 1 sd.\nNote: these are random samples, so the values will be different each time you run the code. However, R uses pseudo-random number generation. Use set.seed() for fully reproducible code.\nThe discrepancy (\\(\\mu\\) vs. \\(\\bar{y}\\), \\(\\sigma\\) vs. \\(s\\)) is called sampling error. In most situations, we do not know \\(\\mu\\) and \\(\\sigma\\), but must estimate them from our sample.\nIf the sample is very large (and representative) then the estimate of the population parameters is likely very good. However, as the sample size is reduced, the reliability of the estimate decreases. Try adjusting sample_size in the code above and see how it affects the results.\nThe t-distribution is the distribution of values you get when you subtract sample means from the population mean and standardize by the sample standard error (i.e., \\(\\frac{\\bar{y} - \\mu}{SE_{\\bar{y}}}\\)).\nWe can extend the simulated sampling above to calculate a T-statistic for each sample in addition to \\(\\bar{y}\\) and \\(s\\). We will set num_samples very large to better represent the distribution of sample T-statistics. The red histogram in Figure 4.4 shows the distribution of these sample T-statistics. The solid curve is the theoretical t-distribution (df=sample_size - 1) and the dotted line is a standard normal distribution.\nmu &lt;- 10 # population mean\nsigma &lt;- 10 # population sd\nnum_samples &lt;- 1e5 # number of samples\nsample_size &lt;- 3 # size of each sample\nT_sample &lt;- numeric(num_samples) # sample t statistics\n\n# for each repeat: draw a sample, calculate SE and T, and store T in T_sample\nfor(i in 1:num_samples) {\n  sample_i &lt;- rnorm(sample_size, mu, sigma)\n  sample_SEM &lt;- sd(sample_i) / sqrt(sample_size)\n  T_sample[i] &lt;- (mean(sample_i) - mu) / (sample_SEM)\n}\n\ncurve(dnorm(x, 0, 1), from = -6, to = 6, lty = 2, \n      xlab = \"Simulated T-statistics\", ylab = \"Density\",\n      main = paste(\"T-statistics of\", \n                   format(num_samples, big.mark=\",\", scientific=F), \n                   \"samples, each with N =\", sample_size))\nhist(T_sample, freq = F, add = T, col = rgb(1, 0, 0, 0.25), \n     breaks = seq(floor(min(T_sample)), ceiling(max(T_sample)), by=0.2))\ncurve(dnorm(x, 0, 1), from = -6, to = 6, add = T, lty = 2)\ncurve(dt(x, sample_size - 1), from = -6, to = 6, add = T)\nlegend(\"topright\", lty = c(2, 1, 1), col = c(1, 1, 2), bty = \"n\",\n       c(\"Normal(0,1)\", paste0(\"t(df=\", sample_size-1, \")\"), \"t-stat (sim)\"))\n\n\n\n\n\n\nFigure 4.4: Histogram of 100,000 T-statistics calculated from 100,000 samples, along with the corresponding theoretical t-distribution (solid line) and a standard normal (dotted line).\nNotice how the histogram and the solid lines are nearly identical? These simulations illustrate that the t-distribution is the distribution of t-statistics for a given sample size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The t-distribution and confidence intervals</span>"
    ]
  },
  {
    "objectID": "P4_t_CIs.html#single-sample-t-tests",
    "href": "P4_t_CIs.html#single-sample-t-tests",
    "title": "4  The t-distribution and confidence intervals",
    "section": "",
    "text": "Q. 4.3Given the hypothesis, is this one or two tailed test?\n\n\n\n\n\n\nQ. 4.4Do you think the normality assumption is reasonable?\n\n\n\nQ. 4.5What parameter are we actually trying to understand / model? How does the distribution of this parameter change with sample size (think CLT)?\n\n\n\n\nQ. 4.6Calculate the sample mean, standard deviation, and standard error of the mean (Chapter 7).\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nCodecod_df |&gt;\n  summarise(mean=mean(CodLength_cm),\n            sd=sd(CodLength_cm),\n            N=n(),\n            SEM=sd(CodLength_cm)/sqrt(N))\n\n# A tibble: 1 × 4\n   mean    sd     N   SEM\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1  35.8  1.55    20 0.347\n\n\n\n\n\n\nQ. 4.7Now manually calculate the t-statistic for this sample and determine the probability of observing your data assuming that \\(\\mu = 36.6 cm\\).\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nCodeybar &lt;- mean(cod_df$CodLength_cm)\nSEM &lt;- sd(cod_df$CodLength_cm) / sqrt(length(cod_df$CodLength_cm))\nT_stat &lt;- (ybar - 36.6) / SEM\nround(T_stat, 2)\n\n[1] -2.35\n\n\n\n\n\n\nQ. 4.8How does this value compare to the expectation under the null hypothesis? Use pt().\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nCodept(T_stat, df=19) |&gt; round(5)\n\n[1] 0.01482\n\n\n\n\n\n\nQ. 4.9Check you answer against that given by the t.test() function. See ?t.test.\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nCodet.test(cod_df$CodLength_cm, mu = 36.6, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  cod_df$CodLength_cm\nt = -2.3515, df = 19, p-value = 0.01482\nalternative hypothesis: true mean is less than 36.6\n95 percent confidence interval:\n     -Inf 36.38429\nsample estimates:\nmean of x \n   35.785 \n\n\n\n\n\n\n\nQ. 4.10What is your next step as the regulatory agent?\n\n\n\n\nQ. 4.11Given the P value, do you reject the null hypothesis?\n\n\n\nQ. 4.12If you had set \\(\\alpha\\) at 0.01 would you reject the null hypothesis?\n\n\n\nQ. 4.13If you set \\(\\alpha\\) at 0.01 rather than 0.05, what type of error are you reducing and what type of error are you increasing? Which wrong conclusion becomes more likely and which becomes less likely?\n\n\n\n\nQ. 4.14What is the standard deviation in this model?\n\n\n\nQ. 4.15What does the symbol ‘~’ mean?\n\n\n\n\n\n\nQ. 4.16Is there a discrepancy between the population parameters and the sample statistics? Does the discrepency seem greater for the mean or the standard deviation?\n\n\n\n\n\n\nQ. 4.17How does this distribution relate to the T-statistic calculated above? The P-value?\n\n\n\n\nQ. 4.18Explore different values for sample_size below to see how the shapes change.\n\n\n\n\n\n\n\n\n\n\nWhen we perform a t-test, we compare the T-statistic from our sample to this distribution: the distribution of T-statistics we would expect under the hypothesized \\(\\mu\\). The P-value gives the probability of our T-statistic (or one more extreme) under the hypothesized \\(\\mu\\). If it is very small, it is very unlikely that our sample would occur with that \\(\\mu\\), and we might conclude the value of \\(\\mu\\) is something different.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The t-distribution and confidence intervals</span>"
    ]
  },
  {
    "objectID": "P4_t_CIs.html#confidence-intervals",
    "href": "P4_t_CIs.html#confidence-intervals",
    "title": "4  The t-distribution and confidence intervals",
    "section": "\n4.2 Confidence Intervals",
    "text": "4.2 Confidence Intervals\nSay you are interested in knowing the mean of a population (e.g. barnacle mass on the back beach). You cannot afford to determine the mass of each barnacle, so you take a random sample. You don’t know how ‘good’ (i.e. representative) your sample is. It might have included lots of small barnacles, or big ones, or a wide- or narrow-range of sizes. You can never know (unless you sample everything). When you calculate the mean of this sample you don’t know how close it is to the population mean, but you do know the probability associated with that estimate. Confidence intervals capture this uncertainty, and you use the t-distribution to determine them.\nWe’ll invent a population of barnacle diameters, called barnacle_diam. We’ll create a histogram of that population and superimpose values on that. Again, these are random numbers so your values will be slightly different from mine.\n\n# barnacle population meta-details\npop_size &lt;- 100000  # number of barnacles in the population\nbarnacle_mu &lt;- 200\nbarnacle_sigma &lt;- 25\n\n# the full population:\nbarnacle_diam &lt;- rnorm(pop_size, mean = barnacle_mu, sd = barnacle_sigma)\nmu &lt;- mean(barnacle_diam)\nsigma &lt;- sd(barnacle_diam)\n\n# histogram of the population with middle 95% shown\nhist(barnacle_diam, main = NULL)\nQ95 &lt;- quantile(barnacle_diam, c(0.025, 0.975))\nabline(v = Q95, col = \"green\", lwd = 3)\ntext(x=Q95[2], y=pop_size/7, \n     labels=paste0(\"mu: \", round(mu, 1), \"\\nsigma:\", round(sigma, 1)))\n\n\n\n\n\n\nFigure 4.5: Histogram of a simulated barnacle population with 2.5% and 97.5% quantiles.\n\n\n\n\nNow we can take samples from that population. This is reality: you take samples from populations where you don’t know the population mean and standard deviation. Let’s take 4 samples, each with size sample_size.\n\nhist(barnacle_diam, main = NULL)\nsample_size &lt;- 5\nnum_samples &lt;- 4\nabline(v = mu, col = \"blue\", lwd = 4)\n\nfor (i in 1:num_samples) {\n  sample_i &lt;- sample(barnacle_diam, size = sample_size)\n  print(sample_i)\n  abline(v = mean(sample_i), col = \"red\", lwd = 0.5)\n}\n\n[1] 252.2560 209.5652 194.1364 180.0893 199.9900\n[1] 181.4844 212.2622 212.0780 205.8330 209.9440\n[1] 214.3341 150.0059 206.0774 194.1674 191.1762\n[1] 168.1111 194.2603 180.3917 178.7273 193.4117\n\n\n\n\n\n\n\nFigure 4.6: Histogram of the barnacle population showing location of 4 sample means (red lines), each with N = 5. The blue line shows the true population mean mu.\n\n\n\n\nOur sample means inevitably differ from the true population mean (\\(\\mu\\)), even if only a bit. Likewise, the sample standard deviations will differ from the true population standard deviation (\\(\\sigma\\)).\nIf we repeat this sampling enough times, we can generate a distribution of sample standard deviations (Figure 4.7). This distribution is not normal, but is instead related to the chi-square distribution (don’t worry too much about this). The point is that if your sample size is small, your estimate of the standard deviation is often very poor.\n\nCodepar(mfrow=c(2,2))\n# sim_df will hold the sample sizes N, and the median and mean sample sd's \nnum_samples &lt;- 1e4\nsim_df &lt;- data.frame(N=c(2, 4, 10, 30),\n                     sd_median=NA, sd_mean=NA,\n                     sd_q025=NA, sd_q25=NA, sd_q75=NA, sd_q975=NA,\n                     mn_median=NA, mn_mean=NA,\n                     mn_q025=NA, mn_q25=NA, mn_q75=NA, mn_q975=NA) \n\n# for each sample size N, draw a sample and store its mean and sd\n# repeat this num_samples times\n# plot a histogram of the sample sd's, then store the mean and median\nfor (i in 1:nrow(sim_df)) {\n  samp_sd_i &lt;- numeric(num_samples) \n  samp_mn_i &lt;- numeric(num_samples) \n  for (j in 1:num_samples) { \n    sample_ij &lt;- sample(barnacle_diam, size = sim_df$N[i])\n    samp_sd_i[j] &lt;- sd(sample_ij)\n    samp_mn_i[j] &lt;- mean(sample_ij)\n  }\n  hist(samp_sd_i, main = paste(num_samples, \"sample SDs for N =\", sim_df$N[i]), \n       breaks = 20, xlim = c(0, 100))\n  abline(v = sigma, col = \"blue\", lwd = 2)\n  sim_df[i, 2:13] &lt;- c(median(samp_sd_i), mean(samp_sd_i),\n                       quantile(samp_sd_i, probs = c(0.025, 0.25, 0.75, 0.975)),\n                       median(samp_mn_i), mean(samp_mn_i),\n                       quantile(samp_mn_i, probs = c(0.025, 0.25, 0.75, 0.975)))\n}\n\n\n\n\n\n\nFigure 4.7: Histograms of sample standard deviations from repeated samples of the same barnacle population. The true population standard deviation is shown in blue.\n\n\n\n\n\nCodepar(mfrow=c(1,2))\nplot(sim_df$N, sim_df$sd_median,\n  xlim = c(0, 30), ylim = range(c(sim_df[,2:7], sigma)),\n  type = \"b\", xlab = \"Sample size\", ylab = \"Standard deviation\"\n)\nsegments(sim_df$N, sim_df$sd_q25, sim_df$N, sim_df$sd_q75, lwd=2)\nsegments(sim_df$N, sim_df$sd_q025, sim_df$N, sim_df$sd_q975)\nlines(sim_df$N, sim_df$sd_mean, type = \"b\", col = \"dodgerblue\")\nabline(h = sigma, lty = 2)\nlegend(\"topright\", c(\"Mean sample SD\", \"Median sample SD\", \"True SD\"),\n  col = c(\"black\", \"dodgerblue\", \"black\"),\n  lty = c(1, 1, 2), pch = c(1, 1, NA), bty = \"n\"\n)\nplot(sim_df$N, sim_df$mn_median,\n  xlim = c(0, 30), ylim = range(c(sim_df[,8:13], mu)),\n  type = \"b\", xlab = \"Sample size\", ylab = \"Mean\"\n)\nsegments(sim_df$N, sim_df$mn_q25, sim_df$N, sim_df$mn_q75, lwd=2)\nsegments(sim_df$N, sim_df$mn_q025, sim_df$N, sim_df$mn_q975)\nlines(sim_df$N, sim_df$mn_mean, type = \"b\", col = \"dodgerblue\")\nabline(h = mu, lty = 2)\nlegend(\"topright\", c(\"Mean sample mean\", \"Median sample mean\", \"True mean\"),\n  col = c(\"black\", \"dodgerblue\", \"black\"),\n  lty = c(1, 1, 2), pch = c(1, 1, NA), bty = \"n\"\n)\n\n\n\n\n\n\nFigure 4.8: Mean (black), median (blue), and 50% and 95% quantiles (vertical lines) for (left) sample standard deviations at each sample size compared to the true population standard deviation (dotted line) or for the (right) sample means.\n\n\n\n\n\nQ. 4.19Does the difference between the mean (black) and median (blue) in Figure 4.8 match your expectations based on the shape of the distributions in Figure 4.7?\n\n\nThe t-distribution allows for the fact that the standard deviation of small samples is usually less than that of the population as seen in Figure 4.7 and Figure 4.8.\nThe take home message here is that when we sample from a population with unknown \\(\\mu\\) and \\(\\sigma\\), we won’t know how ‘accurate’ the sample is, but we do know how your random samples ‘behave’ - they are modelled using the t-distribution. From this knowledge, we can build a 95% confidence interval which is described as an interval which, if repeated for many samples, would include \\(\\mu\\) within its boundaries in 95% of those samples. Read that again. You don’t have knowledge of the true value of the mean or sd (as you did for Z score calculations) and the t-distribution accounts for this uncertainty.\n\n\n\n\n\n\nA 95% confidence interval is the interval that, when calculated on infinite repeated samples, contains the population mean for 95% of those samples (and thus misses the population mean in 5% of the samples).\n\n\n\nWe can modify Figure 4.6 by adding 95% confidence intervals for each sample from our barnacle population.\n\nnum_samples &lt;- 5\nsample_size &lt;- 3\n\n# plot population\nhist(barnacle_diam, \n     xlim = c(barnacle_mu-6*barnacle_sigma, barnacle_mu+6*barnacle_sigma), \n     ylim = c(0, length(barnacle_diam)/6),\n     main = NULL, \n     col = \"grey90\", border = \"grey50\", xlab = \"Barnacle diameter\")\nabline(v = mu, col = \"blue\", lwd = 2)\ny_pos &lt;- seq(0, length(barnacle_diam)/6, length.out=num_samples)\n\n# draw samples, calculate mean and 95% CIs, and plot them\nfor (i in 1:num_samples) {\n  sample_i &lt;- sample(barnacle_diam, size = sample_size)\n  points(x = mean(sample_i), y = y_pos[i], col = \"red\", pch = 16, cex = 0.75)\n  sample_ci &lt;- c(\n    mean(sample_i) + qt(0.025, (sample_size - 1)) * (sd(sample_i) / sqrt(sample_size)),\n    mean(sample_i) + qt(0.975, (sample_size - 1)) * (sd(sample_i) / sqrt(sample_size))\n  )\n  arrows(sample_ci[1], y_pos[i], sample_ci[2], y_pos[i], \n         col = \"red\", code = 3, angle = 90, length=0.05)\n}\n\n\n\n\n\n\nFigure 4.9: Histogram illustrating the barnacle population with population mean (blue) and sample means with 95% CIs (red) repeated across 5 samples.\n\n\n\n\n\nQ. 4.20Keep repeating the above code until you get an example where your 95% CI misses the true value of the mean.\n\n\n\nQ. 4.21Try different values for sample_size. How does this influence the width of your CIs?\n\n\n\nQ. 4.22What proportion of your 95% CIs would you expect to include the true value of the mean? Does that change for different values of sample_size?\n\n\n\nQ. 4.23Find the relevant bit of the code and determine 99% CIs, then 79% CIs. Note that you may need to adjust the x-axis limits which are set to 6 \\(\\sigma\\) on either side of \\(\\mu\\) in hist()).\n\n\nIn the above code, we calculated CIs manually using qt(). We can instead simply use t.test() and specify whatever confidence level we like:\n\nbarnacle_sample &lt;- sample(barnacle_diam, size=10)\nexamp_ttest &lt;- t.test(barnacle_sample, conf.level=0.89) # 89% CIs\nexamp_ttest\n\n\n    One Sample t-test\n\ndata:  barnacle_sample\nt = 35.493, df = 9, p-value = 5.533e-11\nalternative hypothesis: true mean is not equal to 0\n89 percent confidence interval:\n 196.7245 217.4111\nsample estimates:\nmean of x \n 207.0678 \n\n# Or just the confidence intervals:\nexamp_ttest$conf.int\n\n[1] 196.7245 217.4111\nattr(,\"conf.level\")\n[1] 0.89\n\n\nLet’s explore the influence of sample size on the width of the confidence interval a little more.\nFor sample sizes of 2, 5, and 10, we will collect a sample and calculate the 95% CIs with t.test(). As above, we will repeat our sampling a few times to assess the variability among samples of the same size.\n\nCodenum_samples &lt;- 10\nsample_sizes &lt;- c(2, 5, 10)\n\nCI_df &lt;- expand_grid(N=sample_sizes,\n                     sample_id=1:num_samples) |&gt;\n  rowwise() |&gt;\n  mutate(obs=list(sample(barnacle_diam, N)),\n         mn=mean(obs),\n         ci_lo=t.test(obs)$conf.int[1],\n         ci_hi=t.test(obs)$conf.int[2]) |&gt;\n  ungroup() |&gt;\n  mutate(N=factor(N, levels=unique(N), labels=paste(\"N =\", sample_sizes)))\nggplot(CI_df, aes(mn, sample_id, xmin=ci_lo, xmax=ci_hi, colour=N)) +\n  geom_vline(xintercept=barnacle_mu, colour=\"cadetblue\") + \n  geom_point(size=1.5) + \n  geom_errorbarh(height=0.15) + \n  facet_wrap(~N, ncol=1, scales=\"free_y\", strip.position=\"left\") + \n  labs(x=\"Sample mean and 95% confidence interval\") +\n  theme_bw() + \n  theme(axis.text.y=element_blank(), \n        axis.ticks.y=element_blank(),\n        axis.title.y=element_blank(),\n        legend.position=\"none\",\n        panel.grid.minor=element_blank())\n\n\n\n\n\n\nFigure 4.10: Confidence interval size vs. sample size.\n\n\n\n\n\nQ. 4.24What do you notice about the average width of CIs in Figure 4.10 as sample size changes? What about the variability in width?\n\n\n\nQ. 4.25What proportion of 95% CIs would you expect to include \\(\\mu\\) for N=2? For N=100?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The t-distribution and confidence intervals</span>"
    ]
  },
  {
    "objectID": "P4_t_CIs.html#comparing-means-two-sample-t-tests",
    "href": "P4_t_CIs.html#comparing-means-two-sample-t-tests",
    "title": "4  The t-distribution and confidence intervals",
    "section": "\n4.3 Comparing means (two-sample t tests)",
    "text": "4.3 Comparing means (two-sample t tests)\nThe two-sample t-test is a widely used inferential statistical test. The two-sample t-test is a special case of analysis of variance (ANOVA) where there are only two groups. The results are identical and so we only mention its existence. It is good to be aware of the two-sample t-test because it is so commonly used, but you will be comparing means using ANOVA in Chapter 5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The t-distribution and confidence intervals</span>"
    ]
  },
  {
    "objectID": "P4_t_CIs.html#non-parametric-tests",
    "href": "P4_t_CIs.html#non-parametric-tests",
    "title": "4  The t-distribution and confidence intervals",
    "section": "\n4.4 Non-parametric Tests",
    "text": "4.4 Non-parametric Tests\nParametric tests are so named because they estimate population parameters. Non-parametric tests are often used to compare samples where the data are non-continuous or fail the assumptions of parametric general linear models, typically converting data to ranks for analysis rather than using the actual values. Non-parametric test include ‘classics’ such as the ‘Mood’ and ‘Wilcoxon’ tests. However, we make you aware of the GLM family which will usually supply you with a much more elegant solution to model data that doesn’t fit the simple linear model. You should be aware of the existence of ‘non-parametric’ tests because they are prevalent in the literature. Remind yourself of the disadvantages of non-parametric tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The t-distribution and confidence intervals</span>"
    ]
  },
  {
    "objectID": "P4_t_CIs.html#conclusions",
    "href": "P4_t_CIs.html#conclusions",
    "title": "4  The t-distribution and confidence intervals",
    "section": "\n4.5 Conclusions",
    "text": "4.5 Conclusions\nThe t-test is a ‘classic’ statistical test which doesn’t assume knowledge of population parameters. The strength of the t-test (its ability to quantify differences between samples) is proportional to the sample size. The larger the sample size, the better the estimate of the population parameters and the more precisely we are to be able to detect differences between the means.\nThe central limit theorem tells us that the means of non-normally distributed data will be normally distributed if the sample size is sufficiently large. If your sample size is \\(&gt;\\) 30 it is likely that the means of that sample will be normally distributed regardless of the distribution of the original data.\nParametric tests including the t-test are quite ‘robust’ against deviations from normality, particularly as sample sizes increase. However, parametric test are less robust against heteroscedasticity, regardless of sample size. Always check this assumption and be prepared to transform the data if the assumption of homoscedasticity is not tenable (more of this in Chapter 5).\nThe t-test is in the ‘general linear model’ family (which is a subset of the generalized linear modelling family). General linear models are usually used to model continuous data where residual error is approximately normal. If you have count data, you should start with a different member of the GLM family before trying transformations to acheive approximate normality. Non-parametric tests are frequently adopted when data do not conform to the assumptions of normality but they are invariably used for NHST with all the inherent problems with that approach.\nA final reminder with regard to many statistical tests, including all in the GLM family: they make the assumption that data are independent. You must always ensure that your experimental design lends itself to making independent observations in relation to the question you are asking. This is the most critical and fundamental of the assumptions of parametric and non-parametric tests. Non-independence (e.g. measuring the same urchin over time) can be modelled using more complex ‘mixed’ models. Application of mixed modelling is beyond this course but you should be aware of the limitations of the techniques that you are learning and know where to go next.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The t-distribution and confidence intervals</span>"
    ]
  },
  {
    "objectID": "P5_ANOVA_regression.html",
    "href": "P5_ANOVA_regression.html",
    "title": "5  ANOVA and regression",
    "section": "",
    "text": "5.1 Analysis of variance (ANOVA)\nGeneral linear models are a key member of the generalized linear modelling (GLM) family and they are among the most widely used models in the marine science literature, particularly biology. This course focuses on two subsets of linear models: ANOVA and regression. ANOVA and regression are typically used for different data modelling scenarios: ANOVA when the predictor is categorical and regression when the predictor is continuous.\nANOVA is a method to compare means among groups and put confidence intervals on the differences between those means. For a predictor with only two categories, ANOVA is identical to the two-sample t-test, so we’ll just use ANOVA.\nIn an ANOVA, we compare the means of different groups by analyzing the variance (Figure 5.1). The variance is partitioned into 1) the mean variance between treatment means and the overall mean (thick arrows), and 2) the mean variance within treatments (thin arrows). By comparing the magnitude of these two quantities, we draw conclusions about whether the difference between group means is plausibly due to noise or not.\nCodelibrary(tidyverse)\ntheme_set(theme_classic())\n# simulate two populations A and B\nsigma &lt;- 1.5\nmu_df &lt;- data.frame(Group=c(\"A\", \"B\"),\n                    Lab=c(\"mu[A]\", \"mu[B]\"),\n                    mu=c(12, 17)) |&gt;\n  mutate(density=dnorm(mu, mu, sigma))\n\npop_df &lt;- data.frame(value=seq(min(mu_df$mu) - 3*sigma, \n                               max(mu_df$mu) + 3*sigma, \n                               length.out=1e3)) |&gt;\n  mutate(A=dnorm(value, mu_df$mu[1], sigma),\n         B=dnorm(value, mu_df$mu[2], sigma)) |&gt;\n  pivot_longer(2:3, names_to=\"Group\", values_to=\"density\")\nx_lim &lt;- xlim(min(pop_df$value), max(pop_df$value))\ny_lim &lt;- ylim(-max(pop_df$density)*0.1, max(pop_df$density)*1.1)\n\n# simulate sampling\nN &lt;- 10\nset.seed(1)\nsamp_df &lt;- data.frame(Group=rep(c(\"A\", \"B\"), each=N),\n                      y=c(rnorm(N, mu_df$mu[1], sigma),\n                          rnorm(N, mu_df$mu[2], sigma))) |&gt;\n  mutate(ypos=seq(max(mu_df$density)*0.05, max(mu_df$density)*0.6, length.out=n())) |&gt;\n  group_by(Group) |&gt;\n  mutate(y_bar=mean(y),\n         y_bar_pos=mean(ypos)) |&gt;\n  ungroup() |&gt;\n  mutate(y_bar_lab=max(ypos)*1.1,\n         y_bar_bar=mean(y),\n         Lab=paste0(\"bar(y)[\", Group, \"]\"))\n\n# plot\nggplot(samp_df, aes(y, colour=Group)) + \n  # population in background\n  geom_hline(yintercept=0, linewidth=0.2, colour=\"grey80\") +\n  geom_line(data=pop_df, aes(value, density), linewidth=1, alpha=0.25) + \n  geom_text(data=mu_df, \n            aes(x=mu, y=-density*0.05, label=Lab), \n            vjust=1, parse=T, size=6, alpha=0.5) +\n  geom_segment(data=mu_df, \n               aes(x=mu, xend=mu, y=-density*0.05, yend=0), \n               linewidth=2, linetype=1, alpha=0.5) +\n  # observations\n  geom_point(aes(y, ypos), size=2.5, shape=1, stroke=1) +\n  # group means\n  geom_segment(aes(x=y_bar, xend=y_bar, yend=0, y=y_bar_lab), linewidth=0.75, linetype=2) +\n  geom_text(aes(x=y_bar, y=y_bar_lab, label=Lab), nudge_y=0.02, parse=T, size=6) +\n  # grand mean\n  geom_segment(aes(x=y_bar_bar, xend=y_bar_bar, yend=0, y=y_bar_lab), colour=\"black\",\n               linewidth=0.75, linetype=2) +\n  annotate(\"text\", x=samp_df$y_bar_bar[1], y=samp_df$y_bar_lab[1]+0.02, label=\"bar(bar(y))\", \n           parse=T, size=6, colour=\"black\") +\n  # within-group variation\n  geom_segment(aes(x=y_bar, xend=y, y=ypos, yend=ypos), \n               arrow=arrow(ends=\"both\", length=unit(0.05, \"inches\")), colour=\"black\") +\n  # among-group variation\n  geom_segment(aes(x=y_bar, xend=y_bar_bar, y=y_bar_lab*0.9, yend=y_bar_lab*0.9), \n               arrow=arrow(ends=\"both\", length=unit(0.1, \"inches\")), \n               colour=\"black\", linewidth=1) +\n  scale_colour_manual(values=c(\"steelblue2\", \"firebrick\"), guide=\"none\") +\n  x_lim + y_lim + \n  labs(x=\"Response variable\", y=\"\")\n\n\n\n\n\n\nFigure 5.1: Sources of variation (within group vs. among groups) as quantified by ANOVA. Points are observations, dashed vertical lines give sample means for A (blue), B (red), and overall (black). Thick arrows show differences between group means and the overall mean, and thin arrows show differences between observations and group means. The population means (mu[A], mu[B]) and the effect size (mu[B] - mu[A]) are unknown and must be inferred from the sample with confidence intervals.\nThe null hypothesis in an ANOVA is that there is no difference in group means: \\(\\mu_A = \\mu_B = ... = \\mu_k\\). Under the null hypothesis, the data come from a single population and our groupings are meaningless with regard to the response variable.\nIn some cases, this may be of interest. What is almost always more interesting, however, is estimating means, differences between means, and our confidence in those.\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(emmeans)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA and regression</span>"
    ]
  },
  {
    "objectID": "P5_ANOVA_regression.html#analysis-of-variance-anova",
    "href": "P5_ANOVA_regression.html#analysis-of-variance-anova",
    "title": "5  ANOVA and regression",
    "section": "",
    "text": "When you see “analysis of variance”, think “analysis of means”. The variance in the data is partitioned in different ways to draw conclusions about the means.\n\n\n\n\n\n\n\n\n\n5.1.1 ANOVA in R\nThere are numerous variations on the theme of ANOVA. We cover one-way ANOVA and we mention two-way ANOVA. The objective of ANOVA is to establish the size of the difference (called the ‘effect size’) between different groups (e.g., treatments or locations) and put a confidence interval on those differences.\n\n5.1.2 One-way ANOVA\nOne-way ANOVA is a procedure we use to estimate the magnitude of differences between means of two or more groups. We also use it to put confidence intervals on those differences.\nThe first example data is the yield in \\(\\mu g ~ C ~ ml^{-1}\\) of a species of microalgae (Isochrysis galbana) in laboratory culture exposed to three light levels (low, medium, high). We are interested in these particular light levels because they represent the means of winter, spring, and summer Scottish sun intensity. The data are in the worksheet ‘Microalgae’.\n\nalgae_wide_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"Microalgae\")\n\nCheck these data as usual.\n\nQ. 5.1What is your objective in this type of experiment? What are you interested in estimating?\n\n\n\nQ. 5.2What assumptions should be met prior to undertaking parametric ANOVA?\n\n\n\nQ. 5.3Under which circumstances could you begin to relax the assumption that the data are normally distributed (think central limit theorem)?\n\n\n\nQ. 5.4What is the sample size in this case? Can we assume sample means will be normally distributed?\n\n\n\nQ. 5.5Are the data normally distributed? Be careful how you word your answer to this question.\n\n\n\nQ. 5.6Is it reasonable to assume that these data are drawn from a population that is normally distributed?\n\n\nTo work with our data, we need to rearrange the data.frame to a tidy format with each variable corresponding to one column, and each observation corresponding to one row. We’ll use the tidyverse as before.\n\nhead(algae_wide_df, 2)\n\n# A tibble: 2 × 3\n    low medium  high\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  13.1   12    14.2\n2  11.5   11.5  13.1\n\nalgae_df &lt;- algae_wide_df |&gt;\n  pivot_longer(everything(), names_to = \"Treatment\", values_to = \"Yield\")\nglimpse(algae_df)\n\nRows: 15\nColumns: 2\n$ Treatment &lt;chr&gt; \"low\", \"medium\", \"high\", \"low\", \"medium\", \"high\", \"low\", \"me…\n$ Yield     &lt;dbl&gt; 13.07599, 12.00000, 14.20000, 11.53923, 11.50000, 13.10000, …\n\n\n\nQ. 5.7What do you notice about the variable types? Prepare algae_df$Treatment for analysis and plotting.\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\nThe column should be a factor with the levels in a logical order. Remember that unless you specify the levels, R will order them alphabetically. In this case, that would be \"high\", \"low\", \"medium\".\n\nCodealgae_df$Treatment &lt;- factor(algae_df$Treatment, \n                             levels=c(\"low\", \"medium\", \"high\"))\nglimpse(algae_df)\n\nRows: 15\nColumns: 2\n$ Treatment &lt;fct&gt; low, medium, high, low, medium, high, low, medium, high, low…\n$ Yield     &lt;dbl&gt; 13.07599, 12.00000, 14.20000, 11.53923, 11.50000, 13.10000, …\n\nCodeggplot(algae_df, aes(Treatment, Yield)) + \n  geom_boxplot() +\n  geom_point(shape=1)\n\n\n\n\n\n\nFigure 5.2: Boxplot of algal yield at different light levels.\n\n\n\n\n\n\n\nWhile there are formal tests to evaluate model assumptions (e.g., the Shapiro-Wilkes test or the Bartlett test), a better way of checking model assumptions is to investigate the residual error. Residual error is the difference between the actual data values and the values predicted by the model. Here we have randomly assigned five cultures each of the same species to three specific treatments (light levels).\n\n\n\n\n\n\nResidual error is the leftover noise: the difference between your model prediction and each observation.\nSampling error is the error due to the sampling process: the difference between your model prediction and the true population value.\n\n\n\nNext to conduct the analysis. This is a one-way ANOVA (one predictor) with fixed effects (we are interested in the differences between specific groups rather than characterizing the variation among groups generally).\n\nalgae_aov &lt;- aov(Yield ~ Treatment, data = algae_df) # ?aov\n\n\n\n\n\n\n\nThe function to conduct an ANOVA is aov(). The function anova() converts various statistical model outputs to the standard ANOVA-table output, including any from the GLM family. An ANOVA table can be produced with aov(...) or anova(lm(...)), but post-hoc analysis requires aov().\n\n\n\nBefore we look at the output, let’s assess the assumptions using the residuals. Rather than using qqnorm() as in Chapter 3, it is better to use plot(aov_object). These default residual plots (Figure 5.3) enable us to rapidly assess whether the model assumptions are reasonable. Remember, the assumptions to assess here are:\n\nThe residual error is normally distributed (rather than variable per se, which is what qqnorm() gives).\nThe residual error is homoscedastic, with constant variance across groups.\n\n\npar(mfrow = c(2, 2), mar=c(4,4,1,1)) \nplot(algae_aov) \n\n\n\n\n\n\nFigure 5.3: Residual plots from one-way ANOVA.\n\n\n\n\nInterpretation of residual patterns:\n\n\nUpper left: Residuals v. fitted. This is the residual values against the fitted values. The fitted values are the means of the three groups (remember that ANOVA is about comparing means). The spread for the lower values (low and medium light) is higher than for the high light so this might make us consider the homoscedasticity assumption.\n\n\nUpper right: Normal Q-Q plot. This assesses the normality assumption. The points (each point is an observation) lie around the straight line so this assumption is reasonable. Note that general linear models assume that the means of groups are normally distributed, and this always applies when the means are based on large sample sizes (roughly \\(n &gt; 30\\)). When \\(n &lt; 30\\), you should check that the distribution of the residuals is reasonably ‘normal’.\n\n\nLower left: Scale-location. This specifically looks to assess whether residual magnitude increases with fitted values, which is a common issue in these types of analyses. In this case, the scale decreases with fitted value. This is similar to the Upper Left plot, but with sqrt(abs(standardized_residuals)) on the y-axis instead of just residuals to focus just on the magnitude of the residuals.\n\n\nLower right: Constant leverage, residuals vs. factor levels. This indicates how each treatment is fitted (i.e. the residuals associated with each treatment). You might be concerned if one particular treatment was associated with extremely high residuals (outliers). R automatically identifies potential outliers (8, 11, and 13 in this case) for you to further assess. In this case there is nothing in particular to worry about.\n\nThe residual plots allow you to investigate different aspects of the data and the how their assumptions are met. The interpretation of the plots overlaps in the sense that the same issue might be apparent in several of the plots.\n\nQ. 5.8What are the ‘fitted values’ for an ANOVA?\n\n\n\nQ. 5.9Are your effects fixed or random?\n\n\nEverything looks OK, so we can then look at the results of the ANOVA.\n\n# anova(algae_aov) # outputs an anova-type table, but unnecessary with aov()\nsummary(algae_aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nTreatment    2 10.050   5.025   6.487 0.0123 *\nResiduals   12  9.296   0.775                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nQ. 5.10Assuming you have chosen \\(\\alpha = 0.05\\), what do you conclude? What might you be interested in going on to test next?\n\n\nReporting that there are ‘significant’ differences between means is not enough. What your readers should be interested in is what the differences between the means actually are, and how confident you are in your assessment. This can be provided by the Tukey HSD test.\n\n# HSD stands for 'honestly significant difference'\nalgae_grp_diffs &lt;- TukeyHSD(x = algae_aov, conf.level = 0.95) \nalgae_grp_diffs\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Yield ~ Treatment, data = algae_df)\n\n$Treatment\n                   diff        lwr      upr     p adj\nmedium-low  -0.09077539 -1.5758551 1.394304 0.9854641\nhigh-low     1.68922461  0.2041449 3.174304 0.0260570\nhigh-medium  1.78000000  0.2949203 3.265080 0.0194474\n\nplot(algae_grp_diffs)\n\n\n\n\n\n\nFigure 5.4: Tukey HSD plot from a one-way ANOVA.\n\n\n\n\nThe mean yield under high light is significantly higher than under both low light (mean difference [95% CI]: 1.69 [0.204-3.17] \\(\\mu g ~ C ~ml^{-1}\\); p=0.03) and medium light (1.78 [0.295-3.27] \\(\\mu g ~ C ~ml^{-1}\\), p=0.02). There was no significant difference between low and medium light levels (p=0.99).\n\nQ. 5.11In the output from TukeyHSD(), why is the first value in diff negative?\n\n\nWe of course also want to report the group means with confidence intervals. While we could calculate these individually as in Chapter 4, the simplest way is to use the emmeans package. This has the added benefit of using our model’s assumption of homoscedasticity in calculating the standard error.\nThe emmeans() function can correctly calculate means and confidence intervals for complex models (?emmeans). Our model is simple, so we only need to provide our aov object and the name of the predictor variable (specs=...). The confidence level is specified with level=..., which is set to 0.95 by default.\n\nalgae_emm &lt;- emmeans(algae_aov, specs=\"Treatment\", level=0.89)\nalgae_emm\n\n Treatment emmean    SE df lower.CL upper.CL\n low         12.1 0.394 12     11.4     12.8\n medium      12.0 0.394 12     11.3     12.7\n high        13.8 0.394 12     13.1     14.5\n\nConfidence level used: 0.89 \n\n\n\n5.1.3 One-way ANOVA (reprise)\nImport the ‘LimpetDist’ sheet from “H2DS_practicalData.xlsx”, which gives travel distance of limpets on three different surfaces. Perform a one-way ANOVA of distance predicted by surface type. Make sure you perform appropriate data quality checks and assumption checks. Report the ANOVA table and the pairwise effect sizes with confidence intervals.\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\n\n\n\n\n\nView step 1\n\n\n\n\n\nFirst load and prepare the data.\n\nCodelimpet_wide_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"LimpetDist\")\nlimpet_df &lt;- limpet_wide_df |&gt;\n  pivot_longer(everything(), names_to=\"Surface\", values_to=\"Distance\") |&gt;\n  mutate(Surface=factor(Surface, levels=c(\"Smooth\", \"Intermediate\", \"Rough\")))\nglimpse(limpet_df)\n\nRows: 24\nColumns: 2\n$ Surface  &lt;fct&gt; Smooth, Intermediate, Rough, Smooth, Intermediate, Rough, Smo…\n$ Distance &lt;dbl&gt; 97, 66, 55, 83, 98, 49, 90, 78, 41, 93, 70, 64, 71, 89, 69, 1…\n\nCodeggplot(limpet_df, aes(Surface, Distance)) + \n  geom_boxplot() +\n  geom_point(shape=1)\n\n\n\n\n\n\nFigure 5.5: Boxplot of limpet travel distances by surface.\n\n\n\n\n\n\n\n\n\n\n\n\n\nView step 2\n\n\n\n\n\nSecond, run the ANOVA.\n\nCodelimpet_aov &lt;- aov(Distance ~ Surface, data=limpet_df)\n\n\n\n\n\n\n\n\n\n\n\nView step 3\n\n\n\n\n\nThird, check assumptions.\n\nCodepar(mfrow = c(2, 2), mar=c(4,4,1,1)) \nplot(limpet_aov) \n\n\n\n\n\n\nFigure 5.6: Residual plots from the limpet one-way ANOVA.\n\n\n\n\nIf the assumptions seem invalid, try transformations. Once satisfactory, continue.\n\n\n\n\n\n\n\n\n\nView step 4\n\n\n\n\n\nIn this case, the residuals show that the assumptions of normally distributed residuals with homoscedasticity among groups are reasonable. No transformation is needed.\nFourth, produce the ANOVA table and evaluate the null hypothesis.\n\nCodesummary(limpet_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nSurface      2   4478  2239.1   18.99 1.95e-05 ***\nResiduals   21   2476   117.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you failed to reject \\(H_0\\), report this and stop. Otherwise, continue.\n\n\n\n\n\n\n\n\n\nView step 5\n\n\n\n\n\nWe’ve rejected the omnibus null hypothesis, so we proceed with pairwise comparisons to determine which means are different from each other, by how much, and how confident we are in that difference. We also of course calculate means for each group with CIs.\nFifth, calculate group means with CIs and perform a Tukey HSD test.\n\nCodeemmeans(limpet_aov, specs=\"Surface\")\n\n Surface      emmean   SE df lower.CL upper.CL\n Smooth         89.8 3.84 21     81.8     97.7\n Intermediate   75.1 3.84 21     67.1     83.1\n Rough          56.4 3.84 21     48.4     64.4\n\nConfidence level used: 0.95 \n\nCodelimpet_grp_diffs &lt;- TukeyHSD(x = limpet_aov) \nlimpet_grp_diffs\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Distance ~ Surface, data = limpet_df)\n\n$Surface\n                       diff       lwr         upr     p adj\nIntermediate-Smooth -14.625 -28.31037  -0.9396343 0.0348495\nRough-Smooth        -33.375 -47.06037 -19.6896343 0.0000123\nRough-Intermediate  -18.750 -32.43537  -5.0646343 0.0064480\n\nCodeplot(limpet_grp_diffs)\n\n\n\n\n\n\nFigure 5.7: Tukey HSD plot from a one-way ANOVA.\n\n\n\n\nReport the results. In a full report or paper, this should include the ANOVA table, group means and CIs, effect sizes and CIs, and likely a figure communicating the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA and regression</span>"
    ]
  },
  {
    "objectID": "P5_ANOVA_regression.html#regression",
    "href": "P5_ANOVA_regression.html#regression",
    "title": "5  ANOVA and regression",
    "section": "\n5.2 Regression",
    "text": "5.2 Regression\nCorrelation and regression are used to examine the strength of association between two variables. In correlation, both variables are measured (and therefore associated with measurement error). In regression, one variable is fixed (by the experimenter) and is assumed to have no ‘error’ associated with it and the other, called the ‘response variable’, is measured (so has measurement error). You must be able to distinguish whether correlation or regression analyses are most appropriate for a given research question and design.\nCorrelation analysis is used to measure association, where you are not attempting to formally link cause-and-effect. Regression analysis is generally used where you have experimentally manipulated the fixed factor and are looking at the response in another factor. Causation is implicit in inferential regression analysis (correlation analysis is often used in ‘exploratory’ data analysis where any link between cause-and-effect is inherently more speculative).\nThe media often misreport science because it is difficult to resist the impulse to attribute causation. An overwhelming number of spurious correlations (i.e., those clearly having no causal relationship) are documented on tylervigen.com.\n\n5.2.1 Overview\nRegression is at the heart of linear models. ANOVA and t-tests are, basically, special cases of linear regression models. The regression coefficient is a measure of the strength of the relationship between the dependent variable (the one you measure) and the independent variable (the one you fix like a fixed factor in ANOVA). The regression coefficient is denoted by \\(R^2\\) compared with \\(r\\) in correlation. The regression coefficient \\(R^2\\) ranges from 0 to 1 (unlike \\(r\\) which ranges from -1 to 1). A value \\(R^2 = 0\\) indicates no relationship to the independent variable while \\(R^2=1\\) indicates that the independent variable is entirely responsible for the variability in the measured variable.\nAs usual, null hypothesis significance testing is often applied to regression statistics. As usual, the null hypothesis being tested is usually “there is no functional relationship between the response and the predictor” and this is usually conceptually nonsense. In conducting regression analysis, your objective is to quantify to the most appropriate precision and accuracy possible the relationship between X (the aspect you control, the predictor, plotted on the X axis) and Y (the variable you measure, the response, plotted on the Y axis). Your objective is to quantify this relationship, put confidence intervals on it, and then interpret your findings in relation to the objectives of the study and in relation to other research.\n\nQ. 5.12What does the plot look like when there is no relationship between the predictor and the response?\n\n\nLet us now consider an example in which cause and effect does exist. The data in worksheet ‘Beetles’ in practical_6.xlsx shows the weight loss in Tribolium confusum, the confused flour beetle, at different relative humidities (data from Sokal and Rohlf, 1995). The relative humidity (RH) to which the beetles are exposed can be fixed and the weight loss (via evaporative losses) of the beetles then assessed. There is no way that the null hypothesis can be true in this case: humidity will obviously influence weight loss in beetles.\n\nQ. 5.13In this case, what is your response variable (what are you measuring) and your predictor (i.e. what is it that you are manipulating to determine the extent of the response)?\n\n\n\nQ. 5.14Plot the data in R and check your prediction. In this case, the predictor must be displayed on the x-axis and the response must be on the y-axis.\n\n\nWe are interested in whether the whole data set can be usefully represented by a linear regression relationship. We wish to estimate the relationship, and put a confidence interval on our estimate. Common sense tells us that there is some sort of relationship (testing a null hypothesis is not very useful) but it might go in either direction (positive or negative) and we don’t know the strength (i.e. slope) of that relationship.\n\n5.2.2 Linear regression in R\nIn R we can use a variety of techniques to conduct linear regression. The easiest is to use lm(). It is worth noting that lm() would also work for all your other general linear models (e.g. ANOVA). They are, in fact, the same model, it is just the default output (and necessary input formatting) that differs. Try reproducing the ANOVAs above with anova(lm(...)).\nImport data and begin:\n\nbeetle_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"Beetles\")\n# inspect the dataframe, then make a scatter plot\npar(mfrow=c(1,1))\nplot(WeightLoss_Mg ~ Humidity, data = beetle_df) \n\n\n\n\n\n\nFigure 5.8: Beetle weight loss as a function of relative humidity.\n\n\n\n\nAn aside on plotting: you can provide plot() with either a vector for the x-axis and a vector for the y-axis (i.e., plot(x_var, y_var)) or you can use a formula, specifying the dataframe (i.e., plot(y ~ x, data=data_df)). Just be aware of which variable is on which axis.\nNow we have explored and plotted the data we can conduct the regression analysis.\n\n# weight loss is modelled as (~) a function of humidity\nbeetle_lm &lt;- lm(WeightLoss_Mg ~ Humidity, data = beetle_df) \n# beetle_lm\n# str(beetle_lm) # lm outputs are complex structures\n\nBefore we go on and interpret the model output we need to assess the model assumptions. This is done in the same way as for ANOVA with the same commands.\n\npar(mfrow = c(2, 2), mar=c(4,4,1,1)) # set up 4 in 1 plot.\nplot(beetle_lm) # plot the regression residuals.\n\n\n\n\n\n\nFigure 5.9: Regression diagnostics\n\n\n\n\nThe small sample size here (\\(n=9\\)) makes a proper analysis of the residuals difficult. The plot should be assessed in the same way as for the ANOVA residuals. Basically, any pattern is bad. The upper left (Residuals v Fitted) doesn’t cause any major concern, though the upper right (Normal QQ) indicates a possible problem. Scale-Location (lower left) is difficult to interpret but no obvious pattern is present. The Residuals v. leverage (lower right) indicates a potential issue as well. A point with a large residual (i.e. where it is very different to that expected by the model) and with a high leverage (i.e. at the extreme ends of the predictors range) has a large Cook’s distance and has a disproportionate effect on the slope and intercept. These points should be examined in more detail.\n\nQ. 5.15Which point has the largest Cook’s distance?\n\n\nWe will now proceed to looking at the linear regression analysis results on the basis that the residuals do not raise any concerns.\n\nsummary(beetle_lm)\n\n\nCall:\nlm(formula = WeightLoss_Mg ~ Humidity, data = beetle_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46397 -0.03437  0.01675  0.07464  0.45236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.704027   0.191565   45.44 6.54e-10 ***\nHumidity    -0.053222   0.003256  -16.35 7.82e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2967 on 7 degrees of freedom\nMultiple R-squared:  0.9745,    Adjusted R-squared:  0.9708 \nF-statistic: 267.2 on 1 and 7 DF,  p-value: 7.816e-07\n\n\nThe regression equation of the form \\(y = a + bx\\) can be determined. The regression equation is:\n\\(WeightLoss = 8.70 - 0.05322 * humidity\\)\nCommon-sense check: the coefficient is negative. As the humidity increases, the weight loss decreases (as expected and shown in the scatter plot).\n\nQ. 5.16What is the effect on weight loss of increasing the relative humidity by 10%?\n\n\n\nQ. 5.17What is the weight loss, predicted by the model, when relative humidity is 0%?\n\n\n\nQ. 5.18What does the model suggest the weight loss will be when relative humidity is -50% and +150%? Are these values sensible? What does this tell you about extrapolating beyond the data range in using regression analysis in predictions?\n\n\nThe residual error is the variance in y around the line. The \\(R^2\\) is the proportion of this variance that is explained by the regression line. In the current case \\(R^2 = 0.97\\). This is an extremely high value and indicates that the regression model is extraordinarily good at accounting for the variance in weight loss based on the relative humidity.\nThe P values allow us to assess if the slope and the intercept are likely different from zero.\n\nQ. 5.19Given the very high \\(R^2\\) (and looking at your plot) would you expect the regression model to be significantly better than the null model in explaining the variance in weight loss?\n\n\n\nQ. 5.20With \\(\\alpha = 0.05\\), do you reject or accept the null hypothesis? What would you wish to report in relation to the slope coefficient if you were reporting the results from this analysis?\n\n\n\nconfint(beetle_lm)\n\n                  2.5 %      97.5 %\n(Intercept)  8.25104923  9.15700538\nHumidity    -0.06092143 -0.04552287\n\n\nThe confidence intervals are, again, ‘clunky’ to describe.\nIf we imagine there were many alternate you’s (like in a multiverse) repeating the same experiment on the same population with the same sample size (but independent samples), and each ‘you’ calculated 95% CIs with confint(), then 95% of you would have intervals that include the true population intercept and slope. While you do not know if you are in the unlucky 5% that failed to capture the population values, the 95% confidence interval serves as our best estimate for likely values (but see Bayesian statistics for more intuitive intervals!).\n\n5.2.3 Plotting the regression line and confidence intervals\nA regression model (i.e. the linear relationship between the predictor and response variables) allows us to predict values for any value of the predictor, along with confidence levels. We can plot this regression line without too much effort.\n\nbeetle_pred_line &lt;- predict(beetle_lm, interval = \"confidence\", level = 0.95)\n\npar(mfrow=c(1,1))\nplot(WeightLoss_Mg ~ Humidity, data = beetle_df, ylim=c(3, 10),\n     xlab = \"Relative humidity (%)\", ylab = \"Weight loss (mg)\") \nlines(beetle_df$Humidity, beetle_pred_line[, \"fit\"])\nlines(beetle_df$Humidity, beetle_pred_line[, \"lwr\"], lty = 2)\nlines(beetle_df$Humidity, beetle_pred_line[, \"upr\"], lty = 2)\n\n\n\n\n\n\nFigure 5.10: Regression line (solid) with upper and lower 95% confidence intervals on the regression line (dashed).\n\n\n\n\nTry generating 90% confidence intervals and add them to the plot.\n\nQ. 5.21Which will have the wider interval, a 99.99% interval or a 50% interval and why?\n\n\n\nQ. 5.22Do the confidence intervals in Figure 5.10 run parallel to the regression line?\n\n\n\nQ. 5.23If not, what does this suggest about the degree of confidence you have in values predicted at various points along the line?\n\n\n\nQ. 5.24At what value of relative humidity are your predictions of weight loss likely most accurate?\n\n\nWe can make predictions based on our regression line, and put confidence intervals on those predictions. Say we had a relative humidity of 50% in the above example. You could ask for the model-predicted weight loss and you’d want confidence intervals on that prediction.\n\n# predict() needs a data.frame with the same predictors used in beetle_lm\npredict(beetle_lm, \n        newdata = data.frame(Humidity = 50), \n        interval = \"predict\", \n        level = 0.95)\n\n      fit      lwr      upr\n1 6.04292 5.303471 6.782368\n\n# or more fully:\npredict(beetle_lm, \n        newdata = data.frame(Humidity = seq(0, 100, by=25)), \n        interval = \"predict\", \n        level = 0.95)\n\n       fit      lwr      upr\n1 8.704027 7.868990 9.539064\n2 7.373474 6.608630 8.138317\n3 6.042920 5.303471 6.782368\n4 4.712366 3.949031 5.475701\n5 3.381812 2.549540 4.214084\n\n\nAnd we can plot these intervals too:\n\nnew_humidity_df &lt;- data.frame(Humidity = 0:100)\nbeetle_pred_line &lt;- predict(beetle_lm, \n                            newdata = new_humidity_df,\n                            interval = \"confidence\", \n                            level = 0.95)\nbeetle_pred_obs &lt;- predict(beetle_lm,\n                           newdata = new_humidity_df, \n                           interval = \"prediction\", \n                           level = 0.95)\n\npar(mfrow=c(1,1))\nplot(WeightLoss_Mg ~ Humidity, data = beetle_df, ylim=c(3, 10),\n     xlab = \"Relative humidity (%)\", ylab = \"Weight loss (mg)\") \nlines(new_humidity_df$Humidity, beetle_pred_line[, \"fit\"])\nlines(new_humidity_df$Humidity, beetle_pred_line[, \"lwr\"], lty = 2)\nlines(new_humidity_df$Humidity, beetle_pred_line[, \"upr\"], lty = 2)\nlines(new_humidity_df$Humidity, beetle_pred_obs[, \"lwr\"], lty = 3)\nlines(new_humidity_df$Humidity, beetle_pred_obs[, \"upr\"], lty = 3)\n\n\n\n\n\n\nFigure 5.11: Regression line (solid) with upper and lower 95% confidence intervals on the regression line (dashed) and 95% prediction intervals (dotted).\n\n\n\n\nThese are prediction intervals and they are broader than confidence intervals. The confidence intervals express your confidence about the regression line for the population. The prediction interval expresses your confidence about the distribution of the observations for the population.\n\n5.2.4 Regression (reprise)\nImport the ‘PhosphateCalibration’ sheet from “H2DS_practicalData.xlsx” (1st year practical data) into R and perform a linear regression with absorbance predicted by concentration. Perform all appropriate checks for data quality and assumptions. Report relevant parameter estimates with 92% confidence intervals.\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\n\n\n\n\n\nView step 1\n\n\n\n\n\nFirst load and prepare the data.\n\nCodephosphate_df &lt;- read_xlsx(\"data/H2DS_practicalData.xlsx\", \"PhosphateCalibration\")\nphosphate_df\n\n# A tibble: 6 × 2\n  Concentration Absorbance\n          &lt;dbl&gt;      &lt;dbl&gt;\n1           0        0.001\n2           0.2      0.006\n3           0.5      0.011\n4           1        0.031\n5           2        0.054\n6           3        0.067\n\nCodeplot(Absorbance ~ Concentration, data = phosphate_df)\n\n\n\n\n\n\nFigure 5.12: Phosphate calibration data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nView step 2\n\n\n\n\n\nSecond, run the linear regression.\n\nCodephos.mod &lt;- lm(Absorbance ~ Concentration, data = phosphate_df)\n\n\n\n\n\n\n\n\n\n\n\nView step 3\n\n\n\n\n\nThird, check assumptions.\n\nCodepar(mfrow=c(2,2), mar=c(4,4,1,1))\nplot(phos.mod)\n\n\n\n\n\n\nFigure 5.13: Phosphate calibration regression diagnostics.\n\n\n\n\nIf the assumptions seem invalid, try transformations. Once satisfactory, continue.\n\n\n\n\n\n\n\n\n\nView step 4\n\n\n\n\n\nFourth, view the results and evaluate the null hypothesis.\n\nCodesummary(phos.mod)\n\n\nCall:\nlm(formula = Absorbance ~ Concentration, data = phosphate_df)\n\nResiduals:\n        1         2         3         4         5         6 \n-0.001605 -0.001213 -0.003125  0.005355  0.005314 -0.004726 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.002605   0.002853   0.913 0.412856    \nConcentration 0.023040   0.001849  12.464 0.000238 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.004823 on 4 degrees of freedom\nMultiple R-squared:  0.9749,    Adjusted R-squared:  0.9686 \nF-statistic: 155.3 on 1 and 4 DF,  p-value: 0.0002383\n\n\n\n\n\n\n\n\n\n\n\nView step 5\n\n\n\n\n\nFifth, produce a figure with the best fit line and 92% confidence intervals.\n\nCodenewPhosphate &lt;- tibble(Concentration=seq(min(phosphate_df$Concentration),\n                                         max(phosphate_df$Concentration),\n                                         length.out=100))\nphos_pred_line &lt;- predict(phos.mod, \n                          newdata = newPhosphate,\n                          interval = \"confidence\", \n                          level = 0.92)\n\nplot(Absorbance ~ Concentration, data = phosphate_df)\nlines(newPhosphate$Concentration, phos_pred_line[, \"fit\"])\nlines(newPhosphate$Concentration, phos_pred_line[, \"lwr\"], lty = 2)\nlines(newPhosphate$Concentration, phos_pred_line[, \"upr\"], lty = 2)\n\n\n\n\n\n\nFigure 5.14: Phosphate calibration scatter plot with best fit line and 92% confidence intervals.\n\n\n\n\nReport the results. In a full report or paper, this should include the intercept and slope with CIs, p-value, R2, and likely a figure communicating the results.\n\n\n\n\n\n\nThis is the workflow for performing a linear regression.\n\n\n\n\n\n\nMost often, researchers report 95% CIs, but occasionally 80%, 90%, or 95%. It would be a bit cheeky to report the 92% CIs, but keep in mind that there is nothing magic about 95% (just like 0.05).\n\n\n\n\nQ. 5.25For a concentration of 0.75 units, what values would you expect (95 times in 100) to see from your experimental set-up?\n\n\nYou should get:\n\n\n         fit         lwr        upr\n1 0.01988519 0.005298129 0.03447225",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA and regression</span>"
    ]
  },
  {
    "objectID": "P5_ANOVA_regression.html#conclusions",
    "href": "P5_ANOVA_regression.html#conclusions",
    "title": "5  ANOVA and regression",
    "section": "\n5.3 Conclusions",
    "text": "5.3 Conclusions\nCorrelation is a measure of association between two variables. It is appropriate to use correlation to measure this association when one cannot or does not wish to assume that any relationship is causative. Pearson correlation coefficients should only be used where it is fair to assume (by looking at scatter plot) that the relationship is approximately linear. Where linearity does not apply, attempt to transform one or both of the variables. Where there are outliers (that cannot be removed) or where one is uncertain about some of the data, then non-parametric ranked based correlation coefficients, such as the Spearman coefficient, should be used. As with GLMs, correlation analysis assumes that all points are independent of each other.\nLinear regression is one of the most widely used statistical techniques. It is used to examine causal relationships, often where experimental manipulations are conducted. Regression is a general linear model and it lies within the generalized linear model family (GLMs). GLMs allow you to model data that is not normally distributed, including proportions (bounded by 0 and 1), or counts (bounded by 0). Using a GLM is a much better way of analyzing these data compared with transforming the response variable or using non-parametric techniques. All members of the GLM family make the assumptions that measurements are independent of each other. Where this assumption fails you can use generalized linear mixed models (GLMMs). Extensions of simple linear regression include multiple regression which examines the influence of two or more continuous variables on a response variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ANOVA and regression</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html",
    "href": "P6_machine_learning.html",
    "title": "6  Machine Learning",
    "section": "",
    "text": "6.1 Multivariate analysis\nSo far, every model we’ve used has had a single response variable. Multivariate analyses are techniques that allow the analysis of multiple response variables, such as counts of each species within a community. Statistical routines for multivariate analysis are relatively new and have co-evolved with computational capacity. Multivariate data are typically stored as a matrix of samples (rows) v ‘features’ (columns). Features may include things like faunal counts, chemical concentrations, or environmental conditions.\nMultivariate analysis typically includes three stages:\nEach of these steps has numerous options, with advantages and disadvantages to each. Multivariate analysis are less inferential than most univariate approaches and implementation can feel subjective. Statisticians are still arguing about the best way to approach multivariate analyses.\nMultivariate data occur in a number of common situations, including species inventories, multiple data streams from the same station (e.g., a glider with CTD), and in bioinformatics (e.g., metabarcoding). Many are critical resources in understanding relative change through time.\nMore detail on the the material we cover here is accessible in Clarke et al. (2014) on Brightspace, entitled Change in marine communities: An approach to statistical analysis and interpretation.\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(vegan)\nlibrary(plotly)\nset.seed(2025)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html#multivariate-analysis",
    "href": "P6_machine_learning.html#multivariate-analysis",
    "title": "6  Machine Learning",
    "section": "",
    "text": "Data transformation or standardisation\nCalculation of a dissimilarity matrix\nOrdination (display) of the dissimilarity matrix",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html#non-metric-multidimensional-scaling-nmds",
    "href": "P6_machine_learning.html#non-metric-multidimensional-scaling-nmds",
    "title": "6  Machine Learning",
    "section": "\n6.2 Non-Metric Multidimensional Scaling (NMDS)",
    "text": "6.2 Non-Metric Multidimensional Scaling (NMDS)\nFor the NMDS workflow, we’ll work through the above steps (data |&gt; transform() |&gt; dissimilarity() |&gt; ordination()) first with two simulated datasets and then with a more complex (real) dataset included in the package vegan.\nRecall that the ^ operator raises each element in a vector to a power. For example, for the vector obs_values &lt;- c(1, 10, 35), we can calculate a fourth-root transformation of the whole vector with obs_values^(1/4). Remember from previous courses that this is identical to sqrt(sqrt(obs_values)).\n\n6.2.1 1: Data transformation\n\n# these toy data duplicate those in the multivariate lecture.\nworm_df &lt;- data.frame(\n  row.names = c(\"CE1\", \"CE2\", \"Ref1\", \"Ref2\"),\n  Capitella = c(1000, 1500, 10, 50),\n  Malacoeros = c(500, 2000, 50, 25),\n  Mysella = c(1, 0, 25, 30),\n  Nucella = c(1, 0, 20, 15)\n)\nworm_df\n\n     Capitella Malacoeros Mysella Nucella\nCE1       1000        500       1       1\nCE2       1500       2000       0       0\nRef1        10         50      25      20\nRef2        50         25      30      15\n\n# alt-log transformation: ifelse(x==0, 0, log(x))\nworm_df_log &lt;- decostand(worm_df, method = \"log\", logbase = 10)\nworm_df_4rt &lt;- worm_df^0.25\n\nTake a look at worm_df, worm_df_log, and worm_df_4rt to be sure they make sense.\n\n6.2.2 2: Dissimilarity matrix\nA dissimilarity matrix summarises the dissimilarity between each pair of samples. There are many methods of summarising the dissimilarity (or distance), particularly when that dissimilarity occurs across multiple dimensions (here, genera).\nThe vegdist() function can generate distance matrices using many different methods. See ?vegdist for more information. We’ll use Bray-Curtis for the raw and 4th-root transformed data, and alternative Gower for the alt-log transformed data. See the help page and Brightspace material for more information on these. Feel free to investigate other combinations of data transformation and dissimilarity matrices.\n\nvegdist(worm_df, method = \"bray\")\n\n           CE1       CE2      Ref1\nCE2  0.4002399                    \nRef1 0.9228376 0.9667129          \nRef2 0.9050555 0.9585635 0.3333333\n\nvegdist(worm_df_4rt, method = \"bray\")\n\n            CE1        CE2       Ref1\nCE2  0.18044721                      \nRef1 0.39098221 0.59100112           \nRef2 0.36024122 0.55728021 0.08642723\n\nvegdist(worm_df_log, method = \"altGower\")\n\n           CE1       CE2      Ref1\nCE2  0.6945378                    \nRef1 1.4247425 2.1192803          \nRef2 1.3138181 2.0083559 0.3010300\n\n\n\nQ. 6.1Based on the dissimilarity matrices, which sites are more similar (smaller numbers) and which are more dissimilar (bigger numbers)? Does this align with an ‘eyeballing’ of the data? How has the data transformation changed the resultant dissimilarity matrix?\n\n\n\n6.2.3 3: Ordination\nNext we want to plot the dissimilarities to visualize which sites are more similar or dissimilar to one another. We will use functions from vegan. Remember that you can run ?packageName to learn more about any R package, typically with helpful examples and vignettes of the most useful applications of the package.\nWe have numerous options in relation to displaying the dissimilarity matrices. We’ll explore non-metric multiple dimensional scaling, abbreviated to NMDS, nMDS, nmMDS, or just MDS.\nAs you might suspect, there are R functions which combine the three steps, though often it is preferable to separate them. The metaMDS() function calculates a dissimilarity matrix (as we did above) and produces an R object with all the information needed for plotting. It expects that samples are in rows and species (features) are in columns. We can also specify the distance metric, the number of axes to project to, whether to autotransform the data, and many other options. See ?metaMDS for the default values and other available arguments.\n\nord_raw &lt;- metaMDS(worm_df, distance = \"bray\", k = 2,\n                   autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(worm_df, distance = \"bray\", k = 2, autotransform = FALSE, :\nstress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nordiplot(ord_raw, choices = c(1, 2), display = \"sites\", \n         type = \"text\", main = \"NMDS: raw data, Bray-Curtis\")\n\n\n\n\n\n\nFigure 6.1: Simple pattern in example worm data.\n\n\n\n\nNote that metaMDS() involves some randomization, and your plot will change each time you run the code. For fully reproducible code, use set.seed().\nCompare this with the 4th-root transformed data using the same distance metric.\n\nord_4rt &lt;- metaMDS(worm_df_4rt, distance = \"bray\", k = 2, \n                   autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(worm_df_4rt, distance = \"bray\", k = 2, autotransform =\nFALSE, : stress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\n\n\nQ. 6.2Plot ord_4rt. How has the 4th root changed your data interpretation?*\n\n\n\nQ. 6.3Interpret the ordinations, cross referencing to the raw and transformed data. Are the patterns that you see in the data apparent on the ordination?\n\n\nYou can include on your plot the species ‘locations’ (as determined by their correlation with the axes). This shows where the main associations are occurring.\n\n# you can also plot the 'species' on the ordination\nordiplot(ord_4rt, choices = c(1, 2), display = c(\"sites\", \"species\"), \n         type = \"text\", main = \"NMDS, 4th-rt trans., Bray-Curtis\")\n\n\n\n\n\n\nFigure 6.2: Species superimposed onto ordination, indicating species-site associations.\n\n\n\n\n\nQ. 6.4Re-plot the untransformed data, but this time include the species. How has the transformation changed your interpretation of the data?\n\n\nLet’s have a look at another simulated dataset.\n\ncomm_df &lt;- data.frame(\n  row.names = c(\"Dunst\", \"Creran\", \"Lismore\", \"Charl\"),\n  SpA = c(1, 20, 30, 40),\n  SpB = c(11, 22, 50, 1),\n  SpC = c(500, 40, 30, 20),\n  SpD = c(10, 25, 35, 50),\n  SpE = c(4, 3, 2, 1),\n  SpF = c(40, 250, 1, 9)\n)\ncomm_df\n\n        SpA SpB SpC SpD SpE SpF\nDunst     1  11 500  10   4  40\nCreran   20  22  40  25   3 250\nLismore  30  50  30  35   2   1\nCharl    40   1  20  50   1   9\n\nord_comm &lt;- metaMDS(comm_df, distance = \"bray\", k = 2, \n                    autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(comm_df, distance = \"bray\", k = 2, autotransform = FALSE, :\nstress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nordiplot(ord_comm, choices = c(1, 2), \n         display = c(\"sites\", \"species\"), type = \"text\")\n\n\n\n\n\n\nFigure 6.3: Ordination for a simulated dataset showing sites and species.\n\n\n\n\n\nQ. 6.5Have a look at these raw data. What are the main trends? Which sites are more similar?\n\n\nThese data are still much simpler than most ‘real’ data sets but it is still difficult to summarise the similarities and differences between stations. However, multivariate analyses help you in this process.\n\nQ. 6.6Now fourth-root transform these data, generate the new dissimilarity matrix and plot it.\n\n\n\n\n\n\n\n\nView solution\n\n\n\n\n\n\nCodecomm_4rt_df &lt;- comm_df^0.25\nord_comm_4rt &lt;- metaMDS(comm_4rt_df, distance = \"bray\", k = 2, \n                        autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(comm_4rt_df, distance = \"bray\", k = 2, autotransform =\nFALSE, : stress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nCodeordiplot(ord_comm_4rt, choices = c(1, 2), \n         display = c(\"sites\", \"species\"), type = \"text\")\n\n\n\n\n\n\nFigure 6.4: Ordination for the 4th root transformed data.\n\n\n\n\n\n\n\n\nQ. 6.7What has the transformation done to your interpretation of differences between the Sites and Site x Species associations?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html#diversity-indices",
    "href": "P6_machine_learning.html#diversity-indices",
    "title": "6  Machine Learning",
    "section": "\n6.3 Diversity indices",
    "text": "6.3 Diversity indices\nPrior to the development of the multivariate techniques you’ll be using today, univariate indices were derived from multivariate data. A classic example of such a univariate measure in ecology is the Shannon-Wiener diversity index (a.k.a., Shannon’s H). This index balances the number of species in a sample and the relative abundance of each species (where ‘species’ can once again be any sort of feature). Univariate measures of ‘evenness’ can also be derived from multivariate data and, when reporting species data, you may also wish to include species richness, which is just the number of species present regardless of their abundances.\nWe can use the comm_df dataset we invented to explore some diversity concepts.\n\nQ. 6.8Looking at the comm_df data (by eye) and given the description above, which of the sites is associated with the lowest and highest diversity?\n\n\n\nshannon_H &lt;- diversity(comm_df, \"shannon\", base = exp(1)) \nrichness &lt;- specnumber(comm_df) \nbarplot(shannon_H, main = NULL, ylab = \"Shannon's H\")\n\n\n\n\n\n\nFigure 6.5: Shannon diversity\n\n\n\n\nTry plotting richness.\n\nQ. 6.9Do the plots correspond to what you expected?\n\n\n\nQ. 6.10What does specnumber() do?\n\n\n\nQ. 6.11How could you ‘counter’ any extremes (as in superabundant taxa) in the raw count data that you’ve generated? Try your idea.\n\n\n\nQ. 6.12Which description (diversity or richness) is ‘best’ for describing your multivariate data? How does this compare to NMDS?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html#nmds-on-real-data",
    "href": "P6_machine_learning.html#nmds-on-real-data",
    "title": "6  Machine Learning",
    "section": "\n6.4 NMDS on real data",
    "text": "6.4 NMDS on real data\nUsing simulated (or at least simple) data to learn new statistical techniques is usually the best approach because it gives you the opportunity to get a better sense for how the algorithms work (and to be sure your code is free from bugs!). Now we will progress to real observations.\nHave a look at the varespec and varechem datasets included in the vegan package. I’ve reproduced the examples below:\n\ndata(varespec) # ?varespec -- percent cover of 44 spp in lichen pastures \ndata(varechem) # ?varechem -- associated soil characteristics\nord_vare &lt;- metaMDS(varespec^0.25, distance = \"bray\", \n                   trace = FALSE, autotransform = FALSE)\n\n\npar(mfrow=c(1,3), mar=c(4,4,1,1)) \nordiplot(ord_vare, choices = c(1, 2), display = \"sites\", type = \"text\")\nordiplot(ord_vare, choices = c(1, 2), display = \"species\", type = \"text\")\nordiplot(ord_vare, choices = c(1, 2), display = c(\"sites\", \"species\"),\n  type = \"text\")\n# You can superimpose environmental variables onto NMDS-ordinations.\nef &lt;- envfit(ord_vare, varechem) #\nplot(ef, p.max = 0.1, col = \"green\") # overlay environmental variables\nplot(ef, p.max = 0.01, col = \"blue\") # subset based on p\n\n\n\n\n\n\nFigure 6.6: vare- data from vegan showing sites only, species only, and both plus environmental overlays.\n\n\n\n\nSee how the multivariate analysis has taken all those data, both species and environmental, and ‘communicated’ them in one single figure? You can see numerous relationships (both positive and negative) in this figure, and species-site-environment associations. It also illustrates a potential challenge with multivariate ordinations. It quickly gets cluttered and overloaded. There is no easy way around this, though there is further help in these packages.\nYou are in charge of the analysis. You can change the emphasis and elements of the message depending on your data transformation, dissimilarity metric, and ordination technique. There is no absolutely ‘correct’ way to go about multivariate stats, so different statisticians will have their favoured approaches and methods.\nNote: Some functions (e.g. metaMDS()) default to autotransform your data if the function thinks it is necessary. This can be useful but, in scientific reports, you must specify what transformations you used. Here you don’t know what the function applies as it depends on the data, but it could be any of several or a combination. My advice is only to use transformations that you specify.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html#principal-components-analysis-pca",
    "href": "P6_machine_learning.html#principal-components-analysis-pca",
    "title": "6  Machine Learning",
    "section": "\n6.5 Principal components analysis (PCA)",
    "text": "6.5 Principal components analysis (PCA)\nPCA is a long-established multivariate technique that is often applied to ‘environmental’ data rather than ‘count’ data. Environmental data is, usually, quite different from species count data in that most environmental parameters (e.g. metal concentrations) are present, at least to some degree. This contrasts to species data where many species are often absent (zeros). These zero counts would lead to problems if analysed using PCA, since PCA would consider sites with many shared absences to be more similar, which is often not desirable.\nIn PCA, it is important that all variables are of a roughly similar magnitude. Environmental data might include all manner of different variables on different scales (e.g., radiant flux in lumens, temperature in C, nutrient/contaminant concentrations in mg/l, coordinates in easting/northing). If used in their raw form, the variables with larger values would have a greater influence on the outcome of the PCA.\nInstead, we want all of our variables to be treated ‘equally’. You can do all this by setting pcomp(..., scale=TRUE). Scaling means that each measurement is expressed in units of standard deviation (a Z-score!!). Usually it is desirable to center the data as well by subtracting the mean. Centering and scaling means that each of the environmental variables is of ‘equal importance’ regardless of the magnitude of the raw values.\nA basic, and very friendly, introduction to PCA is given in Chapter 4 of Clarke et al. (2014).\nThe data set we’ll use is from SAMS Professor Tom Wilding’s PhD thesis. He set up an experiment to examine the relative leaching of trace metals from concrete, granite, and a control (artificial seawater). Concrete contains cement which is enriched in vanadium and molybdenum, and these elements could leach out in dangerous amounts. Granite, the main constituent of this concrete, might also leach some trace elements. He suspended concrete and granite powder in artificial water, constantly agitated it, and measured the leachate concentrations over 100 days Wilding and Sayer 2002.\n\nleach_df &lt;- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"Leaching\") |&gt;\n  mutate(Treat_abbr = factor(Treat, # abbreviate for cleaner plotting\n                             levels = c(\"concrete\", \"control\", \"granite\"),\n                             labels = c(\"conc\", \"ctrl\", \"gran\")),\n         Treat_day = paste(Treat_abbr, Day, sep=\"_\"), # treat + days in exprmnt\n         Conc = signif(Conc)) |&gt;\n  select(Treat_day, Element, Conc) # remove columns that aren't of use,\n#summary(leach_df)\n\n# not dominated by zeros; try other values (e.g. &lt;10)\ntable(leach_df$Conc == 0) \n\n\nFALSE  TRUE \n  143     4 \n\nmean(leach_df$Conc == 0) # recall that R treats T/F as 1/0 \n\n[1] 0.02721088\n\n\n\nQ. 6.13Do some exploratory analysis of this dataset. How are the concentrations distributed? Do similar treatments show qualitatively similar distributions? Elements?\n\n\nNext, we need to re-organise the data into a wider format so that each element is a column and each row is a sample.\n\nleach_df_wide &lt;- leach_df |&gt;\n  pivot_wider(names_from=\"Element\", values_from=\"Conc\")\n\nThe column Treat_day is coded as trt_day, where trt is the 4 letter code indicating treatment type and day is the number of days elapsed in the experiment (one of 1, 4, 7, 17, 32, 50 or 100). So gran_32 means the granite treatment sampled at day 32.\nWe can calculate the principal components using prcomp(), subsetting the dataframe to give only the columns with element concentrations (i.e., removing Treat_day, which is the first column). We’ll also set the arguments for centering and scaling to TRUE.\n\nPCA_leach &lt;- prcomp(leach_df_wide[, -1], center = TRUE, scale = TRUE) \nscreeplot(PCA_leach, main = NULL, ylab = \"Relative variation explained\")\n\n\n\n\n\n\nFigure 6.7: Scree plot showing the variance explained by each principal component.\n\n\n\n\n\n# take a look at PCA_leach\nPCA_leach\nstr(PCA_leach)\n\nYou can see from the scree plot that the amount of variance explained declines with principal component as expected, and that there is very little variation left after 3 principal components. That is, nearly all of the variation in the dataset is captured by PC1, PC2, and PC3. In this case, PCA has essentially solved the ‘curse of dimensionality’ by successfully reducing 7D data to about 3D.\nData transformations are critical to PCA analysis, as they are with NMDS. In most PCAs the data are standardized so that each column is on the same scale.\nWe can plot our results using biplot() which has some helpful defaults including labels for the samples (Treat_day) and the correlation strength of each element with PC1 and PC2:\n\nbiplot(PCA_leach, xlabs = leach_df_wide$Treat_day, cex = 0.75)\n\n\n\n\n\n\nFigure 6.8: PCA of the complete dataset, illustrating the common challenge of overplotting.\n\n\n\n\nTo plot more than 2 dimensions you could use a static 3D plot, but these are often difficult to interpret since it is reduced back to 2D on a page. Another option is to use colour for the 3rd axis, or, for digital distribution, a package like plotly can produce an interactive 3D plot.\n\nCodeif(knitr::pandoc_to() == \"html\") {\n  \narrow_df &lt;- tibble(element=row.names(PCA_leach$rotation),\n                   PC1=PCA_leach$rotation[,1]*6,\n                   PC2=PCA_leach$rotation[,2]*6,\n                   PC3=PCA_leach$rotation[,3]*6)\n\nPCA_leach$x |&gt;\n  as_tibble() |&gt;\n  mutate(Treat_day = leach_df_wide$Treat_day) |&gt;\n  separate_wider_delim(Treat_day, delim=\"_\", names=c(\"Treatment\", \"Days\")) |&gt;\n  mutate(Days=as.numeric(Days)) |&gt;\n  plot_ly(type=\"scatter3d\", x=~PC1, y=~PC2, z=~PC3, symbol=~Treatment, color=~Days, mode=\"markers\",\n            symbols=c(\"circle-open\", \"diamond-open\", \"cross\")) |&gt;\n  add_text(data=arrow_df, x=~PC1, y=~PC2, z=~PC3, text=~element, color=I(\"black\"), symbol=NULL)\n}\n\n\n\n\n\n\nFigure 6.9: PCA of the complete dataset using plotly.\n\n\n\nThe ordination plots the relative positions (in terms of similarity) of the samples. If there are numerous label overlaps, this makes the interpretation of the ordination difficult. If you were producing this for publication you would need to sort this out to present the reader with a figure that communicates the message effectively.\n\nQ. 6.14Which elements are positively associated with granite and concrete, particularly after longer periods of leaching?\n\n\nAs is typical, overlapping points make interpretation more difficult. There are elegant solutions to this (in terms of labelling) but for now, we’ll split the data and analyse it separately.\nWe’ll need more data wrangling to split it efficiently and we’ll use grepl(), which identifies a pattern within a character using a regular expression (a.k.a., regex), returning a TRUE or FALSE for each element in the character vector. Here, we’ll filter the dataframe to only include rows where Treat_day contains conc or gran. Remember the ‘or’ operator |?\n\n#?grepl\n#cbind(leach_df_wide$Treat_day, grepl(\"conc|gran\", leach_df_wide$Treat_day))\nleach_df_trts &lt;- leach_df_wide |&gt;\n  filter(grepl(\"conc|gran\", Treat_day))\nPCA_trts &lt;- prcomp(leach_df_trts[, -1], scale = TRUE, center = TRUE)\nbiplot(PCA_trts, xlabs = leach_df_trts$Treat_day)\n\n\n\n\n\n\nFigure 6.10: PCA of the concrete and granite, illustrating temporal and treatment differences.\n\n\n\n\n\nQ. 6.15Repeat the analysis, but set scale = FALSE. Which element now seems to dominate the analysis? Explain what you see.\n\n\nA PCA is normally reported with the proportion of the variation explained by each of the principal components (and/or the cumulative proportion). If the cumulative proportion for the 1st two components is high, then the 2D (PC1 and PC2) ordination is a good representation of the similarities between samples. In that sense a high cumulative proportion is analogous to a low stress for NMDS.\nLet’s have a look at a summary of the principal components.\n\nPCA_leach_summary &lt;- summary(PCA_leach) \n#PCA_leach_summary\n#str(PCA_leach_summary)\nPCA_leach_summary$importance[, 1:4] # extract only PC1-4\n\n                            PC1      PC2       PC3       PC4\nStandard deviation     1.988091 1.450775 0.7782675 0.5599535\nProportion of Variance 0.564640 0.300680 0.0865300 0.0447900\nCumulative Proportion  0.564640 0.865320 0.9518500 0.9966400\n\n\nAs you can see, PC1, PC2, and PC3 capture more than 95% of the variance in our data. Adding PC4 brings that up above 99%.\nFinally, we can look at factor loadings. This is a measure of how each feature (metal concentration in this case) relates to the principal components. In PCA, the principal components are sequentially ‘less’ influential since they describe increasingly smaller amounts of variation. By centering and standardising the variables, you can assess the relative importance of each in driving the patterns in the ordination. The magnitude is what we’re interested in rather than the sign.\nIn an object created by prcomp(), the loadings are stored as .$rotation.\n\n# ?prcomp\n# str(PCA_leach)\nsignif(PCA_leach$rotation[, 1:5], 3) # factor loadings for PC1-5\n\n      PC1     PC2     PC3    PC4      PC5\nBa -0.367  0.2090  0.6510 -0.619 -0.00405\nFe -0.380 -0.0440 -0.7370 -0.553  0.01420\nMn -0.146  0.6500 -0.0963  0.211  0.67300\nMo -0.430 -0.3360  0.0795  0.283 -0.09810\nRb -0.462 -0.2440  0.0327  0.305  0.21600\nSr -0.495 -0.0788  0.0663  0.168  0.01510\nU  -0.238  0.5940 -0.1110  0.254 -0.70000\n\n\nHere you can see that Sr, Rb, Mo are relatively ‘important’ in driving the multivariate pattern you’ve observed (i.e., high absolute values on PC1) while Mn and U have high values for PC2 (you could say that PC2 accounts for Mn and U).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P6_machine_learning.html#conclusions",
    "href": "P6_machine_learning.html#conclusions",
    "title": "6  Machine Learning",
    "section": "\n6.6 Conclusions",
    "text": "6.6 Conclusions\nMultivariate analyses are extremely useful in a variety of contexts. The form of analysis is often more qualitative and less inferential compared to univariate analyses we’ve covered, but this is an active field of research and there are well-developed methods and R packages for more complex analyses (e.g., ade4.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html",
    "href": "P7_appendix.html",
    "title": "7  Appendix",
    "section": "",
    "text": "7.1 Probability",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#probability",
    "href": "P7_appendix.html#probability",
    "title": "7  Appendix",
    "section": "",
    "text": "7.1.1 Probability for equally likely outcomes\nThe probability of event \\(A\\) occurring is \\(P(A) = \\frac{x}{n}\\), where \\(n\\) is the number of trials and \\(x\\) is the number of trials during which \\(A\\) occurred.\n\n\n7.1.2 Multiplication and addition\nThe general case of the multiplication rule is that, for two events \\(A\\) and \\(B\\):\n\\(P(A \\& B) = P(A) * P(B|A)\\), where \\(P(B|A)\\) is the probability that \\(B\\) occurs given than \\(A\\) has already occurred.\nThe Addition Law states that, for two events \\(A\\) and \\(B\\):\n\\(P(A\\ or\\ B\\ or\\ A\\&B) = P(A) + P(B) - P(A\\&B)\\)\nand\n\\(P(A\\ or\\ B\\ but\\ not\\ A\\&B) = P(A) + P(B) - 2*P(A\\&B)\\)\nand when \\(A\\) and \\(B\\) are mutually exclusive, then:\n\\(P(A\\ or\\ B) = P(A) + P(B)\\)\n\n\n7.1.3 Bayes’ theorem\n\\(P(A|B)= \\frac{P(B|A) * P(A)}{P(B)}\\), where \\(P(B)= P(A) * P(B|A) + P(A’) * P(B|A’)\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#univariate-statistics",
    "href": "P7_appendix.html#univariate-statistics",
    "title": "7  Appendix",
    "section": "7.2 Univariate statistics",
    "text": "7.2 Univariate statistics\n\n7.2.1 Mean\nmean()\nThe mean is calculated as \\(\\bar{y} = \\frac{\\sum{y}}{n}\\).\nFor measures of central tendency and dispersion, we use Greek letters to refer to population values and Latin letters to refer to samples:\n\n\n\n\nPopulation\nSample\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{y}\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\\(s^2\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s\\)\n\n\n\n\n\n7.2.2 Median, quartiles and adjacent values\nmedian(), quantile(), IQR()\nWith data ordered by rank, the median value is the middle value or the \\((\\frac{n+1}{2})^{th}\\) value, the lower quartile, \\(Q1\\), is the \\((\\frac{n+1}{4})^{th}\\) value, the upper quartile, \\(Q3\\), is the \\((\\frac{3*(n+1)}{2})^{th}\\) value. The inter-quartile range is \\(IQR = Q3 – Q1\\). The upper adjacent value is the upper value that is less than \\(Q3 + (1.5 * IQR)\\). The lower adjacent value is the lower value that is more than the \\(Q1 – (1.5 * IQR)\\). Outliers are defined as values that lie outside the range \\(Q1 - 1.5*IQR\\) or \\(Q3 + 1.5*IQR\\).\n\n\n7.2.3 The sum of squares\nThe sum of squares is given by \\(SS = \\sum{y^2} - \\frac{(\\sum{y})^2}{n}\\) where \\(n\\) is the number of observations.\n\n\n7.2.4 Measures of dispersion\nvar(), sd(), length()\nWhen dealing with populations the following formulas are used:\nVariance: \\(\\sigma^2 = \\frac{SS}{n}\\)\nStandard deviation: \\(\\sigma = \\sqrt{\\frac{SS}{n}} = \\sqrt{\\sigma^2}\\)\nWhen dealing with samples the following formulas are used:\nVariance: \\(s^2 = \\frac{SS}{n-1}\\)\nStandard deviation: \\(s = \\sqrt{\\frac{SS}{n-1}}\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#the-binomial-distribution",
    "href": "P7_appendix.html#the-binomial-distribution",
    "title": "7  Appendix",
    "section": "7.3 The Binomial distribution",
    "text": "7.3 The Binomial distribution\ndbinom(), pbinom(), qbinom(), rbinom()\nThe binomial distribution describes the expected distribution for two mutually exclusive outcomes. The formula is given by:\n\\(P(x) = \\frac{n!}{x!(n-x)!} p^x q^{n-x}\\)\nwhere \\(P(x)\\) is the probability of \\(x\\) ‘successes’ occurring, \\(n\\) is the number of trials, \\(p\\) is the probability of \\(x\\) in a single trial, and \\(q\\) is the probability of not \\(x\\) in a single trial with \\(p+q=1\\).\nA binomially distributed variable has mean = \\(n*p\\) and variance = \\(n*p*q\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#the-poisson-distribution",
    "href": "P7_appendix.html#the-poisson-distribution",
    "title": "7  Appendix",
    "section": "7.4 The Poisson distribution",
    "text": "7.4 The Poisson distribution\ndpois(), ppois(), qpois(), rpois()\nThe Poisson distribution for a sample is defined as follows:\n\\(P(x) = \\frac{\\bar{y}^x e^{-\\bar{y}}}{x!}\\)\nwhere \\(\\bar{y}\\) is the sample mean and \\(x\\) is the value (count) of interest.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#z-scores",
    "href": "P7_appendix.html#z-scores",
    "title": "7  Appendix",
    "section": "7.5 Z scores",
    "text": "7.5 Z scores\nscale()\nThe standard normal distribution is \\(Norm(\\mu=0, \\sigma=1)\\). Z scores are used to convert any normal distribution to the standard normal distribution:\n\\(z = \\frac{y-\\mu}{\\sigma}\\)\nwhere \\(y\\) is the value, \\(\\mu\\) is the mean of the population and \\(\\sigma\\) is the standard deviation of the population.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#samples-taken-from-a-population",
    "href": "P7_appendix.html#samples-taken-from-a-population",
    "title": "7  Appendix",
    "section": "7.6 Samples taken from a population",
    "text": "7.6 Samples taken from a population\nsd(), sqrt(), length(), mean(), sqrt()\n\n7.6.1 Standard error of the mean\nThe standard error is the standard deviation of sample means:\n\\(SEM = \\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(n\\) is the sample size. If we are testing a sample taken from a population of known population mean and population standard deviation we use the formula:\n\\(z = \\frac{\\bar{y}-\\mu}{\\sigma_{\\bar{y}}}\\)\nwhere \\(\\bar{y}\\) is a particular sample mean, \\(\\mu\\) is the population mean, and \\(\\sigma_{\\bar{y}}\\) is the standard error of the mean.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#the-t-distribution",
    "href": "P7_appendix.html#the-t-distribution",
    "title": "7  Appendix",
    "section": "7.7 The t distribution",
    "text": "7.7 The t distribution\ndt(), pt(), qt(), rt()\nThe t-distribution describes the distribution of \\(T\\) statistics expected under the null hypothesis.\n\n7.7.1 The one sample t-test\nt.test()\nThe T statistic is calculated as:\n\\(T = \\frac{\\bar{y}-\\mu}{s_{\\bar{y}}}\\)\nWhere \\(\\bar{y}\\) is the sample mean, \\(\\mu\\) is the hypothesized population mean, and \\(s_{\\bar{y}}\\) is the standard error such that \\(s_{\\bar{y}} = \\frac{s}{\\sqrt{n}}\\) with sample standard deviation \\(s\\).\nUse t.test(), or find the p-value associated with your T with pt(T, df-1).\n\n\n7.7.2 Confidence intervals for sample means\nt.test(), qt(), mean(), sd(), length(), sqrt()\nThe 95% CI for a mean is calculated as:\n\\(\\bar{y} \\pm t_{\\alpha/2, df=n-1} * s_{\\bar{y}}\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#anova",
    "href": "P7_appendix.html#anova",
    "title": "7  Appendix",
    "section": "7.8 ANOVA",
    "text": "7.8 ANOVA\nanova(lm()), aov()\nAn ANOVA table contains the following:\n\n\n\n\n\n\n\n\n\n\nSource of variation\nSum of Squares\nDegrees of Freedom\nMean Square\nF\n\n\n\n\nBetween groups\n\\({SS}_{groups}\\)\n\\({df}_{groups}\\)\n\\(\\frac{{SS}_{groups}}{{df}_{groups}}\\)\n\\(\\frac{{MS}_{groups}}{{MS}_{error}}\\)\n\n\nError\n\\({SS}_{error}\\)\n\\({df}_{error}\\)\n\\(\\frac{{SS}_{error}}{{df}_{error}}\\)\n\n\n\nTotal\n\\({SS}_{total}\\)\n\\({df}_{total}\\)\n\n\n\n\n\nwith:\n\\({SS}_{groups} = \\sum{n_j (\\bar{y_j} - \\bar{\\bar{y}})^2}\\)\n\\({SS}_{error} = \\sum{(y_{ij} - \\bar{y_j})^2}\\)\n\\({SS}_{total} = {SS}_{groups} + {SS}_{error} = \\sum{(y_{ij} - \\bar{\\bar{y}})^2}\\)\nwhere \\(y_{ij}\\) is the \\(i^{th}\\) observation in group \\(j\\), \\(\\bar{y_j}\\) is the mean for group \\(j\\), and \\(\\bar{\\bar{y}}\\) is the grand mean. With \\(N\\) total observations, \\(n_j\\) observations per group, and \\(k\\) groups, the degrees of freedom are:\n\\({df}_{groups} = k - 1\\)\n\\({df}_{error} = N - k = k (n_j-1)\\)\n\\({df}_{total} = {df}_{groups} + {df}_{error} = N-1\\)\nThe critical value (CV) for samples of equal size is given as:\n\\(CV = q \\sqrt{\\frac{{MS}_{error}}{n}}\\)\nwhere \\(q\\) either comes from a table online (the studentized range distribution), or from qtukey(1-alpha, k, df_error).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "P7_appendix.html#regression-correlation",
    "href": "P7_appendix.html#regression-correlation",
    "title": "7  Appendix",
    "section": "7.9 Regression & correlation",
    "text": "7.9 Regression & correlation\nlm(), cor(), confint(), predict()\nWe do not bother calculating regression coefficients by hand.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  }
]