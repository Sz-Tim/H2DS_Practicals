[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H2 Data Science Practicals",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#practical-sessions",
    "href": "index.html#practical-sessions",
    "title": "H2 Data Science Practicals",
    "section": "Practical sessions",
    "text": "Practical sessions\nIn these practicals, you will apply concepts you learn throughout the H2 Data Science course. The questions you answer by hand are useful for learning and revision, while producing appropriate graphics and correctly describing results are essential parts of the scientific process. Expertise in these core skills is essential to do well in future courses and scientific projects. H2 Data Science is truly one of the most important courses you’ll take!\nYou will use datasets provided on Brightspace, datasets included in R, and datasets you simulate yourself.\nPlease read through the practical before the class. Each session is scheduled for 3.5h with a break in the middle. You will work through a series of coding exercises and questions which are not assessed or marked, but may appear in the assessments. You should complete all the material in each practical.\nThroughout the sessions, we will alternate between individual work and short re-caps as a class. If you find yourself racing ahead, all well and good. If you find yourself falling behind, then try some additional study ahead of the next session."
  },
  {
    "objectID": "P0_R_setup.html#getting-started-in-r",
    "href": "P0_R_setup.html#getting-started-in-r",
    "title": "R setup",
    "section": "Getting started in R",
    "text": "Getting started in R\nR is a statistical language and computing platform that is widely used in the sciences. It is free and open source. We will be using it extensively. There are many resources available, including:\n\nYour notes and course material from H1 Maths and Data Science (also on Brightspace &gt; Practicals)\nCourses such as those from Software Carpentry or Swirl\n\nOnline videos such as IQUIT R\n\n\nYou should access these resources in your own time to complement, revise, and reinforce the concepts you’ll learn during this course.\nDuring the practicals, use ‘copy-and-paste’ thoughtfully. R is best learned through your fingers, and working through errors, though frustrating, is an essential skill."
  },
  {
    "objectID": "P0_R_setup.html#sec-RStudio_projects",
    "href": "P0_R_setup.html#sec-RStudio_projects",
    "title": "R setup",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nWorking in a ‘Project’ within RStudio is the best way to avoid working directory complications. This is a very common source of frustration and errors.\nWe will create an R Project for the semester:\n\nSign into OneDrive, then open RStudio\nSelect File &gt; New Project…\n\nChoose New Directory &gt; New Project\n\nSet the Directory name as H2_DataScience and use the Browse button by Create project as a subdirectory of: to set a convenient location in your OneDrive folder. Click ‘Create Project’.\nIn the Files panel in RStudio, click the New Folder button and create a folder called data.\nOn Brightspace, download the files in Practicals &gt; data and move them into your newly created data folder.\nIn the Files panel in RStudio, click the New Folder button and create a folder called code.\nOn Brightspace, download the files in Practicals &gt; code and move them into the H2_DataScience folder.\nClose RStudio (to learn how to open appropriately)\n\nYou’re now prepared and organized for a semester in R!\nNext, let’s go through the typical start-up process:\n\nOpen Windows Explorer (or Finder on Mac) and find your H2_DataScience folder.\n\nDouble click on H2_DataScience.Rproj. This opens your R project with the working directory set to that folder. The working directory is shown at the top of the Console pane, and you can check it with getwd(). This is where R is ‘situated’ when loading or saving files.\nIn the Files panel, open the .qmd file for the week.\n\nAt the start of each practical, repeat steps 2 and 3 for (hopefully) painless work in R.\nThe code in the practicals assumes this organization. If you choose to put your data files elsewhere, you will need to update the scripts accordingly."
  },
  {
    "objectID": "P0_R_setup.html#rstudio-settings",
    "href": "P0_R_setup.html#rstudio-settings",
    "title": "R setup",
    "section": "RStudio Settings",
    "text": "RStudio Settings\nYou can adjust many settings in RStudio via Tools &gt; Global options. In the Appearance tab in the popup box, you can set the theme (e.g., if you prefer a dark theme), font size, etc. The Code tab has many nice features as well (e.g., rainbow parentheses under Display)."
  },
  {
    "objectID": "P0_R_setup.html#sec-R_packages",
    "href": "P0_R_setup.html#sec-R_packages",
    "title": "R setup",
    "section": "R packages and libraries",
    "text": "R packages and libraries\nR packages are collections of functions, custom data structures, and datasets that are developed by the user base. A new installation of R includes many useful default packages, visible on the ‘Packages’ tab in RStudio. There are many additional packages available from the official CRAN repository or less officially from GitHub. If you find yourself re-using custom functions across projects, you can even create your own personal package.\nTo install a package from CRAN, use the function install.packages(\"packageName\"). This downloads the package files to your computer. Each time you open R, you will need to load that package to use it with library(packageName).\nInstalling from other package sources is slightly more complicated, so see me if you have a need.\nYou can get an overview of a package with ?packageName, and then see a list of all of the functions by scrolling to the bottom of the help page and clicking the “index” link.\nThe help for each function is available with ?functionName, and you can see the underlying code with functionName without parentheses."
  },
  {
    "objectID": "P0_R_setup.html#sec-quarto",
    "href": "P0_R_setup.html#sec-quarto",
    "title": "R setup",
    "section": "R scripts (.R) vs Quarto documents (.qmd)",
    "text": "R scripts (.R) vs Quarto documents (.qmd)\nIn H1, you used R scripts (.R). These are just text files. The extension tells your computer to associate them with R, and also lets you run lines of code with ‘ctrl + enter’ and other convenient things in R and RStudio.\nQuarto documents (.qmd) are also text files. However, RStudio interprets the text differently, allowing you to intersperse written prose, R code, and output in a single document (similar to a jupyter notebook or a live script in Matlab). Code is marked as “chunks” and you can specify options for how the code and output are displayed. The text uses markdown formatting, which allows all sorts of formatting (headers, bold, italic, equations, hyperlinks, tables, images…). RStudio can render a .qmd file into many other formats (e.g., pdf, docx, html, epub…).\nCode chunks can be added with the green button with a ‘c’ on the top right of a .qmd document in RStudio. They look like this:\n\n```{r}\n# This is a code chunk\na &lt;- 1:3\na\n```\n\n[1] 1 2 3\n\n\nThe output from the code is shown just below the block.\nFor code-heavy work, Quarto documents are a handy way to produce nicely formatted output without the hassle of copying and pasting code, output, and figures into, e.g., a word document. There are many guides and tutorials online.\nIn fact, this manual is written as a Quarto book. Versions of the .qmd files for each practical are available on Brightspace to make things easier for you. Note that the project will be submitted as a .qmd file."
  },
  {
    "objectID": "P0_R_setup.html#writing-r-code",
    "href": "P0_R_setup.html#writing-r-code",
    "title": "R setup",
    "section": "Writing R code",
    "text": "Writing R code\nR has established best practices to make your meaning clear. Just like any language, you|can|write|with|your|own|system, but it’s easier for everyone to use standard conventions. See the full style guide for more.\nA few key points:\n\nUse &lt;- to assign a value to an object. You may see =, which works, but is not preferred.\nUse spaces to make your code legible: a &lt;- c(1, 2, 3).\nAvoid spaces in column names or file names as these are a pain to work with.\nUse names for objects that are short, but descriptive.\nLimit the length of a line of code to about 80 characters.\nUsually, variables should be nouns and functions should be verbs.\nUse # to write a comment which R will ignore.\nRun the line of code where your cursor is (or everything you’ve selected) with ctrl + enter"
  },
  {
    "objectID": "P1_R_recap.html#basic-data-exploration",
    "href": "P1_R_recap.html#basic-data-exploration",
    "title": "1  R recap",
    "section": "\n1.1 Basic data exploration",
    "text": "1.1 Basic data exploration\n\n1.1.1 Object structure\nWe will mostly work with dataframes. A dataframe is a 2D rectangular structure with columns and rows. In a tidy dataset, each row represents an ‘observation’ and each column represents a ‘variable’. R (and often packages) contains several built-in dataframes.\nThe dataframe cars gives the max speeds and stopping distances for cars built in the early 20th century. We are going to use cars to demonstrate a few basic concepts in relation to R programming and statistical analysis.\n\n# functions for basic details of objects\nstr(cars)\nclass(cars)\nnames(cars)\nhead(cars)\n\n\nhead(cars, 2)\n\n  speed dist\n1     4    2\n2     4   10\n\ntail(cars, 2)\n\n   speed dist\n49    24  120\n50    25   85\n\n# what are the last 10 rows?\n\n\n1.1.2 Subsetting, renaming, and rearranging\nThere are several ways to access subsets of a dataframe:\n\nUse .$columnName or .[[\"columnName\"]] to extract a single column\nUse .[rows,columns] to extract a block\n\n\ncars$speed # whole column\ncars[[\"speed\"]] # whole column\n\n\ncars[1, 1] # row 1, column 1\n\n[1] 4\n\ncars[1:5, 1] # rows 1-5, column 1\n\n[1] 4 4 7 7 8\n\ncars[1:3, ] # leaving the 'columns' space blank returns all columns\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n\n\nWe can also change column names. For illustration, let’s make a copy of the dataframe to do that.\n\ncars2 &lt;- cars \nnames(cars2)\n\n[1] \"speed\" \"dist\" \n\nnames(cars2)[1] &lt;- \"speed_mph\"\nnames(cars2)\n\n[1] \"speed_mph\" \"dist\"     \n\nnames(cars2) &lt;- c(\"speed_mph\", \"dist_ft\") # change both column names\n\nRearranging and duplicating columns is also easy.\n\nhead(cars2, 2)  \n\n  speed_mph dist_ft\n1         4       2\n2         4      10\n\ncars2 &lt;- cars2[, 2:1] # rearrange columns \nhead(cars2, 2) \n\n  dist_ft speed_mph\n1       2         4\n2      10         4\n\ncars3 &lt;- cars2[, c(2, 1, 1)] # duplicate a column\nhead(cars3, 2)\n\n  speed_mph dist_ft dist_ft.1\n1         4       2         2\n2         4      10        10\n\ncars3 &lt;- cars3[, 1:2] # remove the duplicated column\nhead(cars3, 2)\n\n  speed_mph dist_ft\n1         4       2\n2         4      10\n\ncars3$dist_x_speed &lt;- cars3$dist_ft * cars3$speed_mph # create a new column\nhead(cars3, 2)\n\n  speed_mph dist_ft dist_x_speed\n1         4       2            8\n2         4      10           40\n\nrm(cars3) # remove the dataframe 'cars3'\n\nYou can also subset based on criteria. Say we only want rows where the speed is \\(&gt;\\) 20 mph:\n\nfast_i &lt;- which(cars2$speed_mph &gt; 20) \nstr(fast_i)\n\n int [1:7] 44 45 46 47 48 49 50\n\ncars_fast &lt;- cars2[fast_i, ] # or: cars2[which(cars2$speed_mph &gt; 20), ]\nclass(cars_fast) \n\n[1] \"data.frame\"\n\nncol(cars_fast)  # and how many *rows* are there?\n\n[1] 2\n\nhead(cars_fast, 2)\n\n   dist_ft speed_mph\n44      66        22\n45      54        23\n\n\n\n1.1.3 NAs and summary\nWhen you import data, you should check for missing values. These are represented as NA.\nWe can check each element of a vector using is.na(), which will return TRUE if an element is NA, and FALSE if an element is not NA.\n\nis.na(cars2$speed_mph)\n\nR converts a logical vector (i.e., TRUE/FALSE) to numeric (i.e., 1/0) automatically. This is handy, but dangerous if you don’t realize it.\n\nsum(is.na(cars2$speed_mph))\n\n[1] 0\n\ncarsNA &lt;- cars2\ncarsNA[c(2, 4, 5, 10), 1] &lt;- NA\nsum(is.na(carsNA$dist_ft))\n\n[1] 4\n\n\nAnother very useful check is summary():\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\nsummary(carsNA)\n\nOnce you have assured yourself that your dataframe looks sensible, that it contains the data you expect, and that you know what the data-types are, you can start to explore and summarise your data.\nThere are many graphical methods for data exploration. The appropriate method depends on the nature of the data and what you wish to communicate to the reader."
  },
  {
    "objectID": "P1_R_recap.html#graphical-methods-for-displaying-data",
    "href": "P1_R_recap.html#graphical-methods-for-displaying-data",
    "title": "1  R recap",
    "section": "\n1.2 Graphical methods for displaying data",
    "text": "1.2 Graphical methods for displaying data\nAlways keep in mind that the primary reason for data visualization is to impart information concisely and accurately to your reader.\nGraphics must be clear, concise and easy to understand. Brightspace contains some examples of bad graphics (‘Learning resources&gt;Lecture support material&gt;Introduction (Lectures 1-3)&gt;Graphics’).\n\n\nFigure 1.1: An example of a terrible graphic, as published in a Scottish government report.\n\nIn addition to poor design choices for effective communication (Figure 1.1), graphics can also be deliberately misleading (Figure 1.2).\n\n\nFigure 1.2: A misleading graphic. What type of plot is this and how is it misleading?\n\nThe scatter plot is used to plot two continuous variables against each other. It is commonly used for analyses like correlation or linear regression.\n\n\n\n\nFigure 1.3: Symbol options.\n\n\n\nQ1. Search the plot help page for ‘title’, then add an appropriate title to your plot.\nQ2. ?points opens the help page for points. Search the help page for ‘pch’ and change the symbol of your plot\nQ3. With the cars dataset, plot stopping distance by speed for only those cars with a speed greater than 20.\n\nplot(dist ~ speed, data=cars[cars$speed &gt; 20, ], \n     xlab=\"Speed (mph)\", ylab=\"Distance (ft)\", pch=2)\n\n\n\nFigure 1.4: Stopping distance by speed.\n\n\n\n\n1.2.1 Boxplots\nBox plots are used to summarise a continuous variable by levels of a factor. We will use the mtcars dataset to illustrate this.\nExplore the dataframe using the strategies covered above. Which variables are categorical? Which are continuous?\n\nhead(mtcars, 2)\n\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\n\n\n\n\n\nFigure 1.5: Boxplot showing miles per litre vs. number of carburetors\n\n\n\n\n?boxplot\n# See examples at bottom of the help page\n# Produce a box plot with axes labels and a title\n\nReproduce the plot shown in Figure 1.5 (assume 1 gallon = 4.5 L). Remember you can learn about these data with ?mtcars. You will need to generate a new variable (miles per litre) and label your box plot appropriately. You can limit the extent of the y-axis by adding the argument ylim=c(a, b) where a and b are the limits you want (e.g., ylim=c(0, 100)).\nQ4. Use ?boxplot to investigate what the box and whiskers in the box plot actually represent. Check you can reproduce the upper and lower adjacent values manually (see Chapter )\nNote that box plots are not the most visually intuitive. Packages like ggplot2 (and extensions) make alternatives like those in Figure 1.6 simple to produce. We will cover some of these later.\n\n\n\n\nFigure 1.6: Alternatives to boxplots\n\n\n\n\n1.2.2 Line plots\nLine plots are most often seen in time-series plots with time on the x-axis and the response on the y-axis. Line plots typically involve joining points with a line, which indicates that you have made assumptions about the value of the response variable between successive measurements.\nWe will examine these plots using the dataset lynx, which consists of the number of Canadian lynx pelts sold per year between 1821 - 1934. It is a ‘classic’ dataset as it shows a cyclical ‘boom-and-bust’ lynx population (demonstrating predator-prey interactions).\nFirst, we will create a variable Year.\n\nstr(lynx)\n\n Time-Series [1:114] from 1821 to 1934: 269 321 585 871 1475 ...\n\n\n\nclass(lynx) # ts = time-series\nlynx2 &lt;- as.data.frame(lynx) \nclass(lynx2) \nhead(lynx2, 2) \nlynx2$Year &lt;- seq(from=1821, to=1934, by=1)\n\nIn R, we use functions to perform actions on objects. Functions have arguments, taking the form functionName(arg1=..., arg2=...). If you do not name the arguments, the function will assume that you are listing the arguments in order. See the help file for a function with ? to see the argument order (e.g., ?seq).\nQ5. Using seq(), write a piece of code which generates odd numbers between 1 and 20 with and without specifying by.\n\n# change the name of the 1st column to 'Trappings'\nnames(lynx2)[1] &lt;- \"Trappings\"\nstr(lynx2)\nlynx2$Trappings &lt;- as.numeric(lynx2$Trappings) # Time-Series is complicated.\nstr(lynx2)\n\nUse ?plot to investigate options for plotting. Find the type= argument for plotting both the points and a connecting line. This might be the best option in this case. Why?\nQ6. Using the R plot function, produce a line plot of the Trappings data, as per Figure Figure 1.7.\n\n\n\n\nFigure 1.7: The number of lynx trapped in Canada (1820-1934)\n\n\n\nChange the plot to show only the years up to 1900, then plot the Trappings on the log scale.\n\n1.2.3 Histograms\nHistograms are used to illustrate the distribution of continuous data. In histograms the bars are adjacent (no gap) and this indicates that there is a continuum (i.e. that the data are not discrete).\n\n# this gives very different information.\nhist(lynx2$Trappings, main=\"Lynx trapping\", xlab=\"Trapped lynx per year\")\n\n\n\nFigure 1.8: Lynx pelts per year with default settings.\n\n\n\nWhich range of values was most common across years?\nBe aware that histograms can be quite sensitive to the bins that you use.\n\npar(mfrow=c(1,2)) # panels for the plotting window\n# R takes the number of breaks as a suggestion\nhist(lynx2$Trappings, main=\"Lynx trapping\", xlab=\"Trapped lynx per year\",\n     breaks=5)\n# this forces R to plot according to the defined breaks\nhist(lynx2$Trappings,\n     main=\"Lynx trapping\", xlab=\"Trapped lynx per year\",\n     breaks=c(0, 500, 1000, 2000, 5000, 10000))\n\n\n\nFigure 1.9: Lynx pelts per year with breaks=5 (left) and a vector of breaks (right).\n\n\n\n\npar(mfrow=c(2, 2)) # plot panels (2 rows x 2 columns)\npar(mar=rep(2, 4)) # change the plot margins\nhist(lynx2$Trappings, main=\"bin width: 100\", xlab=\"Trapped lynx per year\", \n     breaks=seq(from=0, to=10000, by=100))\nhist(lynx2$Trappings, main=\"bin width: 500\", xlab=\"Trapped lynx per year\", \n     breaks=seq(from=0, to=10000, by=500))\nhist(lynx2$Trappings, main=\"bin width: 1000\", xlab=\"Trapped lynx per year\", \n     breaks=seq(from=0, to=10000, by=1000))\nhist(lynx2$Trappings, main=\"bin width: 2000\", xlab=\"Trapped lynx per year\", \n     breaks=seq(from=0, to=10000, by=2000))\n\n\n\nFigure 1.10: Histograms of lynx pelts per year with different breaks\n\n\n\n\npar(mfrow=c(1, 1)) # reset the par setting.\n\nWhich of these plots is the most useful? There is no definitive answer to this, but the first is very busy and the last fails to show relevant detail near 0. Setting the bin width to 500 or 1000 communicates the patterns in the data most clearly.\nAs a general guideline, 5-15 breaks usually work well in a histogram.\n\n1.2.4 Bar graphs\nWhen you create a data.frame it defaults to naming the rows 1…n, where n is the number of rows. You may occasionally come across a data.frame with row names. Converting between data types may lose this information. Consequently, it is better practice to store relevant information in a column.\nBar graphs are used to plot counts of categorical or discrete variables. We’ll be using the islands dataset (which has data stored as row names).\nWorking with data involves a lot of time spent tidying the datasets: cleaning, checking, and reshaping into useful formats. We will cover a more modern set of methods for this later in the course using the tidyverse package. For now, we’ll stay with base R. First, we need to tidy the islands data.\n\nstr(islands) \nclass(islands) # this is a named numeric vector\nhead(islands)\n\n# convert to a dataframe\nislands.df &lt;- as.data.frame(islands) \nhead(islands.df, 2)\n\n\n# put the row names into a new column\nislands.df$LandMass &lt;- row.names(islands.df) \nhead(islands.df, 2)\n\n           islands   LandMass\nAfrica       11506     Africa\nAntarctica    5500 Antarctica\n\n# set row names to the row number\nrow.names(islands.df) &lt;- 1:nrow(islands.df) \nnames(islands.df)[1] &lt;- \"Area\" \nhead(islands.df, 2) \n\n   Area   LandMass\n1 11506     Africa\n2  5500 Antarctica\n\n# reorder by area\nislands.df &lt;- islands.df[order(islands.df$Area, decreasing=TRUE), ]\nhead(islands.df, 3)\n\n    Area      LandMass\n3  16988          Asia\n1  11506        Africa\n35  9390 North America\n\n\nWe can use the function barplot() to plot the vector of island areas.\n\npar(mar=c(4, 0, 0, 0)) # change the margin sizes\nbarplot(islands.df$Area)\n\n\n\nFigure 1.11: Island areas with barplot defaults\n\n\n\nThe whole dataset includes a lot of very small areas, so let’s cut it down to just the 10 largest. Since the dataset is already sorted, we can take rows 1:10.\n\nbarplot(islands.df$Area[1:10])\n\n\n\nFigure 1.12: Top 10 island areas\n\n\n\nAnd the next step is to add some names to the x-axis…\n\nbarplot(islands.df$Area[1:10], names=islands.df$LandMass[1:10])\n\n\n\nFigure 1.13: Top 10 island areas with names\n\n\n\nWhich of course are unreadable. The las argument (?par) controls how the axis labels relate to the axis line, so we can try adjusting that…\n\nbarplot(islands.df$Area[1:10], names=islands.df$LandMass[1:10], las=3)\n\n\n\nFigure 1.14: Top 10 island areas with names rotated\n\n\n\nMaybe we just need to make the bars horizontal. To do this, we should adjust the margins again with par(mar=...)), set horiz=TRUE, and las=1, and use [10:1] so the largest is on top.\n\npar(mar=c(4, 10, 0, 0))\nbarplot(islands.df$Area[10:1], names=islands.df$LandMass[10:1], \n        horiz=TRUE, las=1, xlab=\"Area (km2)\")\n\n\n\nFigure 1.15: Finally! Did you know Antarctica is bigger than Europe?\n\n\n\nAs you may have noticed, visualization is an iterative process with lots of trial and error until you find a plot that communicates the message within the data well. There are several packages (e.g., ggplot2) that make these sort of adjustments and explorations less opaque than all of the options in par()."
  },
  {
    "objectID": "P1_R_recap.html#summary-statistics",
    "href": "P1_R_recap.html#summary-statistics",
    "title": "1  R recap",
    "section": "\n1.3 Summary statistics",
    "text": "1.3 Summary statistics\nYou will often need to summarise your data before you present it. Data summaries are usually contained in tables and they can sometimes replace graphics (e.g., where the data is relatively simple or where individual precise values are important). There are many types of summary statistics. Here we are concerned with central tendency and variability.\nQ8. What are the three main measures of central tendency?\nQ9. What are three measures of variability?\nMeasures of central tendency and variability each have pros and cons and you need to be able to apply the most appropriate method to your data. Another summary statistic that you might include is sample size. R is very good at producing summary statistics, and there are myriad ways to produce them. We’ll return to the cars2 dataset.\n\nsummary(cars2) \n\n    dist_ft         speed_mph   \n Min.   :  2.00   Min.   : 4.0  \n 1st Qu.: 26.00   1st Qu.:12.0  \n Median : 36.00   Median :15.0  \n Mean   : 42.98   Mean   :15.4  \n 3rd Qu.: 56.00   3rd Qu.:19.0  \n Max.   :120.00   Max.   :25.0  \n\n\n\nsummary(cars2[cars2$speed_mph &gt; 20, ]) \n\n\n# There are several ways to access a column in a dataframe\nsummary(cars2$speed_mph) \nsummary(cars2[, 2])\nsummary(cars2[, \"speed_mph\"])\nsummary(cars2[, c(\"speed_mph\", \"dist_ft\")])\n\nOften you’ll wish to summarise your data across levels of a certain factor. For example, levels of a certain treatment that you are applying. More complex summaries can be made using the dplyr package. We’ll go into more detail later on some of the very powerful ways this package (and its friends in the tidyverse) can be used.\nWe’ll use the built-in dataset InsectSprays. Viewing your raw data can be an important check as well. You can open a spreadsheet-style viewer in R using View(YourDataFrame).\n\nstr(InsectSprays)\n\n'data.frame':   72 obs. of  2 variables:\n $ count: num  10 7 20 14 14 12 10 23 17 20 ...\n $ spray: Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(InsectSprays) # glimpse() is loaded with tidyverse\n\nRows: 72\nColumns: 2\n$ count &lt;dbl&gt; 10, 7, 20, 14, 14, 12, 10, 23, 17, 20, 14, 13, 11, 17, 21, 11, 1…\n$ spray &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B, B, B…\n\n\n\n# spray is the categorical predictor; count is the response\nView(InsectSprays)\n\nTo do more complex summaries, we’re going to string together a series of functions. This can be done in a nested format (e.g., fun1(fun2(fun3(dataset)))), but this gets unwieldy very quickly.\nSo, let’s use the pipe operator |&gt;. This takes the output from one function and feeds it as the first input of the next (e.g., dataset |&gt; fun3() |&gt; fun2() |&gt; fun1()), making code much more legible. Many functions in the tidyverse are built for piping.\n\n?`|&gt;`\n\n\n# use group_by() with the grouping column name(s)\nspray_summaries &lt;- InsectSprays |&gt;\n  group_by(spray) |&gt;\n  summarise(count_mean=mean(count))\nspray_summaries\n\n\n# it is very easy to calculate any number of summary statistics\nInsectSprays |&gt;\n  group_by(spray) |&gt;\n  summarise(mean=mean(count) |&gt; round(2),\n            median=median(count),\n            max=max(count),\n            sd=sd(count) |&gt; round(2),\n            N=n(),\n            N_over_10=sum(count &gt; 10))\n\n# A tibble: 6 × 7\n  spray  mean median   max    sd     N N_over_10\n  &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;int&gt;\n1 A     14.5    14      23  4.72    12         9\n2 B     15.3    16.5    21  4.27    12        11\n3 C      2.08    1.5     7  1.98    12         0\n4 D      4.92    5      12  2.5     12         1\n5 E      3.5     3       6  1.73    12         0\n6 F     16.7    15      26  6.21    12        10\n\n\n\n1.3.1 Choosing a measure of central tendency\nThe choice of which measure of central tendency to use depends on the nature of the data and objectives of your research. We will use datasets that you downloaded from Brightspace (Practicals &gt; data). Remember to put these into the data folder in your working directory (or modify the file paths in the code accordingly).\n\n# this will load the 'Scallop %fat' data sheet from the xlsx spreadsheet.\nscallop_df &lt;- read_excel(\"data/practical_1.xlsx\", sheet=\"Scallop %fat\")\nstr(scallop_df)\n\ntibble [49 × 1] (S3: tbl_df/tbl/data.frame)\n $ Scallop % fat: num [1:49] 22.5 24.1 18.2 32.5 17.4 23.6 21.5 22.2 27.6 22.2 ...\n\n# avoid spaces and symbols in column names. It's a pain.\nnames(scallop_df) &lt;- \"fat_pct\"\n\nQ10. Check the data using the methods above. Does it look OK to you?\nQ11. Are these data likely to be continuous or discontinuous?\nQ12. Create a plot to visualize the distribution of these data.\nQ13. Do you spot any issues?\n\nhist(scallop_df$fat_pct, main=NULL) # (what does 'main=NULL' do?)\n\n\n\nFigure 1.16: Histogram of fat percentage.\n\n\n\nYou should have spotted a potential outlier. Data entry errors are very common, and a check against the original data sheet shows that the decimal was typed in the wrong place. The following code helps you ID which data entry is in error. We can now search for the ‘odd’ observation i.e. determine in which row the outlier is located.\n\nwhich(scallop_df$fat_pct &gt; 50) \n\n[1] 36\n\nscallop_df$fat_pct[35:37] # row 36 is 99, but should be 9.9\n\n[1] 22.8 99.0 12.9\n\nscallop_df &lt;- scallop_df[, c(1, 1)] # duplicate column\nnames(scallop_df) &lt;- c(\"fat_pct_orig\", \"fat_pct_corr\")\nhead(scallop_df, 2)\n\n# A tibble: 2 × 2\n  fat_pct_orig fat_pct_corr\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         22.5         22.5\n2         24.1         24.1\n\n\n\n# there are many ways to 'fix' the outlier in R.\n# You need to correct the outlier in row 36 of column 'fat_pct_corr'\nscallop_df$fat_pct_corr[36] &lt;- 9.9\nwhich(scallop_df$fat_pct_corr &gt; 90) \n# integer(0) - this means that no elements in fat_pct_corr contain values &gt;90\n\nNow summarise scallop_df using some of the methods above.\nQ14. Create a histogram for the corrected column. How does it differ from the original column with the error?\nQ15. Calculate mean, variance, median, interquartile range, minimum, maximum and range for both fat_pct_orig and fat_pct_corr.\nQ16. Suppose the outlier was even bigger (i.e. you typo was even worse). Adjust your data, multiplying the erroneous data item by 10; copy the ‘_orig’ column and change row 36 in that column to 999.\nQ17. Calculate the same summary statistics.\nQ18. Which measures of central tendency and variability are most ‘robust’ against this outlier?\nOr look individually instead of calculating many metrics at once with dplyr functions:\n\nsummary(scallop_df$fat_pct_corr)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   8.50   16.90   20.60   19.56   22.50   32.50 \n\nvar(scallop_df$fat_pct_corr)\n\n[1] 22.79836\n\nIQR(scallop_df$fat_pct_corr)\n\n[1] 5.6\n\n\nR is excellent at generating well formatted tables such as shown in Table 1.1. What is missing from from this table?\n\n\n\n\nTable 1.1: Summary statistics with and without an outlier. Note which summary stats are most influenced by the outlier.\n\n\n\n\n\n\n\n\n\nColumn\nMean\nMedian\nStandard deviation\nRange\nInterquartile range\n\n\n\nfat_pct_corr\n19.6\n20.6\n4.77\n24.0\n5.6\n\n\nfat_pct_orig\n21.4\n20.6\n12.20\n90.5\n5.3\n\n\n\n\n\n\nQ19. How would the patterns seen in Table 1.1 influence your choice if you were required to summarise data that you thought might contain data that you weren’t sure about? The three measure of central tendency are influenced to different extents by the ‘shape’ of the data they are used to describe.\n\nhake_df &lt;- read_excel(\"data/practical_1.xlsx\", sheet=\"Hake\")\nstr(hake_df) # once again, column names made for excel rather than R\n\ntibble [499 × 2] (S3: tbl_df/tbl/data.frame)\n $ Year            : num [1:499] 1 1 1 1 1 1 1 1 1 1 ...\n $ Hake length (mm): num [1:499] 190 219 181 148 206 204 168 197 178 211 ...\n\n\nQ20. What type of variable is length?\nQ21. Select an appropriate graphical method and display these data.\nQ22. In your own time, use the dplyr functions to summarise the hake data by year.\n\nhake_df$Year &lt;- as.factor(hake_df$Year) # Treat as categorical, not numeric\nnames(hake_df) &lt;- c(\"Year\", \"Length\") # simplify the column names\n\n\n\n\n\nTable 1.2: Summary of hake data.\n\nYear\nMean length (cm)\n\n\n\n1\n201.8\n\n\n2\n497.0\n\n\n3\n988.9\n\n\n\n\n\n\nTry to re-create Table 1.2.\nThe following ‘settling velocity’ data relates to the settling velocity of salmon faecal material. Shona Magill generated these data.\n\nfishPoo_df &lt;- read_excel(\"data/practical_1.xlsx\", sheet=\"Settling velocity\")\nstr(fishPoo_df)\n\ntibble [200 × 1] (S3: tbl_df/tbl/data.frame)\n $ Settling velocity (mm s-1): num [1:200] 2.06 1.03 1.56 1.88 1.16 0.76 1.26 1.13 1.23 1.31 ...\n\n\nQ23. Produce a histogram of the settling velocity. Is it left or right skewed?\nQ24. Which measures of central tendency and variability are most appropriate?\nQ25. Sketch the distribution and indicate the relative positions of the mean and median.\nQ26. Generate a new column of the log-transformed settling velocity data and plot these data.\nQ27. What measures of central tendency and variability could be applied to the log-transformed data? Selecting the preferable measure of central tendency and variability in a dataset is not necessarily straightforward.\nTable 1.3 gives some indication of what issues you might consider.\n\n\n\n\nTable 1.3: Appropriate measures of central tendency and variability according to the underlying data distribution.\n\n\n\n\n\n\nData distribution\nCentral tendency metric\nVariability metric\n\n\n\nContinuous, unimodal, symmetric\nMean\nVariance or sd\n\n\nContinuous, skewed\nMedian\nInterquartile range\n\n\nContinuous, multimodal\nNone; state modes\nNone; summarise by group\n\n\nDiscontinuous\nNone; data-dependent\nRange?"
  },
  {
    "objectID": "P1_R_recap.html#conclusions",
    "href": "P1_R_recap.html#conclusions",
    "title": "1  R recap",
    "section": "\n1.4 Conclusions",
    "text": "1.4 Conclusions\nVisualizing and summarising data are the critical first steps in the data analysis and reporting workflow. We use graphical methods to firstly explore our own data. Once we have made sense of it we select the most appropriate method to convey that understanding to our readers. We may help that communication by summarising data in the most appropriate way taking into account the distribution of the data and the presence of outliers."
  },
  {
    "objectID": "P2_binomial_poisson.html#the-binomial-distribution",
    "href": "P2_binomial_poisson.html#the-binomial-distribution",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.1 The Binomial distribution",
    "text": "2.1 The Binomial distribution\n\n2.1.1 Bernoulli trials\nA Bernoulli trial is a single event with a binary outcome (i.e., a success or a failure). Binary outcomes include:\n\nAlive / dead\nPregnant / not pregnant\nGuilty / not guilty\nHeads / tails\nWin / lose\n\nThings that can have more than two outcomes can be re-coded into two categories. This concept can be extended to numerous situations, such as:\n\nIncome greater than £100,000 vs. less than £100,000\nFlower colour (blue vs. not blue; red vs. not red)\n\nby definition, each Bernoulli trial is independent of all previous trials.\nQ28. If you tossed a fair coin 99 times and each time it landed heads, what is the probability of obtaining a head on the next toss of the coin?\nThis is a different question from asking whether 100 heads or (99 heads + 1 tails) is more likely in a throw of 100 coins. This is different because these events are independent.\n\n2.1.2 The binomial distribution\nThe binomial distribution is a discrete probability distribution that applies to a series of Bernoulli trials. For example, if you had 10 eggs under a (big) chicken and they were all female, you might wonder how likely this was by chance. If this kept happening, you might question whether something was somehow making the eggs female. Here, the outcome can be female or male, and the trial size is 10 (each egg is a ‘trial’ with a binary outcome). You may question the assumption that the ratio female to male was 50:50 and favour an alternative hypothesis that the ratio was more female : less male. The binomial distribution allows you to quantify the probability of getting \\(x\\) females from \\(n\\) eggs for any given probability \\(p=P(female)\\). That is, we are not restricted to 50:50.\nYou need to know two things to use the binomial distribution. These are:\n\nthe number of trials (\\(n\\), or sometimes \\(k\\))\nprobability of success (\\(p\\))\n\nFrom p, we can calculate the probability of failure as \\(q = (1-p)\\), since the two probabilities must sum to 1.\nThe distribution of a binomially distributed variable \\(y\\) is specified \\(y \\sim Binom(n,p)\\). We denote \\(P(x)\\) as the probability of getting \\(x\\) successes where \\(x\\) is an integer \\(0:n\\).\nThe mean of a binomial distribution is \\(n*p\\). This gives you the expected outcome. For example, if \\(P(female)=0.5\\) and \\(n=10\\) eggs the expected number of females is \\(10*0.5=5\\).\n\n2.1.3 Binomial distributions by hand\nWongles always lay two eggs in a clutch but 50% of the eggs are infertile and don’t hatch. We are interested in the proportions that we can expect to hatch from one clutch (two eggs).\nQ29. What is the event here?\nQ30. What is the probability of success?\nQ31. What is the number of trials?\nQ32. How many possible outcomes are there and what are they?\nQ33. Write down the model specification in the standard notation (with parameters)\nQ34. Calculate the expected proportions of clutches of eggs that contain (a) two fertile, (b) two infertile, and (c) one of each.\nOften we are faced with more than two events and the quadratic expansion approach you used above is no longer convenient. In such circumstances you use the binomial expression.\nQ35. Using the binomial probability mass function from Chapter 7, calculate the fertility proportions for two eggs (Q34 a, b, c).\nOozle birds are much more sensible than Wongles and always have broods of eight offspring and all of them hatch. We are interested in modelling the probabilities of the proportion of male and female offspring in broods of eight eggs.\nQ36. What is the Bernoulli event here? (What has two mutually exclusive outcomes?)\nQ37. What are the theoretical limits to your outcomes (i.e, max numbers of each)?\nQ38. What are the possible outcomes, and what is the total number of possible outcomes?\nQ39. Write down the model for Oozle egg sex (\\(y \\sim Binom(n,p)\\)).\nWe will assume that the probability (\\(p\\)) of any offspring being female is 0.5 and being male (\\(q\\)) is 0.5. For the extreme cases where all offspring are one sex, we can use simple probability theory: the probability of getting \\(n\\) females in a brood of size \\(n\\) is equal to \\(p^n\\).\nQ40. Calculate the probability of obtaining eight male offspring.\nQ41. What is the mean number of females you would expect in Oozle broods?\nIt gets more complicated when you want to know the probability of getting, say, 1 male and 7 females from your clutch of eight eggs.\nQ42. Given that \\(p=q\\), what shape would expect the distribution to be?\nQ43. Use the binomial expression to calculate the probability of obtaining 0, 1, 2, 3, 4, 5, 6, 7 and 8 male offspring (note that the distribution is symmetrical).\nIt is much easier, of course, to do this using R.\n\n2.1.4 Binomial distributions in R\nYou can get probabilities for specific outcomes from a massive array of theoretical probability distributions from R. The binomial is just one of them.\n\nnum_female &lt;- 4 # note that 4 is assigned to the variable called num_female\nnum_trials &lt;- 8\np_female &lt;- 0.5\n\n\n# for a single probability: y~Binom(n=8, p=0.5) determine P(y_i=4)\ndbinom(num_female, num_trials, p_female) # dbinom(4, 8, 0.5)\n\n\n# formatted output just because:\npaste0(\"P(\", num_female, \" female | \", num_trials, \" eggs) = \",\n       dbinom(num_female, num_trials, p_female))\n\n[1] \"P(4 female | 8 eggs) = 0.2734375\"\n\n\nOften we want to know cumulative probabilities. This allows us to answer questions like ‘what is the probability of obtaining &lt; 4 females in a brood of 8 eggs?’ Here, &lt;4 equates to the cumulative probability P(0) + P(1) + P(2) + P(3).\n\n# pbinom gives the cumulative probability\npaste(\"The cumulative probability is\", \n      max(pbinom(0:num_female - 1, num_trials, p_female)))\n\n[1] \"The cumulative probability is 0.36328125\"\n\n\nQ44. Why do we parameterise pbinom() with num_female-1 rather than num_female?\nQ45. Would this change if the question was \\(P( \\leq 4)\\)?\nQ46. Take the ’max’ out of the above line and run again. You should see 5 cumulative probabilities. Why is the first cumulative probability zero?\nQ47. Calculate P(&lt; 4 females | 8 eggs) using dbinom() instead of pbinom().\n\n# You can check what R is doing by running parts of code:\n0:num_female - 1 # Oops! Is this what you expected?\n\n[1] -1  0  1  2  3\n\n0:(num_female - 1) # this is actually what we want.\n\n[1] 0 1 2 3\n\n# Correct the code above. Why did this bug have no effect?\n\nQ48. What is the probability of getting 3 females?\nQ49. What is the probability of getting 8 females?\nQ50. What cumulative probabilities would you need to consider to answer the question ‘What is the probability of getting fewer than three females?’\nQ51. Write down the model that describes this random process (number of females per eight eggs). Your answer should be like this: \\(y \\sim Binom(n, p)\\).\nQ52. What is the probability of getting &lt; 4 females?\nQ53. What is the probability of getting \\(\\leq\\) 4 females?\nQ54. What is the probability of getting &gt; than 8 females?\nQ55. What is the probability of getting \\(\\geq\\) than 2 females?\nLet’s visualize these distributions in order to better understand them.\n\n# Run this, then explore values of p_female\n# Note: 'success' and 'failure' is arbitrary. Just make sure you're calculating\n# what you think. How would you calculate the probabilities for males instead?\nnum_female &lt;- seq(0, 8) \np_female &lt;- 0.5 # what are the limits of p_female?\nprFemale_df &lt;- data.frame(num_female = num_female, \n                          prob = dbinom(num_female, max(num_female), p_female)) \nprFemale_df \n\n  num_female       prob\n1          0 0.00390625\n2          1 0.03125000\n3          2 0.10937500\n4          3 0.21875000\n5          4 0.27343750\n6          5 0.21875000\n7          6 0.10937500\n8          7 0.03125000\n9          8 0.00390625\n\nbarplot(prFemale_df$prob, names = prFemale_df$num_female, \n        xlab = \"Number of females\", ylab = \"Probability\")\n\n\n\nFigure 2.1: Binomial probability distribution\n\n\n\n\nprFemale_df$cumul_prob &lt;- cumsum(prFemale_df$prob)\nbarplot(prFemale_df$cumul_prob, names = prFemale_df$num_female, \n        xlab = \"Enter the correct label!\", ylab = \"Enter the correct label!\")\n\n\n\nFigure 2.2: Cumulative binomial probability distribution\n\n\n\nTry re-plotting so that the two panels appear side by side (hint: par(mfrow=c(...))).\nNote \\(P(y_i=x)\\) is read as ‘the probability that a random observation \\(y_i\\) equals \\(x\\)’. Sometimes this includes conditions: \\(P(y_i=x|n,p)\\), which is read as the probability that \\(y_i\\) equals \\(x\\) given \\(n\\) and \\(p\\). You may see \\(P()\\), \\(Pr()\\), \\(p()\\), or \\(Prob()\\), which all mean the same thing.\nThe interpretation of \\(P(y_i=8\\ |\\ n=8,\\ p=0.5) = 0.00391\\) is that the probability of 8 female offspring in a clutch of 8 eggs is 0.00391.\nIn other words, if we have 500 broods, each with 8 eggs, we expect \\(500 * 0.00391 = 1.95 \\approx 2\\) broods to be all female. Is there something strange about our Oozle or is it just one of the 2/500 by chance?\nQ56. Why is a bar graph the appropriate plot here?\nQ57. What do you notice about the shape of the distribution when \\(p=q=0.5\\)?\nQ58. Re-run the analysis with the probability of female as 0.8. Plot the results.\nQ59. What is the shape of the distribution now?"
  },
  {
    "objectID": "P2_binomial_poisson.html#the-poisson-distribution",
    "href": "P2_binomial_poisson.html#the-poisson-distribution",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.2 The Poisson distribution",
    "text": "2.2 The Poisson distribution\nThe Poisson distribution is another probability distribution that describes discrete events that occur in space and/or time. The Poisson distribution is used to model (predict) the distribution of events that are rare, random, and independent. This can include events like earthquakes, storms, or the number of bends arriving at the SAMS recompression facility.\nThe Poisson distribution takes a single parameter: the mean. If a variable is Poisson distributed, its variance will equal its mean. This is a diagnostic feature of the distribution. The Poisson distribution is a discrete probability distribution, but its parameter, the mean, is continuous (similar to continuous \\(p\\) for the discrete binomial distribution).\nThe Poisson formula is simple - find it in Chapter 7 . Here, \\(\\bar{y}\\) is the mean, \\(x\\) is the outcome of interest, \\(e\\) is Euler’s number, and \\(!\\) is factorial. Note that here, we use \\(\\bar{y}\\) as our stand-in for the population parameter \\(\\lambda = \\mu = \\sigma\\) which defines the poisson distribution. Translate this formula into an R function by completing this code:\n\n# hint: exp(2) = e^2\n# hint: factorial(3) = 3!\ncalc_poisson_prob &lt;- function(x, y_bar) {\n  \n}\n\n\n2.2.1 Poisson distributions by hand\nThese problems usually start by asking you to calculate the mean number of observations made per unit of space or time. This is the Poisson parameter, and is referred to as the rate or as lambda (\\(\\lambda\\)). The unit could be spatial (e.g., per \\(m^2\\)) or temporal (e.g., per hour).\nConsider randomly throwing a 1\\(m^2\\) quadrat repeatedly on a sandy beach covered in worm casts Figure 2.3. You then count the number of casts in each quadrat.\n\n\nFigure 2.3: Worm casts on a sandy beach with 1x1 m quadrats.\n\nFrom these data you can calculate the mean number of observations per unit.\nThe code below generates a dataframe from which you can determine that the mean number of worms per quadrat is 1.41. We wish to predict the proportion of quadrats that would contain 0, 1, 2, 3, 4, and 5 worms, assuming that the worms are independently distributed across space (i.e., random: one worm’s location has no effect on another worm’s location, and there is no relevant environmental variation).\nWe thus wish to determine the probability of observing our data assuming that the worms casts are randomly distributed. To do this, we need to know the probability of observing each count (zero to infinity), given the mean count per quadrat.\n\n# num_worms is the number of worms per quadrat\n# num_quadrats is numbers of quadrats observed containing that number of worms\n# this dataset is summarised. Raw data might have columns: quadrat_id, num_worms\nworm_df &lt;- data.frame(num_worms = c(0, 1, 2, 3, 4, 5, 6),\n                      num_quadrats = c(35, 28, 15, 10, 7, 5, 0))\nworm_df # display dataframe\n\n  num_worms num_quadrats\n1         0           35\n2         1           28\n3         2           15\n4         3           10\n5         4            7\n6         5            5\n7         6            0\n\n\nAs the number of worms per quadrat is low compared to the number of worms that could exist within a quadrat we consider these are rare events and hence can be reasonably described by a Poisson distribution (assuming worms occur independently).\nFrom just the mean, we can calculate the expected frequency of observing different numbers of worms in any quadrat (assuming the model assumptions are met - remind yourself what these assumptions are). The number of worms per quadrat (\\(y\\)) is discrete; it can only take integers greater or equal to zero.\nProbability questions often includes terms such as \\(P(y_i \\leq a)\\). That is, the probability that an observation \\(i\\) of the random variable \\(y\\) is less than or equal to \\(a\\). For example, in this context you might get asked for \\(P(y_i \\leq 1)\\) which asks what is the probability of a random quadrat being thrown (\\(y_i\\)) containing 1 or fewer worm casts (\\(a\\), an integer value). To be fully complete, we might even write \\(P(y_i \\leq 1 | \\lambda)\\), which acknowledges that we know the (sample) mean.\nSo, if you are interested in predicting the probability of obtaining 1 or fewer worms per quadrat, you would start your calculation by writing \\(P(y_i = 0) + P(y_i=1) = \\dots\\).\nQ60. Use your calc_poisson_prob() function to calculate the expected frequency of 0, 1 & 2 worms per quadrat.\nQ61. What calculation would you need to conduct to determine \\(P(y_i \\geq 1)\\)?\nR has built-in functions for calculating these, but it is important to know what you are asking them to calculate.\n\n2.2.2 Poisson distributions in R\nYou can determine the expected probabilities for each worm count per quadrat once you have determined the mean count of worms per quadrat. Since we have a summarised dataset (i.e., the number of observations num_quadrats for each number of worms num_worms, rather than the raw data with a row for each quadrat), we need to do some calculations. The mean number of worms per quadrat = (total number of worms) / (total number of quadrats).\n\nsum(worm_df$num_quadrats) # total number of quadrats\n\n[1] 100\n\nlambda_worms &lt;- with(worm_df, sum(num_worms * num_quadrats) / sum(num_quadrats))\nlambda_worms # the mean worms per quadrat\"\n\n[1] 1.41\n\n\nGiven this, we can find \\(P(y_i \\leq 5)\\) (that is, the probability of a random quadrat (\\(y_i\\)) containing five or fewer worms). With a mean of 1.41 worms per quadrat, \\(P(y_i \\leq 5) = 0.997\\) (3 sf). This means that, if your data are Poisson distributed, it is highly likely that there will be fewer than five worms in your quadrat if \\(\\lambda = 1.41\\).\nQ62. If the mean number of worms was 3 per quadrat, would you be more or less likely to get five worms in your quadrat?\nTo do this in R, we use dpois() and ppois():\n\n# ?dpois \n# for questions like 'determine P(y_i=a | lambda)'\na &lt;- 5 \nlambda &lt;- 1.41 \ndpois(a, lambda) # probability of observing 'a' worms per quadrat: dpois()\n\n[1] 0.01133859\n\n# ?ppois\n# for questions like 'determine P(y_i &gt;= a | lambda)\n1-ppois(a-1, lambda) # probability of observing &gt;= 'a' worms: ppois()\n\n[1] 0.01465169\n\n\n\nppois(0:a, lambda) # what does 0:a mean? What's another way to do this? \n\n[1] 0.2441433 0.5883853 0.8310759 0.9451405 0.9853483 0.9966869\n\nsignif(ppois(0:a, lambda), 3) # round with ?signif\n\n[1] 0.244 0.588 0.831 0.945 0.985 0.997\n\nbarplot(dpois(0:a, lambda),\n        ylab = \"Probability\", xlab = \"Number of worms\",\n        space = 0.2, ylim = c(0, 0.5), names.arg = 0:a)\n\n\n\nFigure 2.4: Poisson probability distribution.\n\n\n\nNow change the plot so that the cumulative probabilities are plotted. You will need to change the code to use ppois() instead of dpois(), as well as the y-axis limits (ylim).\n\n# create new columns in worm_df for the probabilities\nworm_df$prob &lt;- dpois(worm_df$num_worms, lambda)\nworm_df$cumul_prob &lt;- cumsum(worm_df$prob)\n\n\n# Make Table 2.1. Use packageName::function() instead of loading with library()\nknitr::kable(worm_df, digits=5) \n\n\n\nTable 2.1: Worm cast observations and expected probabilities.\n\nnum_worms\nnum_quadrats\nprob\ncumul_prob\n\n\n\n0\n35\n0.24414\n0.24414\n\n\n1\n28\n0.34424\n0.58839\n\n\n2\n15\n0.24269\n0.83108\n\n\n3\n10\n0.11406\n0.94514\n\n\n4\n7\n0.04021\n0.98535\n\n\n5\n5\n0.01134\n0.99669\n\n\n6\n0\n0.00266\n0.99935\n\n\n\n\n\n\nQ64. Format the probabilities in the table to 3 decimal places.\nIf individual probability values (not cumulative probability) are multiplied by the total number of quadrats thrown (100), we generate the expected frequency distribution for comparison with the observed results above.\nQ65. Write the appropriate code to add a column(called num_quadrats_expected) to ‘worm_df’ that is the expected number of quadrats (given 100 quadrats total).\nThis is the number of quadrats that you would expect to contain 0, 1, …, 5 worms, given that the mean density of worms is 1.41 per m\\(^2\\). Note that the number of worms per quadrat is a discrete variable, but you can have non-integer ‘expectations’ (i.e. means).\nQ66. Add another column that is the difference in the observed number of quadrats and num_quadrats_expected.\nQ67. Produce a bar graph of this difference."
  },
  {
    "objectID": "P2_binomial_poisson.html#the-poisson-approximation-of-the-binomial-model",
    "href": "P2_binomial_poisson.html#the-poisson-approximation-of-the-binomial-model",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.3 The Poisson approximation of the binomial model",
    "text": "2.3 The Poisson approximation of the binomial model\nWhere the number of trials is large and the probability of success is small, the Poisson distribution can be used as an approximation of the binomial distribution. Under these circumstances you can calculate the mean of a variable that has a binomial distribution (number of trials x probability of success; \\(n*p\\)) and use that as an estimate of \\(\\lambda\\) in the Poisson model. The reason why you may wish to do this is because the Poisson model is much easier to use than the binomial model when \\(n\\) is large.\nThere are several rules of thumb that apply here. Some say that when \\(p&lt;0.1\\), the Poisson approximation may be preferable to the binomial, other texts state that \\(n\\) should be &gt; 50 and \\(p&lt;0.05\\) (so \\(n * p = \\lambda = 2.5\\)) whilst others state \\(n&gt;100\\) and \\(n*p&lt;10\\). The point is that as \\(n\\) increases and \\(p\\) decreases, the approximation gets better.\nThere are an infinite number of ways of multiplying two numbers together to get 5. Call our numbers \\(p\\) and \\(k\\). For example, when \\(p=0.5\\), \\(n=10\\) OR when \\(p=0.05\\), \\(n=100\\), \\(n*p=5\\). We can use this to illustrate the Poisson approximation of the binomial distribution.\n\n# generate dataframe with probability for 0:16 'successes' from different \n# distributions but where mean is 5.\n# note that in y ~ Binom(10,0.5), probability of &gt;10 successes is zero.\n# We're going to leverage the power of the tidyverse here\ny_seq &lt;- 0:16\nbinom_df &lt;- tibble(y=rep(y_seq, times=3), # ?rep\n                   n=rep(c(10, 20, 100), each=length(y_seq)),\n                   p=rep(c(0.5, 0.25, 0.05), each=length(y_seq))) |&gt;\n    mutate(mean=n*p,\n           prob=dbinom(y_seq, n, p),\n           label=paste0(\"y ~ Binom(\", n, \", \", p, \")\"))\npois_df &lt;- tibble(y=y_seq,\n                  n=NA, \n                  p=NA,\n                  mean=5) |&gt;\n    mutate(prob=dpois(y_seq, mean),\n           label=paste0(\"y ~ Pois(\", mean, \")\"))\nPDF_df &lt;- bind_rows(binom_df, pois_df) |&gt;\n  mutate(label=factor(label, levels=unique(label)))\n\n\nhead(binom_df, 2)\n\n# A tibble: 2 × 6\n      y     n     p  mean     prob label             \n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;             \n1     0    10   0.5     5 0.000977 y ~ Binom(10, 0.5)\n2     1    10   0.5     5 0.00977  y ~ Binom(10, 0.5)\n\nhead(pois_df, 2)\n\n# A tibble: 2 × 6\n      y n     p      mean    prob label      \n  &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1     0 NA    NA        5 0.00674 y ~ Pois(5)\n2     1 NA    NA        5 0.0337  y ~ Pois(5)\n\n\n\nggplot(PDF_df, aes(y, prob, fill=label)) +  # ggplot(data, aes(xVar, yVar))\n  geom_bar(stat=\"identity\", position=\"dodge\", colour=\"grey30\") +  # ?geom_bar\n  scale_fill_brewer(\"Distribution\", palette=\"Purples\") + # from colorbrewer2.org\n  labs(x=\"Number of successes with mean = 5\", y=\"Probability\") +\n  theme_classic() + \n  theme(legend.position=c(0.85, 0.85))\n\n\n\nFigure 2.5: Binomial and Poisson distributions converge with larger numbers of events, depending on p.\n\n\n\nQ68. What is the modal value in these distributions?\nGenerate some random numbers from the distributions in Figure 2.5 and calculate their mean and variance. Here is \\(y \\sim Pois(\\lambda=5)\\):\n\ny &lt;- rpois(10000, 5)\npaste0(\"Mean: \", signif(mean(y), 3), \", variance: \", signif(var(y), 3))\n\n[1] \"Mean: 5.01, variance: 4.97\"\n\n\nQ69. What do you notice about the mean and variance in the Poisson model?\nQ70. What do you notice about the mean and variance in the binomial models as \\(n\\) increases and \\(p\\) decreases? When is it more similar to the Poisson? Is this what you expected?"
  },
  {
    "objectID": "P2_binomial_poisson.html#conclusions",
    "href": "P2_binomial_poisson.html#conclusions",
    "title": "2  Binomial and Poisson Distributions",
    "section": "\n2.4 Conclusions",
    "text": "2.4 Conclusions\nThe binomial distribution is a discrete probability distribution that models situations where the outcome of an observation or experiment is binary (i.e., two possibilities) or can be coded as such. The binomial model enables us to predict the probability of making our observation or series of independent observations for any given probability of a success (\\(p\\)). This enables us to quantify how likely our observation is to have occurred, by chance. If the chance of our observation is very low, we can challenge the hypothesis with regard the probability of success (\\(p\\)) and suggest a different value.\nThe Poisson distribution is another discrete probability distribution that is used to predict the chance occurrence of observations that are rare, independent and randomly distributed with mean = variance = \\(\\lambda\\). The Poisson distribution can be used as an approximation of the binomial distribution where the number of trials (\\(n\\)) is large and the probability of success (\\(p\\)) is small. This approximation is useful as, unlike the Poisson distribution, the binomial calculation requires the handling of massive numbers (from large factorials)."
  },
  {
    "objectID": "P3_normal.html#using-the-normal-probability-distributions",
    "href": "P3_normal.html#using-the-normal-probability-distributions",
    "title": "3  Normal distribution",
    "section": "\n3.1 Using the normal probability distributions",
    "text": "3.1 Using the normal probability distributions\nThe length, in cm, of a catch of herring was measured. Five hundred individuals were studied. We will consider this group to be the entire population of interest. The population parameters are: mean length 37.6 cm, standard deviation 1.20 cm.\nQ72. What are the theoretical limits of the normal distribution?\nQ73. Do you think normality a fair assumption for these data?\nQ74. Assume length of herring is approximately normally distributed and write down the model which describes the fish length distribution ( \\(y_i \\sim Norm(\\mu, \\sigma)\\) ).\nWhen we have population parameters such as we have here, we can calculate Z scores for individuals (or groups of individuals) from that population and calculate how unusual they are. For the moment, we are interested in determining what proportion of fish from this population are expected to be &lt; 38 cm.\n\nmu &lt;- 37.6\nsigma &lt;- 1.2\ny &lt;- 38\n\ny_l38_df &lt;- tibble(len=seq(mu-3*sigma, mu+3*sigma, length.out=1e3),\n                     density=dnorm(len, mu, sigma),\n                     shade=len &lt; 38)\nggplot(y_l38_df, aes(len, ymin=0, ymax=density, fill=shade)) + \n  geom_ribbon(colour=\"grey30\") + \n  scale_fill_manual(values=c(\"white\", \"red3\"), guide=\"none\") +\n  labs(x=\"Herring length (cm)\", y=\"density\") +\n  theme_classic()\n\n\n\nFigure 3.1: Normal distribution of herring lengths.\n\n\n\n\n3.1.1 Using R to calculate areas under the normal curve\nTo solve this problem using R we use the cumulative probability. Observations at the extreme low end are extremely unlikely, but the probability of observing data increases and reaches a maximum at the mean, after which it declines again. The normal distribution is symmetrical.\nQ77. What shape is the cumulative probability curve of a normally distributed variable?\nYou can ask R to determine the probabilities of an event occurring within a normal distribution. For example, you could ask what the probability is of observing a fish greater than 38 cm, when the mean is 37.6 cm and the standard deviation is 1.20 cm. You need to specify the model, and the problem. Use ?dnorm or ?pnorm and look at your options (Figure 3.2).\n\n# cumulative probability: which bit of the curve does this relate to?\npnorm(q = 38, mean = 37.6, sd = 1.2) \n\nQ78. Which area on Figure 3.1 are we interested in?\n\n\n\n\nFigure 3.2: Functions for distributions in R illustrated with a standard normal.\n\n\n\nWe can plot any normal distribution we like. Play with the values for mu and sigma below, adjusting the values for from and to as needed to see the distribution.\n\nmu &lt;- 37.6  # population mean\nsigma &lt;- 1.2  # population sd\ncurve(dnorm(x, mean = mu, sd = sigma), from = 30, to = 45,\n      main = \"Normal density\", ylab = \"Density\", xlab = \"Fish length, cm\")\n\nI can define a population like this \\(y \\sim Norm(37.6, 1.2)\\) and ask questions about \\(P(y_i \\leq a)\\). E.g., what is the probability that a random fish drawn from this population is &lt;38 cm: \\(P(y_i &lt; 38)\\)? The answer to that is 0.6306 or approximately 63%."
  },
  {
    "objectID": "P3_normal.html#normal-model-adequacy",
    "href": "P3_normal.html#normal-model-adequacy",
    "title": "3  Normal distribution",
    "section": "\n3.2 Normal model adequacy",
    "text": "3.2 Normal model adequacy\nA separate fish population (long-eared wrasse) is described as \\(y \\sim Norm(5.14 cm, 25 cm)\\).\nQ81. What is the mean length and standard deviation of long-eared wrasse?\nQ82. Calculate the proportion of fish that are expected to be less than zero cm in length.\nQ84. What do your results indicate about the adequacy of the normal model to describe the length distribution of long-eared wrasse?\nQ85. Recalculate your answers with 1/10th the standard deviation: \\(y \\sim Norm(5.14, 2.5)\\)."
  },
  {
    "objectID": "P3_normal.html#calculating-quantiles-in-the-normal-distribution",
    "href": "P3_normal.html#calculating-quantiles-in-the-normal-distribution",
    "title": "3  Normal distribution",
    "section": "\n3.3 Calculating quantiles in the normal distribution",
    "text": "3.3 Calculating quantiles in the normal distribution\nA quantile is one of a series of values that divides a frequency distribution (i.e. a set of numbers) into equally represented groups (i.e., the same number of observations per group). For example, there are three values (Q1, Q2, Q3) that split a normal distribution into four groups (negative infinity to Q1, Q1 to Q2 (median), Q2 to Q3, and Q3 to positive infinity). Q3 - Q1 is the middle 50% of the data: the interquartile range.\nThere are 99 quantiles, called percentiles, that split your data into 100 groups. The 2.5 percentile is the value that splits your data into two groups corresponding to 2.5% along the distribution (from negative infinity for the normal distribution). Just as we can ask what proportion of a distribution is above or below a set value, we can also ask between what values will a given percentage of my data lie (e.g. what values correspond to the middle 95%?).\nTo solve this problem using R we use the quantile function qnorm(), which uses the cumulative probability.\n\nqnorm(p = 0.975, mean = 37.6, sd = 1.2) # p is the cumulative probability \n\n[1] 39.95196\n\nprobs &lt;- seq(from = 0.1, to = 0.9, by = 0.2) # vectorize for multiple values\nrbind(probs, quantiles=qnorm(p = probs, mean = 37.6, sd = 1.2))\n\n              [,1]     [,2] [,3]     [,4]     [,5]\nprobs      0.10000  0.30000  0.5  0.70000  0.90000\nquantiles 36.06214 36.97072 37.6 38.22928 39.13786\n\nprobs_95 &lt;- c(0.025, 0.975) # middle 95% (2.5% on either side)\nqnorm(p = probs_95, mean = 37.6, sd = 1.2) # round as appropriate\n\n[1] 35.24804 39.95196\n\n\nThese values (probs_95 = c(0.025, 0.975)) identify proportions of the cumulative curve. That is, they identify the bottom 2.5% and the bottom 97.5% of the curve. Values between these two parameters constitute the central 95% of your data.\nLet’s return to the herring data where the population was \\(y \\sim Norm(37.6, 1.2)\\).\nQ86. Find the values that capture the middle 95% and 90% of the herring data.\nWe quote our mean and interval like this: “The mean fish length and 95% interval was 37.6 cm (35.2 – 40.0 cm)”. Remember to use to same degree of precision (significant figures) for confidence intervals as was used to gather the data (or as specified in the question, defaulting to three).\nQ87. What value would you expect to correspond to 0.5 (i.e., what value would be found half-way along your distribution)?\nQ88. Check your answer above using qnorm()\nTo visualize regions of the normal distribution, let’s use ggplot.\n\n# The population: y ~ Norm(mean=37.6, sd=1.2) \nmu &lt;- 37.6\nsigma &lt;- 1.2\n# lower boundary and upper boundary of the region of interest\nlb &lt;- 36\nub &lt;- 40 \n\nnorm_df &lt;- tibble(z=seq(-4, 4, length.out=100),\n                  x=z*sigma + mu,\n                  densNorm=dnorm(x, mu, sigma))\nggplot(norm_df, aes(x, densNorm)) + \n  geom_line() + \n  geom_ribbon(data=norm_df |&gt; filter(x &gt; lb & x &lt; ub),\n              aes(ymin=0, ymax=densNorm), fill=\"steelblue\") +\n  labs(x=\"Herring length (cm)\", \n       y=\"Probability density\",\n       subtitle=paste0(\"P(\", lb, \"&lt; y &lt;\", ub, \") = \", \n                       signif(pnorm(ub, mu, sigma) - pnorm(lb, mu, sigma), digits=3))) +\n  theme_classic() # changes how the plot looks: ?theme\n\n\n\nFigure 3.3: The normal distribution illustrating probability of a random herring from your population having a length between 36 and 40 cm.\n\n\n\nQ89. Change the parameters in the above to check that the values you generated for the 95% interval correspond when plugged into lb and ub in the code.\nQ90. What is the difference between \\(&lt; x\\) and \\(\\leq x\\) when applied to continuous data?\nQ91. Does this also apply to discontinuous data?\nThink about how intervals change when the population standard deviation changes.\nQ92. With everything else equal, try doubling, quadrupling, and halving the standard deviation on the herring data then re-running the same code.\nQ93. What effect does this have on your interval?"
  },
  {
    "objectID": "P3_normal.html#testing-for-normality",
    "href": "P3_normal.html#testing-for-normality",
    "title": "3  Normal distribution",
    "section": "\n3.4 Testing for normality",
    "text": "3.4 Testing for normality\nMany statistical tests assume that data are reasonably approximated by a normal distribution and have homogeneous variance. R can be used to formally test the assumption that data are normally distributed, though you should have some idea of whether this is likely through consideration of the data source. This applies particularly where you have a small sample size which makes evaluating the distribution challenging.\nThe data you collect will be part of a population. The normality check assesses the viability of the assumption that the data you collected were drawn from a population that was normally distributed. Note that populations are, in practice, never actually normally distributed. Your test is to assess how reasonable the assumption of normality is.\n\nMS &lt;- read_xlsx(\"data/practical_3_4.xlsx\", sheet = \"Mood shrimp\")\n# check the data using head(), str() etc.\n\n\npar(mfrow = c(1, 2))\nqqnorm(MS$Shrimp1)\nqqline(MS$Shrimp1, col = 2) # the data fall near the line\nqqnorm(MS$Shrimp2)\nqqline(MS$Shrimp2, col = 2) # the data deviate widely from the line\n\n\n\nNormal (QQ) plots. The left indicates that the normality assumption might be reasonable, not so on the right.\n\n\n\nThe axes on the probability plots have been rescaled so that, if your data are perfectly normal, the data points would fall on a straight line (the red line on the graphs generated by qqline()). Any deviation from the red straight line in your data indicates a lack of normality. What we need to assess is how serious any deviation is and whether it is sufficient to indicate that the assumption of normality is not ‘reasonable’. This is a subjective decision, and two different statisticians may tell you different answers about the same data.\nAs your sample size decreases, it will be increasingly difficult to see if your data are reasonably approximated by a normal distribution. There are many formal statistical methods for assessing normality, but as we already know it is impossible for your data to have been drawn from a population that is normal and this means such tests are largely redundant. You must assess the assumptions of your model (e.g. the general linear model, which includes ANOVA and linear regression), but you should be aware that all data fails the assumptions. However, this doesn’t mean the outputs from such models aren’t useful - provided the assumptions are ‘reasonably’ well met. There is no hard rule as to what constitutes ‘reasonable’!\nQ94. Make histograms of both Shrimp1 and Shrimp2. Comment on their apparent distributions."
  },
  {
    "objectID": "P3_normal.html#transformations",
    "href": "P3_normal.html#transformations",
    "title": "3  Normal distribution",
    "section": "\n3.5 Data transformations",
    "text": "3.5 Data transformations\nThe assumption that our sample data are drawn from a normally distributed population is central to the use of many important inferential statistical techniques. However, frequently it is not reasonable to assume that data are likely to be normally distributed and they ‘fail’ normality tests (e.g. clearly do not fall on the red qqline()).\nNon-normality often occurs because our measurements are near a logical zero. For example, chemical concentrations (e.g., of zinc in sediments) is limited by zero; you cannot have negative zinc concentrations. Similarly, you cannot have negative lengths, time, mass, proportions, etc. Where data is collected ‘near’ a logical zero they are often not normally distributed, since the normal distribution predicts a negative-value ‘tail’ which is impossible.\nThere are several simple mathematical transformation options that you can use in an attempt to convert your data to something that is reasonably approximated by a normal distribution even if it is not well approximated in the original measurement units.\nHere we will consider four common transformations. These are the log (base 10 or natural), the square-root, the arcsine, and the logit transformation. Often transformations will also correct unequal variances (see Chapter 5) in addition to non-normality so they are very useful. The appropriate transformation depends on the data.\n\nLog (log(), inverse: exp()): When the distribution is skewed right and all values are &gt;0. Often ‘cures’ heteroscedasticity.\n\nSquare-root (sqrt(), inverse: ^2): When the measurements are areas (e.g., leaf areas). Often used to ‘down-weight’ common species (e.g., Chapter 6), which is unrelated to model assumptions. Values must be &gt;0.\n\nArcsine (asin(), inverse: sin()^2): When the measurements are proportions. Tends to stretch out the tails (e.g., near 0 or 1 for proportions) and squash the middle (e.g., near 0.5).\n\nLogit (boot::logit(), inverse: boot::inv.logit()): Used for proportions excluding 0 and 1. Also commonly used in models with a binary response variable (e.g., survival probability predicted by temperature, where the response variable is ‘alive’/‘dead’).\n\nIdentifying the correct transformation can be led by an underlying comprehension of the nature of the data. However, this often doesn’t work so expect some trial and error.\n\npositive_df &lt;- tibble(orig=seq(0.01, 10, length.out=1e3)) |&gt;\n  mutate(sqrt=sqrt(orig),\n         ln=log(orig),\n         inv.logit=boot::inv.logit(orig),\n         squared=orig^2) |&gt;\n  pivot_longer(cols=2:5, names_to=\"transformation\", values_to=\"new_value\")\n\nggplot(positive_df, aes(orig, new_value)) + \n  geom_line() + \n  facet_wrap(~transformation, scales=\"free\", nrow=1) + \n  labs(main=\"Positive values\", x=\"Original\", y=\"Transformed\")\n\n\n\nFigure 3.4: Effect of common transformations on positive data.\n\n\n\n\nproportion_df &lt;- tibble(orig=seq(0.01, 0.99, length.out=1e3)) |&gt;\n  mutate(sqrt=sqrt(orig),\n         ln=log(orig),\n         asin=asin(orig),\n         logit=boot::logit(orig)) |&gt;\n  pivot_longer(cols=2:5, names_to=\"transformation\", values_to=\"new_value\")\n\nggplot(proportion_df, aes(orig, new_value)) + \n  geom_line() + \n  facet_wrap(~transformation, scales=\"free\", nrow=1) + \n  labs(main=\"Proportions\", x=\"Original value\", y=\"Transformed value\")\n\n\n\nFigure 3.5: Effect of common transformations on proportions.\n\n\n\nQ95. Using the Radon concentration worksheet in practical_3_4.xlsx, plot the data. Do they look normally distributed?\nQ96. In your own time,use R to determine the log, square root, and reciprocals (and combinations of all of them: at least one converts the data to approximate normality).\n\n# get the Radon data into R; the units are parts per billion\nreadxl::excel_sheets(\"data/practical_3_4.xlsx\") \n\n [1] \"Cod lengths\"           \"Shrimps\"               \"Caffeine\"             \n [4] \"Non-parametric shrimp\" \"Mood shrimp\"           \"Mood shrimp2\"         \n [7] \"Swimming activity\"     \"Clare's Al data\"       \"Distribution\"         \n[10] \"Radon (ppb)\"           \"Zinc conc\"             \"Heart-beat,2012\"      \n[13] \"Sheet1\"               \n\n# load the dataset and try some transformations\n\nQ97. Generate a vector of numbers and then use sqrt() on them."
  },
  {
    "objectID": "P3_normal.html#functions-in-r",
    "href": "P3_normal.html#functions-in-r",
    "title": "3  Normal distribution",
    "section": "\n3.6 Functions in R",
    "text": "3.6 Functions in R\nEverything that does in R is a function. Writing your own functions becomes useful as you use R more and more often. It can keep your code legible and reduce work for yourself (since any changes need to be done in only one location instead of everywhere you’ve repeated the same code).\nWe’ll create a function called trim_values(), which takes a single argument (x), formats it to three digits, and returns the formatted output. Play around with the function to see how it works. We’ll use this function to ‘clean’ our output. In order to use the function, you must run the code defining it first. You will see your functions listed (alongside your dataframes, vectors, lists, etc) in RStudio’s ‘Environment’ pane.\n\ntrim_values &lt;- function(x) {\n  y &lt;- format(x, digits = 3)\n  return(y)\n} \n\n# After running the code above, we can use the function:\ntrim_values(1.23456)\n\n[1] \"1.23\"\n\nlong_numbers &lt;- rnorm(3, 2, 1)\ndata.frame(orig=long_numbers,\n           clean=trim_values(long_numbers))\n\n        orig clean\n1 1.68104348  1.68\n2 0.06002108  0.06\n3 1.79460067  1.79\n\n\nNotice anything strange about the output? The function format() converts the vector from numeric to character to display each value with the same number of characters. The digits=3 argument is also taken as a suggestion (kind of like breaks in hist()). Try some of the other options in format() or adjust the digits argument, re-running the function definition each time to see the changes."
  },
  {
    "objectID": "P3_normal.html#the-central-limit-theorem",
    "href": "P3_normal.html#the-central-limit-theorem",
    "title": "3  Normal distribution",
    "section": "\n3.7 The central limit theorem",
    "text": "3.7 The central limit theorem\nThe central limit theorem (CLT) is about how sample means are distributed.\nThe CLT states that the means of normally distributed data will, themselves, be normally distributed. In addition, the theorem states that the means of data which are NOT normally distributed will be normally distributed if the sample size is sufficiently large. In this practical we are going to demonstrate this theorem using random data generated from various probability distributions.\n\n3.7.1 The distribution of means from various data distributions\nThe following code generates a non-normally distributed dataset (obs_data), repeatedly takes samples from it (num_samples samples, with each sample made up of sample_size random observations from obs_data), and then plots histograms and QQ-plots for the raw data and for the sample means.\nCopy the code and experiment with the sample_size.\n\n# Run this code, then adjust the values for obs_data as you like.\n# Play with more distributions if you want: ?stats::distributions\nobs_data &lt;- c(rnorm(2000, 200, 20), \n              rnorm(1500, 100, 20), \n              rlnorm(100, 5, 0.5),\n              rpois(1000, 15), \n              rpois(500, 30))\nxlims &lt;- range(obs_data)\n\n# Define size of each sample and the number of sampling repeats\nsample_size &lt;- 30\nnum_samples &lt;- 10000\n\n# Sample num_samples times from obs_data, with n = sample_size for each sample\nsample_means &lt;- numeric(length=num_samples) # initialize an empty vector\nfor(i in 1:num_samples) {\n  sample_i &lt;- sample(obs_data, sample_size, replace=T)\n  sample_means[i] &lt;- mean(sample_i)\n}\n\n# plot observed distribution\npar(mfrow = c(2, 2))\npar(mar = c(5, 2, 2, 2)) \nhist(obs_data,\n     main = \"Raw data: obs_data\",\n     sub = paste0(\"mean: \", trim_values(mean(obs_data)), \n                  \", sd: \", trim_values(sd(obs_data))),\n     breaks = 30, xlab = \"Observed length (cm)\", xlim = xlims\n) \nqqnorm(obs_data, main = \"Raw data QQ\")\nqqline(obs_data)\n\n# plot distribution of sample means\nhist(sample_means,\n     main = paste0(\"Sample means, N: \", sample_size),\n     sub = paste0(\"mean: \", trim_values(mean(sample_means)), \n                  \", sd: \", trim_values(sd(sample_means))),\n     breaks = 30, xlab = \"Mean length (cm)\", xlim = xlims\n) \nqqnorm(sample_means, main=\"Sample mean QQ\")\nqqline(sample_means)\n\n\n\nFigure 3.6: The central limit theorem in action (when n&gt;30).\n\n\n\nQ98. What do you notice about the location of the mean as a function of N?\nQ99. What do you notice about the spread around the mean of sample means as a function of sample size – why did the pattern you have observed occur?\nThe CLT states that sample_means will be normally distributed if (a) the underlying data are normally distributed, OR (b) the sample_size is large enough. With R, we can see this in action.\nNote also the relationship between the sample size and the range of values for the sample mean. How does the standard deviation of sample_means change with changes in sample_size?."
  },
  {
    "objectID": "P3_normal.html#the-standard-error-of-the-mean",
    "href": "P3_normal.html#the-standard-error-of-the-mean",
    "title": "3  Normal distribution",
    "section": "\n3.8 The standard error of the mean",
    "text": "3.8 The standard error of the mean\nThe standard error of the mean is the standard deviation of sample means, of a given sample size, taken from a population. It is the standard deviation of the distribution shown in the lower histogram in Figure 3.6 : sd(sample_means). Usually we only have a single sample rather than 10,000 (=num_samples). In that case, we estimate it from single sample as \\({SE}_{\\bar{y}} = \\frac{sd(y)}{\\sqrt{n}}\\), where \\(y\\) is a vector of \\(n\\) observations.\n\n# Simulate a normally distributed population with mean mu and sd sigma\nmu &lt;- 10\nsigma &lt;- 2\nsim_obs &lt;- rnorm(10000, mu, sigma)\n\n# Draw samples\nsample_size &lt;- c(2, 10, 30, 100) # size of each sample\nnum_samples &lt;- 1000 # number of samples (=repeats) for each sample size\n\nsample_mean_df &lt;- tibble(N=rep(sample_size, each = num_samples),\n                         id=rep(1:num_samples, times = length(sample_size))) |&gt;\n  mutate(sample_mean=0)\n\nfor(i in 1:nrow(sample_mean_df)) {\n  sample_i &lt;- sample(sim_obs, sample_mean_df$N[i])\n  sample_mean_df$sample_mean[i] &lt;- mean(sample_i)\n}\n\n\nsample_mean_df |&gt;\n  mutate(N=factor(N, levels=unique(N), labels=paste(\"n:\", unique(N)))) |&gt;\n  ggplot(aes(sample_mean, colour=N)) +\n  geom_vline(xintercept=mu, linetype=3) +\n  geom_density(linewidth=0.9) +\n  scale_colour_viridis_d(\"Size of each sample\", option=\"mako\", end=0.85) +\n  labs(x=paste(\"Means of\", num_samples, \"simulated samples\")) +\n  theme_classic()\n\n\n\nFigure 3.7: Simulated sample means.\n\n\n\nQ100. Calculate the theoretical standard error of the mean for each sample_size given sigma (i.e., from the equation) and compare this with the standard error from the simulated sample means in sample_mean_df.\nHint:\n\nsample_mean_df |&gt;\n  group_by(N) |&gt;\n  summarise() # what goes here?\n\nAnd here’s a tidy solution:\n\nsample_mean_df |&gt;\n  group_by(N) |&gt;\n  summarise(se_sim=sd(sample_mean)) |&gt;\n  ungroup() |&gt;\n  mutate(se_calc=sigma/sqrt(N))\n\n# A tibble: 4 × 3\n      N se_sim se_calc\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     2  1.44    1.41 \n2    10  0.613   0.632\n3    30  0.371   0.365\n4   100  0.198   0.2  \n\n\nQ101. Given what you know about the CLT, how does this change with non-normally distributed data? Try changing rnorm() to rpois()\nQ102. Change sigma and sample_size and check that the standard error estimates are in line with their true values (i.e. as determined by using the standard error formula)."
  },
  {
    "objectID": "P3_normal.html#normal-approximations-for-other-distributions",
    "href": "P3_normal.html#normal-approximations-for-other-distributions",
    "title": "3  Normal distribution",
    "section": "\n3.9 Normal approximations for other distributions",
    "text": "3.9 Normal approximations for other distributions\nIn earlier exercises you saw that the Poisson distribution could be used to approximate the binomial distribution. In a conceptually similar way, the normal distribution can be used to approximate the Poisson model.\n\n3.9.1 The Normal approximation of the Poisson distribution\nRecall that the Poisson model takes only one parameter, the mean, and that where a variable is Poisson distributed the variance equals the mean. So if we have a variable \\(y \\sim Pois(10)\\), the mean (say density per quadrat) is 10 and so is the variance. We now have the two parameters that define the normal distribution (the mean and variance).\nQ103. For \\(y \\sim Pois(10)\\) write the equivalent normal distribution in the usual notation (\\(y /sim Norm(...)\\)).\nAs a rough guideline, a normal approximation is reasonable if \\(\\lambda \\geq 30\\). So \\(y \\sim Pois(10)\\) should not be approximated by the normal distribution while \\(y \\sim Pois(30)\\) could be. However, it is generally preferable to use the natural distribution for your data (e.g., a Poisson distribution for counts). The normal distribution is beneficial in some cases, but this is increasingly less so with modern methods.\nQ104. For a Poisson distribution \\(y \\sim Pois(40)\\) define the normal approximation in the usual way (\\(y /sim Norm(...)\\)).\nQ105. Using R and referring to Chapter 2, calculate \\(P(y_i \\leq 35)\\) from \\(y \\sim Pois(40)\\) (Ans = 0.242)\nQ106. Using R, calculate \\(P(y_i &lt; 35)\\) from the above normal approximation of the Poisson distribution (Ans = 0.215).\nQ107. What do you think of the normal approximation?\nQ108. Evaluate \\(P(y_i&lt;3\\ |\\ \\lambda=5)\\) using the Poisson model and its normal approximation.\n\n3.9.2 The Normal approximation of the binomial distribution\nWhilst we accept the fundamental difference between continuous and discrete data, in making continuous-variable measurements we inevitably allocate each measurement to a single measurement category (we are limited by our ability to measure, eg. to the nearest millimeter if using a ruler). If we wish to treat a continuous variable as a continuous variable, the number of categories between the smallest and largest measurement should be a minimum of ~30 (the upper limit doesn’t matter so much but there is no need to &gt;300). For example, suppose we were measuring fish-lengths and the min was 20 cm and the max 40 cm. That means there are 20 x 1 cm measurement units between them. This falls under our 30 unit minimum – measuring to the nearest cm would not be adequate. There are, however, 40 x 0.5 cm units between them so we could measure to the nearest 0.5 cm (we would probably use 0.1 cm though). But, even if we measure to the correct degree of precision, we are still categorizing the lengths. For example, a fish of 20.5 cm actually means we think that fish was between 20.45 and 20.55 cm.\nWhat is the point of this? Well, if you have a binomially distributed variable where \\(n\\), the number of trials, is sufficiently large (meaning the number of outcomes is large too) then the binomial distribution begins to look like the normal distribution, particularly where \\(p\\) is somewhere near 0.5. The general guideline is that where the number of trials multiplied by the smaller of the probability of success (\\(p\\)) or the probability of failure (\\(q=1-p\\)) is greater than 5 then we can reasonably use the normal distribution to model the data (i.e. both \\(n*p &gt; 5\\) and \\(n*(1-p) &gt; 5\\)).\nTurtles lay clutches of 40 eggs. We might be interested in predicting the number of male and female offspring. Assume the proportion that are male is 0.5. Our investigation stems from the observation that several clutches of eggs, on a particular island, contained only 10 males. We might wonder how unlikely this was by chance, assuming \\(P(male)=0.5\\).\nQ109. Referring to Chapter 2 if necessary, use the binomial model, calculate how unusual (i.e. the probability) it is to get \\(\\leq 10\\) males.\nQ110. Plot your probability (i.e. from 0 – 40 males) as a bar graph. Comment on its shape.\nThe variance of a binomially distributed variable is simply \\(n*p*q\\).\nQ111. Calculate the mean and variance of this population.\nQ112. Are \\(n*p\\) and \\(n*(1-p)\\) both &gt;5?\nQ113. Given these two variables, specify the equivalent normal distribution.\nWe have the ‘real’ distribution \\(y \\sim Binom(40,0.5)\\) and its normal approximation. We ask what is \\(P(y_i \\leq 10)\\) given \\(y \\sim Binom(40, 0.5)\\) and given \\(y \\sim Norm(20, \\sqrt{10})\\).\nQ114. Calculate the Z score for 10 males then find the probability of making this observation.\nQ115. How do the values from the different models compare?"
  },
  {
    "objectID": "P3_normal.html#conclusions",
    "href": "P3_normal.html#conclusions",
    "title": "3  Normal distribution",
    "section": "\n3.10 Conclusions",
    "text": "3.10 Conclusions\nThe normal distribution is central to statistics. A huge variety of observations are reasonably approximated by the normal distribution (or can be made to be normally distributed through transformation).\nThe normal distribution can be used to assess the likelihood of a given observation, if we know the population mean and standard deviation from which it came. Furthermore, given our knowledge of the population parameter (mean and variance) we can determine values which define intervals on that population. Frequently scientists determine the values that bound 95% of their data.\nThe central limit theorem says that sample means taken from a normally distributed population will themselves be normally distributed and, in addition, means of sufficiently large samples will also be normally distributed even where the original data are not normally distributed (the sample size required depends on the extent of the skew in the original data).\nThe normal distribution is a good approximation of the Poisson distribution when lambda is large and the binomial model where the number of trials is large and the probability of success around 0.5 (i.e. the distribution of values is not too skewed)."
  },
  {
    "objectID": "P4_t_CIs.html#single-sample-t-tests",
    "href": "P4_t_CIs.html#single-sample-t-tests",
    "title": "4  T distribution and confidence intervals",
    "section": "\n4.1 Single sample t-tests",
    "text": "4.1 Single sample t-tests\nThis is analogous to the calculation of Z scores and enables us to determine how unlikely our sample mean is, given any hypothesized mean. However, before using any parametric tests such as t-tests, we need to assure ourselves that the model assumptions are reasonably met. Note that t-tests, as illustrated here, are not testing null-hypotheses.\nImagine we are fisheries inspectors and have sampled the cod landed by a fishing boat called the ‘The Evening Star’. We know that the mean size of the cod landed should be greater than 36.6 cm. We need to assess how likely it is that our sampled cod come from a population (of landed fish) where the mean actually is \\(\\geq\\) 36.6 cm. We are testing the hypothesis that there is one ‘population’ of legally landed cod, and the Evening Star’s cod are a part of that population. We use the t-distribution to assess the probability the Evening Star cod are drawn from the legally-landed cod population. If this probability is low then we might speculate that the cod are, in fact, drawn from a different population (i.e., that the boat is using illegal gear).\nWe do not know the population mean or standard deviation of the population from which the cod were caught and hence cannot use a normal distribution to model the likelihood of observing any particular value.\nBefore starting problems like this always state your hypotheses. You should (unless instructed otherwise) state both the null and alternative hypothesis. Here we are wondering whether the mean cod size on the Evening Star is \\(&lt;\\) 36.6 cm. The hypothesis should be worded thus:\nH0 (the null hypothesis): The true value of the mean of the Evening Star cod is greater than or equal to 36.6 cm (\\(\\mu \\geq\\) 36.6 cm).\nH1 (the alternative hypothesis): The true value of the mean Evening Star cod is less than 36.6 cm (\\(\\mu &lt;\\) 36.6 cm).\nWe use the t-test to determine the probability of drawing the Evening Star sample from a population where the true mean is 36.6 cm or greater.\nQ118. Given the hypothesis, is this one or two tailed test?\nWe collect a sample of 20 fish (found in the worksheet ‘Cod lengths’). The sample size is \\(&lt;\\) 30 so we can’t assume that the means will be normally distributed under the CLT. We can check the normality assumption by plotting the data using a ‘normality’ plot or ‘QQ-plot’ (Figure 4.2).\n\ncod_df &lt;- read_excel(\"data/practical_3_4.xlsx\", sheet = \"Cod lengths\")\nstr(cod_df)\n\ntibble [20 × 1] (S3: tbl_df/tbl/data.frame)\n $ CodLength_cm: num [1:20] 34.1 35.6 36.1 34.6 38.3 35 36.8 38 34.4 35.5 ...\n\n\n\nqqnorm(cod_df$CodLength_cm, main = NULL)\nqqline(cod_df$CodLength_cm)\n\n\n\nFigure 4.2: QQ-plot for the sample of code from the Evening Star.\n\n\n\nQ119. Do you think the normality assumption reasonable?\nQ120. What parameter are we actually trying to understand/model? How does the distribution of this parameter change with sample size (think CLT)?\nNow we wish to assess how likely our sample is to have been drawn from a population where the mean actually is 36.6 cm. If the ES mean is less than the ‘legal’ mean and it is ‘unlikely’ to have been drawn from the legal population we might wonder if the mean of true Evening Star landed cod population is &lt;36.6 cm and decide to prosecute the skipper.\nQ121. Use R to calculate summary statistics (mean and standard deviation/standard error of the mean) for the cod sample data (as per results below).\n\npaste(\"Mean:\", mean(cod_df$CodLength_cm))\npaste(\"SD:\", sd(cod_df$CodLength_cm))\npaste(\"Var:\", var(cod_df$CodLength_cm))\npaste(\"N:\", length(cod_df$CodLength_cm))\n\n[1] \"Mean: 35.785\"\n[1] \"SD: 1.54996604377067\"\n[1] \"Var: 2.4023947368421\"\n[1] \"N: 20\"\n\n\nQ122. Determine the standard error of the mean (see Chapter @ref(appendix) if necessary)\n\nsd(cod_df$CodLength_cm) / sqrt(length(cod_df$CodLength_cm))\n\n[1] 0.3465829\n\n\nQ123. Now manually calculate the t statistic for this sample and determine the probability of observing your data assuming that the mean of the population was actually 36.6 cm.\n\nT_stat &lt;- (mean(cod_df$CodLength_cm) - 36.6) / \n  (sd(cod_df$CodLength_cm) / sqrt(length(cod_df$CodLength_cm)))\n\nQ124. How does this value compare to the expectation under the null hypothesis? Use pt().\nQ125. Check you answer against that given by R.\n\nt.test(cod_df$CodLength_cm, mu = 36.6, alternative = \"less\",\n       conf.level = 0.95, var.equal = TRUE)\n\n\n    One Sample t-test\n\ndata:  cod_df$CodLength_cm\nt = -2.3515, df = 19, p-value = 0.01482\nalternative hypothesis: true mean is less than 36.6\n95 percent confidence interval:\n     -Inf 36.38429\nsample estimates:\nmean of x \n   35.785 \n\n\nHopefully your manually calculated t statistic and the one generated by R match. The p-value given by R is exact i.e. there is a probability of 0.014819 that a sample of 20 cod with mean of 35.785 cm would be drawn from a legally landed cod population where the true mean was 36.6 cm or more (assuming model assumptions are met).\nQ126. Can we now confidently send the skipper to jail?\nRemember the confidence interval relates to future (often hypothetical) observations, not an observation that has been made. Confidence intervals are notoriously difficult to define and and are often incorrectly used. BrightSpace has numerous resources to help you.\nEvaluating evidence is a central part of statistical analysis/modelling. In this example, assume you are evaluating whether a fishers catch is ‘surprisingly’ small (e.g. that the fisher is using an illegal net). If you don’t believe the fisher (i.e that the fish sample was not drawn from a population of fish with a mean of 36.6 cm) then she goes to jail. If you do believe her, but she was fishing illegally, she avoids jail. In these circumstances you should set-out your P-value thresholds ahead of getting the data. For the moment, assume we set the P-value (\\(\\alpha\\) value) threshold at 0.05. In this scenario, where we have two clear competing hypotheses, we are in the realm of ‘Neyman-Pearson’s’ decision theory (not in Fisher’s hypothesis significance testing approach; see P-value lecture).\nQ127. Given the P value, do you reject the null hypothesis?\nQ128. If you had set alpha at 0.01 would you reject the null hypothesis?\nQ129. If you set alpha at 0.01, rather than 0.05, what type of error are you reducing and what type of error are you increasing?\nNow to play around with some random data that you generate yourself. We generate 100 random numbers drawn from X, where \\(X \\sim Norm(100, 10)\\).\nQ130. What is your standard deviation in this model?\nQ131. What does the symbol ‘~’ mean?\nWe calculate summary statistics for this randomly generated dataset. Note that in this case, we know the population parameters.\n\nnum_iter &lt;- 3\nsample_size &lt;- 3\nmu &lt;- 100\nsigma &lt;- 10\n\npaste0(\"True mean: \", mu, \", true sd: \", sigma)\nfor (i in 1:num_iter) {\n  sample_i &lt;- rnorm(n = sample_size, mean = mu, sd = sigma)\n  sample_mean &lt;- signif(mean(sample_i), 4)\n  sample_sd &lt;- signif(sd(sample_i), 4)\n  print(paste0(\"Sample \", i, \" mean:\", sample_mean, \", sd: \", sample_sd))\n}\n\n[1] \"True mean: 100, true sd: 10\"\n[1] \"Sample 1 mean:110.9, sd: 18.29\"\n[1] \"Sample 2 mean:113.9, sd: 14.99\"\n[1] \"Sample 3 mean:101.2, sd: 9.521\"\n\n\nNote: these are random samples, so the values will be different each time you run the code. However, R uses pseudo-random number generation. Use set.seed() for fully reproducible code.\nQ132. Is there a discrepancy between the population parameters you defined and the actual mean and variance that are estimated from the samples?\nYour answer to the above should be yes. You know there is a discrepancy because you know the true parameters. In most real life situations you do not know the true population mean and variance. You can only sample them. If your sample is very large (and representative) then you can generate a very good estimate of those population parameters. However, as your sample size is reduced, the reliability of your estimate decreases. Look at the random numbers you’ve generated. Get a sense for where most of the numbers lie with sigma=10. To output your sample, just run sample_i.\nThe t-distribution is the distribution of values you get when you subtract sample means from the true mean and standardize by the sample standard error (i.e., \\(\\frac{\\bar{y} - \\mu}{SE_{\\bar{y}}}\\)). Think about this and relate it to the formula for determining single-sample T-statistics and what the critical values actually are.\nThe code below simulates repeated samples from a population with \\(y \\sim Norm(\\mu, \\sigma)\\). Each sample takes sample_size individuals, with num_samples unique samples. For each sample i, the t-statistic is calculated and stored in T_sample[i]. Figure 4.3 shows the distribution of T_sample (the histogram) with the corresponding t-distribution (df=sample_size - 1) as the solid line and a standard normal distribution as the dotted line. Play with the values for sample_size below to see how the shapes change.\n\nmu &lt;- 10 # population mean\nsigma &lt;- 0.5 # population sd\nnum_samples &lt;- 1e5 # number of samples\nsample_size &lt;- 3 # size of each sample\nT_sample &lt;- numeric(num_samples) # sample t statistics\n\n# for each repeat: draw a sample, calculate SE and T, and store T in T_sample\nfor(i in 1:num_samples) {\n  sample_i &lt;- rnorm(sample_size, mu, sigma)\n  sample_SEM &lt;- sd(sample_i) / sqrt(sample_size)\n  T_sample[i] &lt;- (mean(sample_i) - mu) / (sample_SEM)\n}\n\ncurve(dnorm(x, 0, 1), from = -6, to = 6, lty = 2, \n      xlab = \"Simulated t-statistics\", ylab = \"Density\",\n      main = paste(\"t-statistics of\", \n                   format(num_samples, big.mark=\",\", scientific=F), \n                   \"samples, each with N =\", sample_size))\nhist(T_sample, freq = F, add = T, col = rgb(1, 0, 0, 0.25), \n     breaks = seq(floor(min(T_sample)), ceiling(max(T_sample)), by=0.2))\ncurve(dnorm(x, 0, 1), from = -6, to = 6, add = T, lty = 2)\ncurve(dt(x, sample_size - 1), from = -6, to = 6, add = T)\nlegend(\"topright\", lty = c(2, 1, 1), col = c(1, 1, 2), bty = \"n\",\n       c(\"Normal(0,1)\", paste0(\"t(df=\", sample_size-1, \")\"), \"t-stat (sim)\"))\n\n\n\nFigure 4.3: Histogram of 100,000 t-statistics calculated from 100,000 samples, along with the corresponding theoretical t-distribution (solid line) and a standard normal (dotted line).\n\n\n\nNotice how the histogram and the solid lines are nearly identical? These simulations illustrate that the t-distribution is the distribution of t-statistics for a given sample size."
  },
  {
    "objectID": "P4_t_CIs.html#confidence-intervals",
    "href": "P4_t_CIs.html#confidence-intervals",
    "title": "4  T distribution and confidence intervals",
    "section": "\n4.2 Confidence Intervals",
    "text": "4.2 Confidence Intervals\nSay you are interested in knowing the mean of a population (e.g. barnacle mass on the back beach). You cannot afford to determine the mass of each barnacle, so you take a random sample. You don’t know how ‘good’ (i.e. representative) your sample is. It might have included lots of small barnacles, or big ones, or a wide- or narrow-range of sizes, you can never know (unless you sample everything). When you calculate the mean of this sample you don’t know how close it is to the population mean, but you do know the probability associated with that estimate. Confidence intervals capture this uncertainty, and you use the t-distribution to determine them.\nWe’ll invent a population of barnacle diameters, called barnacle_diam, and then create a histogram of that population and superimpose values on that. Again, these are random numbers so your values will be slightly different from mine.\n\npop_size &lt;- 100000  # number of barnacles in the population\nbarnacle_mu &lt;- 200\nbarnacle_sigma &lt;- 25\nbarnacle_diam &lt;- rnorm(pop_size, mean = barnacle_mu, sd = barnacle_sigma)\nmu &lt;- mean(barnacle_diam)\nsigma &lt;- sd(barnacle_diam)\nhist(barnacle_diam, main = NULL)\nQ95 &lt;- quantile(barnacle_diam, c(0.025, 0.975))\nabline(v = Q95, col = \"green\", lwd = 3)\ntext(x=Q95[2], y=pop_size/7, \n     labels=paste0(\"mu: \", round(mu, 1), \"\\nsigma:\", round(sigma, 1)))\n\n\n\nFigure 4.4: Histogram of a simulated barnacle population with 2.5% and 97.5% quantiles.\n\n\n\nNow we can take samples from that population: this is the reality, you take samples (usually) from populations where you don’t know the true mean and standard deviation. Let’s take 4 samples, each with size sample_size.\n\nhist(barnacle_diam, main = NULL)\nsample_size &lt;- 5\nnum_samples &lt;- 4\nabline(v = mu, col = \"blue\", lwd = 4)\n\nfor (i in 1:num_samples) {\n  sample_i &lt;- sample(barnacle_diam, size = sample_size)\n  print(sample_i)\n  abline(v = mean(sample_i), col = \"red\", lwd = 0.5)\n}\n\n[1] 207.9610 175.2847 174.7678 226.6444 207.9468\n[1] 192.7077 156.7925 246.4876 192.1450 215.1096\n[1] 166.8083 187.8788 275.8214 229.5465 199.2081\n[1] 204.8681 238.4593 191.5637 193.9610 212.4221\n\n\n\n\nFigure 4.5: Histogram of the barnacle population showing location of 4 sample means (red lines), each with N = 5. The blue line shows the true population mean mu.\n\n\n\nOur sample means inevitably differ from the true population mean (\\(\\mu\\)), even if only a bit. Likewise, the sample standard deviations will differ from the true population standard deviation (\\(\\sigma\\)). If you keep repeating this sampling you can generate a distribution of sample standard deviations. This distribution is not normal, but is instead related to the chi-square distribution (don’t worry too much about this). The point is that if your sample size is small, your estimate of the standard deviation is often very poor.\n\npar(mfrow=c(2,2))\n# sim_df will hold the sample sizes N, and the median and mean sample sd's \nnum_samples &lt;- 1e4\nsim_df &lt;- data.frame(N=c(2, 4, 10, 30),\n                     sd_median=NA, sd_mean=NA,\n                     sd_q025=NA, sd_q25=NA, sd_q75=NA, sd_q975=NA,\n                     mn_median=NA, mn_mean=NA,\n                     mn_q025=NA, mn_q25=NA, mn_q75=NA, mn_q975=NA) \n\n# for each sample size N, draw a sample and store its mean and sd\n# repeat this num_samples times\n# plot a histogram of the sample sd's, then store the mean and median\nfor (i in 1:nrow(sim_df)) {\n  samp_sd_i &lt;- numeric(num_samples) \n  samp_mn_i &lt;- numeric(num_samples) \n  for (j in 1:num_samples) { \n    sample_ij &lt;- sample(barnacle_diam, size = sim_df$N[i])\n    samp_sd_i[j] &lt;- sd(sample_ij)\n    samp_mn_i[j] &lt;- mean(sample_ij)\n  }\n  hist(samp_sd_i, main = paste(num_samples, \"sample SDs for N =\", sim_df$N[i]), \n       breaks = 20, xlim = c(0, 100))\n  abline(v = sigma, col = \"blue\", lwd = 2)\n  sim_df[i, 2:13] &lt;- c(median(samp_sd_i), mean(samp_sd_i),\n                       quantile(samp_sd_i, probs = c(0.025, 0.25, 0.75, 0.975)),\n                       median(samp_mn_i), mean(samp_mn_i),\n                       quantile(samp_mn_i, probs = c(0.025, 0.25, 0.75, 0.975)))\n}\n\n\n\nFigure 4.6: Histograms of sample standard deviations from repeated samples of the same population. The true population standard deviation is shown in blue.\n\n\n\n\npar(mfrow=c(1,2))\nplot(sim_df$N, sim_df$sd_median,\n  xlim = c(0, 30), ylim = range(c(sim_df[,2:7], sigma)),\n  type = \"b\", xlab = \"Sample size\", ylab = \"Standard deviation\"\n)\nsegments(sim_df$N, sim_df$sd_q25, sim_df$N, sim_df$sd_q75, lwd=2)\nsegments(sim_df$N, sim_df$sd_q025, sim_df$N, sim_df$sd_q975)\nlines(sim_df$N, sim_df$sd_mean, type = \"b\", col = \"dodgerblue\")\nabline(h = sigma, lty = 2)\nlegend(\"topright\", c(\"Mean sample SD\", \"Median sample SD\", \"True SD\"),\n  col = c(\"black\", \"dodgerblue\", \"black\"),\n  lty = c(1, 1, 2), pch = c(1, 1, NA), bty = \"n\"\n)\nplot(sim_df$N, sim_df$mn_median,\n  xlim = c(0, 30), ylim = range(c(sim_df[,8:13], mu)),\n  type = \"b\", xlab = \"Sample size\", ylab = \"Mean\"\n)\nsegments(sim_df$N, sim_df$mn_q25, sim_df$N, sim_df$mn_q75, lwd=2)\nsegments(sim_df$N, sim_df$mn_q025, sim_df$N, sim_df$mn_q975)\nlines(sim_df$N, sim_df$mn_mean, type = \"b\", col = \"dodgerblue\")\nabline(h = mu, lty = 2)\nlegend(\"topright\", c(\"Mean sample mean\", \"Median sample mean\", \"True mean\"),\n  col = c(\"black\", \"dodgerblue\", \"black\"),\n  lty = c(1, 1, 2), pch = c(1, 1, NA), bty = \"n\"\n)\n\n\n\nFigure 4.7: Mean (black), median (blue), and 50% and 95% quantiles (vertical lines) for (left) sample standard deviations at each sample size compared to the true population standard deviation (dotted line) or for the (right) sample means.\n\n\n\nQ133. What is the most common standard deviation for your samples by sample size?\nThe t-distribution allows for the fact that the standard deviation of small samples is, usually, less than that of the population as seen in Figure 4.6 .\nThe take home message here is that when you take a sample from a population with unknown \\(\\mu\\) and \\(\\sigma\\), you won’t know how ‘accurate’ you sample is but you do know how your random samples ‘behave’ - they are modelled using the t-distribution. From this knowledge you can build a 95% confidence interval which is described as an interval which, if repeated for 100 samples, would include \\(\\mu\\) within its boundaries in 95 of those samples (on average). Read that again. You don’t have knowledge of the true value of the mean or sd (as you did for Z score calculations) and the t-distribution accounts for this uncertainty.\n\nnum_samples &lt;- 5\nsample_size &lt;- 3\n\n# plot population\nhist(barnacle_diam, xlim = c(barnacle_mu-6*barnacle_sigma, barnacle_mu+6*barnacle_sigma), \n     main = NULL, ylim = c(0, length(barnacle_diam)/6),\n     col = \"grey90\", border = \"grey50\", xlab = \"Barnacle diameter\")\nabline(v = mu, col = \"blue\", lwd = 2)\ny_pos &lt;- seq(0, length(barnacle_diam)/6, length.out=num_samples)\n\n# draw samples, calculate mean and 95% CIs, and plot them\nfor (i in 1:num_samples) {\n  sample_i &lt;- sample(barnacle_diam, size = sample_size)\n  points(x = mean(sample_i), y = y_pos[i], col = \"red\", pch = 16, cex = 0.75)\n  sample_ci &lt;- c(\n    mean(sample_i) + qt(0.025, (sample_size - 1)) * (sd(sample_i) / sample_size^0.5),\n    mean(sample_i) + qt(0.975, (sample_size - 1)) * (sd(sample_i) / sample_size^0.5)\n  )\n  arrows(sample_ci[1], y_pos[i], sample_ci[2], y_pos[i], \n         col = \"red\", code = 3, angle = 90, length=0.05)\n}\n\n\n\nFigure 4.8: Histogram illustrating the barnacle population with population mean (blue) and sample means with 95% CIs (red) repeated across 5 samples.\n\n\n\nQ134. Try different values for sample_size. How does this influence the width of your CIs?\nQ135. What proportion of your 95% CIs would you expect to include the true value of the mean? Does the sample size have an impact on this?\nQ136. Keep repeating the above code until you get an example where your 95% CI misses the true value of the mean.\nQ137. See if you can find the relevant bit of the code, and determine 99% and 90% or 75% CIs (pick any value you fancy, but note that some values are likely to put the CIs some distance from the mean so get ready to adjust your axis limits which are set to 6 \\(\\sigma\\) on either side of \\(\\mu\\)).\nLet’s explore the influence of sample size on the width of the confidence interval a little more.\nWe will plot the resultant mean estimate and 95% CI on Figure 4.9 . NOTE: the example shown is a random example, not linked to the output below. Repeat this process, but this time determine the mean and sd (or variance) for samples of 2, 5 and 10. Do this simply by changing n=… in rnorm() Repeat this 5 times for each sample size and sketch your results (where they fit) onto Fig. Figure 4.9 .\n\na &lt;- rnorm(n = 10, mean = 100, sd = 10)\nsignif(a, 3)\n\n [1]  98.6  95.3 107.0  99.3  88.3 111.0  99.5 100.0 105.0 107.0\n\nt.test(a, mu = 100)\n\n\n    One Sample t-test\n\ndata:  a\nt = 0.5333, df = 9, p-value = 0.6067\nalternative hypothesis: true mean is not equal to 100\n95 percent confidence interval:\n  96.40016 105.82072\nsample estimates:\nmean of x \n 101.1104 \n\n\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_errorbarh()`).\n\n\n\n\nFigure 4.9: Confidence interval size vs. sample size.\n\n\n\nQ138. Look at your 95% confidence intervals – are they getting broader or narrower as your sample size decreases?\nQ139. What proportion of 95% CIs would you expect to include the true value of the mean?"
  },
  {
    "objectID": "P4_t_CIs.html#comparing-means-two-sample-t-tests",
    "href": "P4_t_CIs.html#comparing-means-two-sample-t-tests",
    "title": "4  T distribution and confidence intervals",
    "section": "\n4.3 Comparing means (two-sample t tests)",
    "text": "4.3 Comparing means (two-sample t tests)\nThe two-sample t-test is one of the most widely used inferential statistical tests. The two-sample t-test is a special case of analysis of variance (ANOVA) where you are only comparing two means. The results are identical and so we do not focus on two-sample t-tests. You need to know of the existence of the two-sample t-test because it is so commonly used and cited, but you will be comparing means using ANOVA in Chapter 5 ."
  },
  {
    "objectID": "P4_t_CIs.html#non-parametric-tests",
    "href": "P4_t_CIs.html#non-parametric-tests",
    "title": "4  T distribution and confidence intervals",
    "section": "\n4.4 Non-parametric Tests",
    "text": "4.4 Non-parametric Tests\nNon-parametric tests are often used to compare samples where the data are non-continuous or fail the assumptions of parametric general linear models, typically converting data to ranks rather than using the actual values. Non-parametric test include ‘classics’ such as the ‘Mood’ and ‘Wilcoxon’ tests. However, we make you aware of the GLM family which will usually supply you with a much more elegant solution to model data that doesn’t fit the simple linear model. You should be aware of the existence of ‘non-parametric’ tests because they are prevalent in the literature. Remind yourself of the disadvantages of non-parametric tests."
  },
  {
    "objectID": "P4_t_CIs.html#conclusions",
    "href": "P4_t_CIs.html#conclusions",
    "title": "4  T distribution and confidence intervals",
    "section": "\n4.5 Conclusions",
    "text": "4.5 Conclusions\nThe t-test is a ‘classic’ statistical test which doesn’t assume knowledge of population parameters. The strength of the t-test (its ability to quantify differences between samples) is proportional to the sample size. The larger the sample size, the better the estimate of the population parameters and the more precisely we are to be able to detect differences between the means.\nThe central limit theorem tells us that the means of non-normally distributed data will be normally distributed if the sample size is sufficiently large. If your sample size is \\(&gt;\\) 30 it is likely that the means of that sample will be normally distributed regardless of the distribution of the original data.\nParametric tests including the t-test are quite ‘robust’ against deviations from normality, particularly as sample sizes increase. However, parametric test are less robust against heteroscedasticity, regardless of sample size. Always check this assumption and be prepared to transform the data if the assumption of homoscedasticity is not tenable (more of this in Chapter 5).\nThe t-test is in the ‘general linear model’ family (and this is a subset of the generalized linear modelling family). General linear models are usually used to model continuous data. If you have count data, you should start with a different member of the GLM family (you might not be able to transform count data to something approximating a normal distribution). Non-parametric tests are frequently adopted when data do not conform to the assumptions of normality but they are invariably used for NHST with all the inherent problems with that approach.\nA final reminder with regard to many statistical tests, including all in the GLM family: they make the assumption that data are independent. You must always ensure that your experimental design lends itself to making independent observations in relation to the question you are asking. This is the most critical and fundamental of the assumptions of parametric and non-parametric tests. Non-independence (e.g. measuring the same ‘subject’ (e.g. an urchin) over time) can be modelled using more complex ‘mixed’ models. Application of mixed modelling is beyond this course but you should be aware of the limitations of the techniques that you are learning and know where to go next."
  },
  {
    "objectID": "P5_ANOVA_regression.html#analysis-of-variance-anova",
    "href": "P5_ANOVA_regression.html#analysis-of-variance-anova",
    "title": "5  ANOVA and regression",
    "section": "\n5.1 Analysis of variance (ANOVA)",
    "text": "5.1 Analysis of variance (ANOVA)\nANOVA is a widely used modelling approach that enables you to compare means and put confidence intervals on the differences between those means. For a predictor with only two categories, ANOVA is identical to the 2-sample t-test, so we’ll just use ANOVA.\nWhen you see “analysis of variance”, think “analysis of means”. In an ANOVA, we analyse the variance in the data in order to compare the means of different groups. In an ANOVA, we compare means by determining the ratio of “the variance between treatments and the overall mean” (large black arrows in Figure 5.1) and “the variance within treatments” (sum of the dotted arrows in Figure 5.1). The black line under the red dots in Figure 5.1 shows the actual data distribution and the actual parameters for mean values (50 and 150 cm for A and B respectively). You then take samples from A and B (n=4 in this example) and, from these, derive your parameter estimates for the mean of each group and the overall mean.\nIn Figure 5.1 below you can see that the solid black arrows are much larger than the dotted ones leading you to think that the chance that these two samples are drawn from the same population (with a value of the overall mean) as very unlikely. Make sure you understand Figure 5.1 (more detail in the ANOVA lecture).\nIn most circumstances you know that the means that you are comparing with ANOVA are different (i.e. that testing a null hypothesis of no difference isn’t useful). ANOVA allows you to put confidence intervals around differences between means, or groups of means.\n\n\nFigure 5.1: Sources of variation (within group and between groups) as quantified by ANOVA.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n\n5.1.1 ANOVA in R\nThere are numerous variations on the theme of ANOVA. We cover one-way ANOVA and we mention two-way ANOVA with and without replication. The objective of ANOVA is to establish the size of the difference (called the ‘effect size’) between different groups (e.g., treatments or locations) and put a confidence interval on those differences.\n\n5.1.2 One-way ANOVA\nOne-way ANOVA is a procedure we use to estimate the magnitude of differences between means of \\(\\geq\\) 2 groups. We also use it to put confidence intervals on those differences.\nThe first example data is the yield in \\(\\mu g\\) C \\(ml^{-1}\\) of a species of microalgae (Isochrysis galbana) in laboratory culture exposed to three light levels (low, medium and high). We are interested in these particular light levels because they represent the means of winter, spring, and summer Scottish sun intensity. The data are in worksheet ‘Microalgae’.\nNote: the function to conduct an ANOVA is aov(). The function anova() converts various statistical model outputs to the standard ANOVA-table output (including any from the GLM family).\n\nalgae_wide_df &lt;- read_excel(\"data/practical_5.xlsx\", sheet = \"Microalgae\")\n# check these data as usual\n\nQ158. What is your objective in this type of experiment? What are you interested in estimating?\nQ159. What assumptions should be met prior to undertaking parametric ANOVA?\nQ160. Under which circumstances could you begin to relax the assumption that the data are normally distributed (think central limit theorem)?\nQ161. Given your sample size can we assume normality of means?\nQ162. Are the data normally distributed (be careful how you word your answer to this question, see the following question)?\nQ163. Is it reasonable to assume that these data are drawn from a population that is normally distributed?\nTo work with our data, we need to rearrange the data so that the data is in a tidy format with each variable corresponding to one column, and each observation corresponding to one row. We’ll use the tidyverse as before.\n\nhead(algae_wide_df, 2)\n\n# A tibble: 2 × 3\n    low medium  high\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  13.1   12    14.2\n2  11.5   11.5  13.1\n\nalgae_df &lt;- algae_wide_df |&gt;\n  pivot_longer(everything(), names_to = \"Treatment\", values_to = \"Yield\")\nglimpse(algae_df, width=80)\n\nRows: 15\nColumns: 2\n$ Treatment &lt;chr&gt; \"low\", \"medium\", \"high\", \"low\", \"medium\", \"high\", \"low\", \"me…\n$ Yield     &lt;dbl&gt; 13.07599, 12.00000, 14.20000, 11.53923, 11.50000, 13.10000, …\n\n\nWhile there are formal tests to evaluate model assumptions (e.g., the Shapiro-Wilkes test or the Bartlett test), a better way of checking model assumptions is to check residual patterns. A ‘residual’ is the difference between an actual data value and that predicted by the model. Here we have randomly assigned five cultures each of the same species to three specific treatments (light levels).\nQ165. What type of experiment is this? (How many factors (=predictors)? Are they fixed or random?)\nNext to conduct the analysis:\n\nalgae_aov &lt;- aov(Yield ~ Treatment, data = algae_df) # ?aov\n\nBefore we look at the output, let’s assess the assumptions using the residuals. The default residual plots created by R are shown in Figure 5.2 and enables us to rapidly assess whether the model assumptions are reasonable.\n\npar(mfrow = c(2, 2), mar=c(4,4,1,1)) \nplot(algae_aov) \n\n\n\nFigure 5.2: Residual plots from one-way ANOVA.\n\n\n\nInterpretation of residual patterns:\n\n\nUpper left: Residuals v. fitted. This is the residual values against the fitted values. The fitted values are the means of the three groups (remember that ANOVA is about comparing means). The spread for the lower values (low and medium light) is higher than for the high light so this might make us consider the homoscedasticity assumption.\n\n\nUpper right: Normal Q-Q plot. This assesses the normality assumption. The points (each point is an observation) lie around the straight line so this assumption is reasonable. Note that general linear models assume that the means of groups are normally distributed, and this always applies when the means are based on large sample sizes (roughly \\(n &gt; 30\\)). When \\(n &lt; 30\\), you should check that the distribution of the residuals is reasonably ‘normal’.\n\n\nLower left: Scale-location. This specifically looks to assess whether residuals increase with fitted values, which is a common issue in these types of analysis. In this case, the scale decreases with fitted value. This is similar to the Upper Left plot, but with sqrt(abs(standardized_residuals)) on the y-axis instead of just residuals to focus just on the magnitude of the residuals.\n\n\nLower right: Constant leverage, residuals vs. factor levels. This indicates how each treatment is fitted (i.e. the residuals associated with each treatment). You might be concerned if one particular treatment was associated with extremely high residuals (outliers). R automatically identifies potential outliers (8, 11, and 13 in this case) for you to further assess. In this case there is nothing in particular to worry about.\n\nThe residual plots allow you to investigate different aspects of the data and the how their assumptions are met. The interpretation of the plots overlaps in the sense that the same issue might be apparent in several of the plots.\nQ166. What are the ‘fitted values’ for an ANOVA?\nQ167. Are your effects fixed or random?\nQ168. Assuming you have chosen \\(\\alpha = 0.05\\), what might you be interested in going on to test next? Hint: you are testing to see whether the means of three populations are different.\nEverything looks OK, so we can then look at the results of the ANOVA.\n\n# anova(algae_aov) # outputs an anova-type table, but unnecessary with aov()\nsummary(algae_aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nTreatment    2 10.050   5.025   6.487 0.0123 *\nResiduals   12  9.296   0.775                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nReporting that there are ‘significant’ differences between means is not enough. What your readers should be interested in is what the differences between the means actually are, and how confident you are in your assessment. This can be provided by the Tukey test in R.\n\nalgae_grp_diffs &lt;- TukeyHSD(x = algae_aov, conf.level = 0.95) \n# HSD stands for 'honestly significant difference'\nalgae_grp_diffs\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Yield ~ Treatment, data = algae_df)\n\n$Treatment\n                   diff       lwr        upr     p adj\nlow-high    -1.68922461 -3.174304 -0.2041449 0.0260570\nmedium-high -1.78000000 -3.265080 -0.2949203 0.0194474\nmedium-low  -0.09077539 -1.575855  1.3943043 0.9854641\n\n\nYou can see that the mean yield at the high light level is higher than at both the low and medium: the 95% confidence interval of the difference in comparing high and low light levels are 1.69 (0.205, 3.17) \\(\\mu g\\) C \\(ml^{-1}\\). (I’ve inverted the results so the the difference is seen as positive (high - low rather than low - high)). This confidence interval is much more important than any P-values and you should report both (CI because it is useful, P value by convention).\n\n5.1.3 Two-way ANOVA\nYou’ve had a look at one-way ANOVA (i.e., one predictor), which is a good starting point. However, in nature you often find numerous factors combine to influence an outcome. This is called an ‘interaction’. Two-way ANOVA allows you to investigate the nature of this interaction term. You can also get three-way ANOVA and more, but these get logistically challenging because you need to replicate across each level. The interpretation also gets increasingly difficult. You need to be aware of the existence of two-way ANOVA and what it offers, and how to interpret simple graphics (below), but we do not cover implementation of two-way ANOVA.\n\n\nFigure 5.3: Graphic illustrating differing levels of interaction on the response from two treatments A and B.\n\nIn Figure 5.3 we have the outcome of an experiment. Each dot on the plot represents the mean Response (e.g. growth) of a number of replicates, subject to the combination of Temperature (cold and hot) and Nutrient (N and P). We are interested in the main effects (Temp and Nutrient) and their interaction (Temp * Nutrient) In panel A, there is no effect of Temp on the Response (the lines between Cold and Hot are horizontal), but there is a main effect of Nutrient (P is higher than N). In B, there is an effect of Temp (Hot is, on average higher than Cold) but there is also an interaction as the effect of Hot is more for P than for N (where it has no effect in this example). In C, the interaction is stronger compared with B. In D, there are no treatment main effects because mean Cold = mean Hot and mean N = mean P, but there is a very strong interaction effect; the effect of Temp is reversed by Nutrient, so that the level of Nutrient (N or P) determines the effect of Temp. When it is Cold the Response is high for Nutrient P and low for Nutrient N, when it is Hot, the Response is low for Nutrient P and high for Nutrient N (but the average for hot=cold, and average for N=P)"
  },
  {
    "objectID": "P5_ANOVA_regression.html#regression",
    "href": "P5_ANOVA_regression.html#regression",
    "title": "5  ANOVA and regression",
    "section": "\n5.2 Regression",
    "text": "5.2 Regression\nCorrelation and regression are used to examine the strength of association between two variables. In correlation, both variables are measured (and therefore associated with measurement error). In regression, one variable is fixed (by the experimenter) and is assumed to have no ‘error’ associated with it and the other, called the ‘response variable’, is measured (so has measurement error). You must be able to distinguish which of correlation or regression analyses are most appropriate.\nCorrelation analysis is used to measure association, where you are not attempting to formally link cause-and-effect. Regression analysis is generally used where you have experimentally manipulated the fixed factor and are looking at the response in another factor. Causation is implicit in inferential regression analysis (correlation analysis is often used in ‘exploratory’ data analysis where any link between cause-and-effect is inherently more speculative).\nThe media often misreport science because it is difficult to resist the impulse to attribute causation. An overwhelming number of spurious correlations (i.e., those clearly having no causal relationship) are documented on tylervigen.com.\n\n5.2.1 Overview\nRegression is at the heart of linear models. ANOVA and t-tests are, basically, special cases of linear regression models. The regression coefficient is a measure of the strength of the relationship between the dependent variable (the one you measure) and the independent variable (the one you fix like a fixed factor in ANOVA). The regression coefficient is denoted by \\(R^2\\) compared with \\(r\\) in correlation. The regression coefficient \\(R^2\\) ranges from 0 to 1 (unlike \\(r\\) which ranges from -1 to 1). A value \\(R^2 = 0\\) indicates no relationship to the independent variable while \\(R^2=1\\) indicates that the independent variable is entirely responsible for the variability in the measured variable.\nAs usual, null hypothesis significance testing is often applied to regression statistics. As usual, the null hypothesis being tested is usually “there is no functional relationship between the response and the predictor” and this is usually conceptually nonsense. In conducting regression analysis, your objective is to quantify to the most appropriate precision and accuracy possible the relationship between X (the aspect you control, the predictor, plotted on the X axis) and Y (the variable you measure, the response, plotted on the Y axis). Your objective is to quantify this relationship, put confidence intervals on it, and then interpret your findings in relation to the objectives of the study and in relation to other research.\nQ169. What does the plot look like when there is no relationship between the predictor and the response?\nLet us now consider an example in which cause and effect does exist. The data in worksheet ‘Beetles’ in practical_6.xlsx shows the weight loss in Tribolium confusum, the confused flour beetle, at different relative humidities (data from Sokal and Rohlf, 1995). The relative humidity (RH) to which the beetles are exposed can be fixed and the weight loss (via evaporative losses) of the beetles then assessed. There is no way that the null hypothesis can be true in this case: humidity will obviously influence weight loss in beetles.\nQ170. In this case, what is your response variable (what are you measuring) and your predictor (i.e. what is it that you are manipulating to determine the extent of the response)?\nQ171. Plot the data in R and check your prediction. In this case, the predictor must be displayed on the x-axis and the response must be on the y-axis.\nWe are interested in whether the whole data set can be usefully represented by a linear regression relationship. We wish to estimate the relationship, and put a confidence interval on our estimate. Common sense tells us that there is some sort of relationship (testing a null hypothesis is not very useful) but it might go in either direction (positive or negative) and we don’t know the strength (i.e. slope) of that relationship.\n\n5.2.2 Linear regression in R\nIn R we can use a variety of techniques to conduct linear regression. The easiest is to use lm(). It is worth noting that lm() would also work for all your other general linear models (e.g. ANOVA). They are, in fact, the same model, it is just the default output (and necessary input formatting) that differs. Try reproducing the ANOVAs above with anova(lm(...)).\nImport data and begin:\n\nbeetle_df &lt;- read_excel(\"data/practical_6.xlsx\", sheet = \"Beetles\")\n# inspect the dataframe, then make a scatter plot\npar(mfrow=c(1,1))\nplot(WeightLoss_Mg ~ Humidity, data = beetle_df) \n\n\n\nFigure 5.4: Beetle weight loss as a function of relative humidity.\n\n\n\nAn aside on plotting: you can provide plot() with either a vector for the x-axis and a vector for the y-axis (i.e., plot(x_var, y_var)) or you can use a formula, specifying the dataframe (i.e., plot(y ~ x, data=data_df)). Just be aware of which variable is on which axis.\nNow we have explored and plotted the data we can conduct the regression analysis.\n\n# weight loss is modelled as (~) a function of humidity\nbeetle_lm &lt;- lm(WeightLoss_Mg ~ Humidity, data = beetle_df) \n# beetle_lm\n# str(beetle_lm) # lm outputs are complex structures\n\nBefore we go on and interpret the model output we need to assess the model assumptions. This is done in the same way as for ANOVA with the same commands.\n\npar(mfrow = c(2, 2), mar=c(4,4,1,1)) # set up 4 in 1 plot.\nplot(beetle_lm) # plot the regression residuals.\n\n\n\nFigure 5.5: Regression diagnostics\n\n\n\nThe small sample size here (\\(n=9\\)) makes a proper analysis of the residuals difficult. The plot should be assessed in the same way as for the ANOVA residuals. Basically, any pattern is bad. The upper left (Residuals v Fitted) doesn’t cause any major concern, though the upper right (Normal QQ) indicates a possible problem. Scale-Location (lower left) is difficult to interpret but no obvious pattern is present. The Residuals v. leverage (lower right) indicates a potential issue as well. A point with a large residual (i.e. where it is very different to that expected by the model) and with a high leverage (i.e. at the extreme ends of the predictors range) has a large Cook’s distance and has a disproportionate effect on the slope and intercept. These points should be examined in more detail.\nQ172. Which point has the largest Cook’s distance?\nWe will now proceed to looking at the linear regression analysis results on the basis that the residuals do not raise any concerns.\n\nsummary(beetle_lm)\n\n\nCall:\nlm(formula = WeightLoss_Mg ~ Humidity, data = beetle_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46397 -0.03437  0.01675  0.07464  0.45236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.704027   0.191565   45.44 6.54e-10 ***\nHumidity    -0.053222   0.003256  -16.35 7.82e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2967 on 7 degrees of freedom\nMultiple R-squared:  0.9745,    Adjusted R-squared:  0.9708 \nF-statistic: 267.2 on 1 and 7 DF,  p-value: 7.816e-07\n\n\nThe regression equation of the form \\(y = a + bx\\) can be determined. The regression equation is:\n\\(WeightLoss = 8.70 - 0.05322 * humidity\\)\nCommon-sense check: the coefficient is negative. As the humidity increases, the weight loss decreases (as expected and shown in the scatter plot).\nQ173. What is the effect on weight loss of increasing the relative humidity by 10%?\nQ174. What is the weight loss, predicted by the model, when relative humidity is 0%?\nQ175. What does the model suggest the weight loss will be when relative humidity is -50% and +150%? Are these values sensible? What does this tell you about extrapolating beyond the data range in using regression analysis in predictions?\nThe residual error is the variance in y around the line. The \\(R^2\\) is the proportion of this variance that is explained by the regression line. In the current case \\(R^2 = 0.97\\). This is an extremely high value and indicates that the regression model is extraordinarily good at accounting for the variance in weight loss based on the relative humidity.\nThe P values allow us to assess if the slope and the intercept are likely different from zero.\nQ176. Given the very high \\(R^2\\) (and looking at your plot) would you expect the regression model to be significantly better than the null model in explaining the variance in weight loss?\nQ177. With \\(\\alpha = 0.05\\), do you reject or accept the null hypothesis? What would you wish to report in relation to the slope coefficient if you were reporting the results from this analysis?\n\nconfint(beetle_lm)\n\n                  2.5 %      97.5 %\n(Intercept)  8.25104923  9.15700538\nHumidity    -0.06092143 -0.04552287\n\n\nThe confidence intervals are, again, ‘clunky’ to describe.\nIf we imagine there were many alternate you’s (like in a multiverse) repeating the same experiment on the same population with the same sample size (but independent samples), and each ‘you’ calculated 95% CIs with confint(), then 95% of you would have intervals that include the true population intercept and slope. While you do not know if you are in the unlucky 5% that failed to capture the population values, the 95% confidence interval serves as our best estimate for likely values (but see Bayesian statistics for more intuitive intervals!).\n\n5.2.3 Plotting the regression line and confidence intervals\nA regression model (i.e. the linear relationship between the predictor and response variables) allows us to predict values for any value of the predictor, along with confidence levels. We can plot this regression line without too much effort.\n\nbeetle_pred_line &lt;- predict(beetle_lm, interval = \"confidence\", level = 0.95)\n\npar(mfrow=c(1,1))\nplot(WeightLoss_Mg ~ Humidity, data = beetle_df, ylim=c(3, 10),\n     xlab = \"Relative humidity (%)\", ylab = \"Weight loss (mg)\") \nlines(beetle_df$Humidity, beetle_pred_line[, \"fit\"])\nlines(beetle_df$Humidity, beetle_pred_line[, \"lwr\"], lty = 2)\nlines(beetle_df$Humidity, beetle_pred_line[, \"upr\"], lty = 2)\n\n\n\nFigure 5.6: Regression line (solid) with upper and lower 95% confidence intervals on the regression line (dashed).\n\n\n\nTry generating 90% confidence intervals and add them to the plot.\nQ178. Which will have the wider interval, a 99.99% interval or a 50% interval and why?\nQ179. Do the confidence intervals in Figure 5.6 run parallel to the regression line?\nQ180. If not, what does this suggest about the degree of confidence you have in values predicted at various points along the line?\nQ181. At what value of relative humidity are your predictions of weight loss likely most accurate?\nWe can make predictions based on our regression line, and put confidence intervals on those predictions. Say we had a relative humidity of 50% in the above example. You could ask for the model-predicted weight loss and you’d want confidence intervals on that prediction.\n\n# predict() needs a data.frame with the same predictors used in beetle_lm\npredict(beetle_lm, \n        newdata = data.frame(Humidity = 50), \n        interval = \"predict\", \n        level = 0.95)\n\n      fit      lwr      upr\n1 6.04292 5.303471 6.782368\n\n# or more fully:\npredict(beetle_lm, \n        newdata = data.frame(Humidity = seq(0, 100, by=25)), \n        interval = \"predict\", \n        level = 0.95)\n\n       fit      lwr      upr\n1 8.704027 7.868990 9.539064\n2 7.373474 6.608630 8.138317\n3 6.042920 5.303471 6.782368\n4 4.712366 3.949031 5.475701\n5 3.381812 2.549540 4.214084\n\n\nAnd we can plot these intervals too:\n\nnew_humidity_df &lt;- data.frame(Humidity = 0:100)\nbeetle_pred_line &lt;- predict(beetle_lm, \n                            newdata = new_humidity_df,\n                            interval = \"confidence\", \n                            level = 0.95)\nbeetle_pred_obs &lt;- predict(beetle_lm,\n                           newdata = new_humidity_df, \n                           interval = \"prediction\", \n                           level = 0.95)\n\npar(mfrow=c(1,1))\nplot(WeightLoss_Mg ~ Humidity, data = beetle_df, ylim=c(3, 10),\n     xlab = \"Relative humidity (%)\", ylab = \"Weight loss (mg)\") \nlines(new_humidity_df$Humidity, beetle_pred_line[, \"fit\"])\nlines(new_humidity_df$Humidity, beetle_pred_line[, \"lwr\"], lty = 2)\nlines(new_humidity_df$Humidity, beetle_pred_line[, \"upr\"], lty = 2)\nlines(new_humidity_df$Humidity, beetle_pred_obs[, \"lwr\"], lty = 3)\nlines(new_humidity_df$Humidity, beetle_pred_obs[, \"upr\"], lty = 3)\n\n\n\nFigure 5.7: Regression line (solid) with upper and lower 95% confidence intervals on the regression line (dashed) and 95% prediction intervals (dotted).\n\n\n\nThese are prediction intervals and they are broader than confidence intervals. The confidence intervals express your confidence about the regression line for the population. The prediction interval expresses your confidence about the distribution of the observations for the population. For more kicks, import the ‘PhosphateCalibration’ sheet (1st year practical data) into R and duplicate the following plots and confidence intervals.\n\nphosphate_df &lt;- read_xlsx(\"data/Practical_6.xlsx\", \"PhosphateCalibration\")\n\n\n\n\n\nFigure 5.8: Phosphate calibration scatter plot.\n\n\n\n\n\n\n\nFigure 5.9: Phosphate calibration regression diagnostics.\n\n\n\n\n\n\nCall:\nlm(formula = Absorbance ~ Concentration, data = phosphate_df)\n\nResiduals:\n        1         2         3         4         5         6 \n-0.001605 -0.001213 -0.003125  0.005355  0.005314 -0.004726 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.002605   0.002853   0.913 0.412856    \nConcentration 0.023040   0.001849  12.464 0.000238 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.004823 on 4 degrees of freedom\nMultiple R-squared:  0.9749,    Adjusted R-squared:  0.9686 \nF-statistic: 155.3 on 1 and 4 DF,  p-value: 0.0002383\n\n\nQ182. Are you happy with your model assumptions?\nQ183. Write down the regression equation.\nQ184. Determine the 95% confidence interval for the regression line.\nQ185. For a concentration of 0.75 units, what values would you expect (95 times in 100) to see from your experimental set-up?\nYou should get:\n\n\n         fit         lwr        upr\n1 0.01988519 0.005298129 0.03447225"
  },
  {
    "objectID": "P5_ANOVA_regression.html#conclusions",
    "href": "P5_ANOVA_regression.html#conclusions",
    "title": "5  ANOVA and regression",
    "section": "\n5.3 Conclusions",
    "text": "5.3 Conclusions\nCorrelation is a measure of association between two variables. It is appropriate to use correlation to measure this association when one cannot or does not wish to assume that any relationship is causative. Pearson correlation coefficients should only be used where it is fair to assume (by looking at scatter plot) that the relationship is approximately linear. Where linearity does not apply, attempt to transform one or both of the variables. Where there are outliers (that cannot be removed) or where one is uncertain about some of the data, then non-parametric ranked based correlation coefficients, such as the Spearman coefficient, should be used. As with GLMs, correlation analysis assumes that all points are independent of each other.\nLinear regression is one of the most widely used statistical techniques. It is used to examine causal relationships, often where experimental manipulations are conducted. Regression is a general linear model and it lies within the generalized linear model family (GLMs). GLMs allow you to model data that is not normally distributed, including proportions (bounded by 0 and 1), or counts (bounded by 0). Using a GLM is a much better way of analyzing these data compared with transforming the response variable or using non-parametric techniques. All members of the GLM family make the assumptions that measurements are independent of each other. Where this assumption fails you can use generalized linear mixed models (GLMMs). Extensions of simple linear regression include multiple regression which examines the influence of two or more continuous variables on a response variable."
  },
  {
    "objectID": "P6_machine_learning.html#multivariate-analysis-in-r",
    "href": "P6_machine_learning.html#multivariate-analysis-in-r",
    "title": "6  Machine Learning",
    "section": "\n6.1 Multivariate analysis in R",
    "text": "6.1 Multivariate analysis in R\nFor the non-metric NMDS workflow, we’ll start by simulating some data. Then, we’ll transform the data (‘log’ and fourth-root) and generate dissimilarity matrices from this transformed data. Finally, we’ll plot it using NMDS. We’ll generate diversity indices with our simulated data, then have a look at some real data (available via the package vegan).\nFor the PCA workflow, we’ll have a look at some environmental data using PCA. Throughout this practical, we’ll illustrate some coding techniques which will help you practice wrangling your data to make it suitable for analysis. As we’ve discussed, data wrangling is a time-consuming and crucial part of data analysis so familiarity with this is essential.\nRecall that the ^ operator raises each element in a vector to a power. For example, say we create a vector obs_values &lt;- c(1, 10, 35). We can calculate a fourth-root transformation of the whole vector with obs_values^(1/4). Remember from previous courses that this is identical to sqrt(sqrt(obs_values)).\n\n# these toy data duplicate those in the multivariate lecture.\nworm_df &lt;- data.frame(\n  row.names = c(\"CE1\", \"CE2\", \"Ref1\", \"Ref2\"),\n  Capitella = c(1000, 1500, 10, 50),\n  Malacoeros = c(500, 2000, 50, 25),\n  Mysella = c(1, 0, 25, 30),\n  Nucella = c(1, 0, 20, 15)\n)\nworm_df\n\n     Capitella Malacoeros Mysella Nucella\nCE1       1000        500       1       1\nCE2       1500       2000       0       0\nRef1        10         50      25      20\nRef2        50         25      30      15\n\n# alt-log transformation: ifelse(x==0, 0, log(x))\nworm_df_log &lt;- decostand(worm_df, method = \"log\", logbase = 10)\nworm_df_4rt &lt;- worm_df^0.25\n\nTake a look at worm_df, worm_df_log, and worm_df_4rt to be sure they make sense.\nNext we use vegdist() to generate different distance matrices for the raw and transformed data. See ?vegdist for more information. We’ll use Bray-Curtis for the raw and 4th-root transformed data, and alternative Gower for the alt-log transformed data. See the multivariate lecture area on Brightspace for details on these algorithms. Feel free to investigate other combinations of data transformation and dissimilarity matrices.\n\nvegdist(worm_df, method = \"bray\")\n\n           CE1       CE2      Ref1\nCE2  0.4002399                    \nRef1 0.9228376 0.9667129          \nRef2 0.9050555 0.9585635 0.3333333\n\nvegdist(worm_df_4rt, method = \"bray\")\n\n            CE1        CE2       Ref1\nCE2  0.18044721                      \nRef1 0.39098221 0.59100112           \nRef2 0.36024122 0.55728021 0.08642723\n\nvegdist(worm_df_log, method = \"altGower\")\n\n           CE1       CE2      Ref1\nCE2  0.6945378                    \nRef1 1.4247425 2.1192803          \nRef2 1.3138181 2.0083559 0.3010300\n\n\nQ140. How many combinations of transformation and dissimilarity matrix do we have already?\nQ141. Examine one or more of the dissimilarity matrices. Which sites are closer (smaller numbers) and which are further apart (bigger numbers)? Does this align with an ‘eyeballing’ of the data? How has the data transformation changed the resultant dissimilarity matrix?\nNext we want to plot our dissimilarities, so we can easily visualize which sites are more similar/dissimilar to each other. We will use functions from vegan to plot the ordinations. Remember that you can run ?packageName to learn more about any R package, typically with helpful examples and vignettes of the most useful applications of the package.\nWe have numerous options in relation to displaying the dissimilarity matrices. We’ll explore non-metric multiple dimensional scaling, abbreviated to NMDS, nMDS, nmMDS, or just MDS.\nQ142. Use ?vegdist to see what alternative distance measures are available.\nThe metaMDS() function calculates a dissimilarity matrix (as we did above) and produces an R object with all the information needed for plotting. It expects that samples are in rows and species (or features) are in columns. We can also specify the distance metric, the number of axes, whether to autotransform the data, and many other options. See ?metaMDS for the default values and other available arguments.\n\nord_raw &lt;- metaMDS(worm_df, distance = \"bray\", k = 2,\n                   autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(worm_df, distance = \"bray\", k = 2, autotransform = FALSE, :\nstress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nordiplot(ord_raw, choices = c(1, 2), display = \"sites\", \n         type = \"text\", main = \"NMDS: raw data, Bray-Curtis\")\n\n\n\nFigure 6.1: Simple pattern in example worm data.\n\n\n\nCompare this with the 4th-root transformed data using the same distance metric.\n\nord_4rt &lt;- metaMDS(worm_df_4rt, distance = \"bray\", k = 2, \n                   autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(worm_df_4rt, distance = \"bray\", k = 2, autotransform =\nFALSE, : stress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\n\nQ143. Plot ord_4rt. How has the 4th root changed your data interpretation?\nQ144. Examine your ordination(s) and interpret, cross referencing to the raw and transformed data. Are the patterns that you see in the data apparent on the ordination?\nYou can include on your plot the species ‘locations’ (as determined by their correlation with the axes). This shows where the main associations are occurring.\n\n# you can also plot the 'species' on the ordination,\nordiplot(ord_4rt, choices = c(1, 2), display = c(\"sites\", \"species\"), \n         type = \"text\", main = \"NMDS, 4th-rt trans., Bray-Curtis\")\n\n\n\nFigure 6.2: Species superimposed onto ordination, indicating species-site associations.\n\n\n\nQ145. Re-plot the untransformed data, but this time include the species. How has the transformation changed your interpretation of the data?\nLet’s have a look at another simulated dataset.\n\ncomm_df &lt;- data.frame(\n  row.names = c(\"Dunst\", \"Creran\", \"Lismore\", \"Charl\"),\n  SpA = c(1, 20, 30, 40),\n  SpB = c(11, 22, 50, 1),\n  SpC = c(500, 40, 30, 20),\n  SpD = c(10, 25, 35, 50),\n  SpE = c(4, 3, 2, 1),\n  SpF = c(40, 250, 1, 9)\n)\ncomm_df\n\n        SpA SpB SpC SpD SpE SpF\nDunst     1  11 500  10   4  40\nCreran   20  22  40  25   3 250\nLismore  30  50  30  35   2   1\nCharl    40   1  20  50   1   9\n\nord_comm &lt;- metaMDS(comm_df, distance = \"bray\", k = 2, \n                    autotransform = FALSE, trace = FALSE)\n\nWarning in metaMDS(comm_df, distance = \"bray\", k = 2, autotransform = FALSE, :\nstress is (nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nordiplot(ord_comm, choices = c(1, 2), \n         display = c(\"sites\", \"species\"), type = \"text\")\n\n\n\nFigure 6.3: Ordination for a simulated dataset showing sites and species.\n\n\n\nQ146. Have a look at these raw data. What are the main trends? Which sites are more similar?\nThese data are still much simpler than most ‘real’ data sets but it is still difficult to summarise the similarities and differences between stations. However, multivariate analyses help you in this process.\nQ147. Now fourth-root transform these data, generate the new dissimilarity matrix and plot it.\nQ148. What has the transformation done to your interpretation of differences between the Sites and Site x Species associations?\nQ149. Add a title to your graph."
  },
  {
    "objectID": "P6_machine_learning.html#diversity-indices",
    "href": "P6_machine_learning.html#diversity-indices",
    "title": "6  Machine Learning",
    "section": "\n6.2 Diversity indices",
    "text": "6.2 Diversity indices\nPrior to the development of the multivariate techniques you’ll be using today, univariate indices were derived from multivariate data. A classic example of such a univariate measure in ecology is the Shannon-Wiener diversity index (a.k.a., Shannon’s H). This index balances the number of species in a sample and the relative abundance of each species (where ‘species’ can once again be any sort of feature). Univariate measures of ‘evenness’ can also be derived from multivariate data and, when reporting species data, you may also wish to include species richness, which is just the number of species present regardless of their abundances.\nWe can use the comm_df dataset we invented to explore some diversity concepts.\nQ150. Looking at the comm_df data (by eye) and given the description above, which of the sites is associated with the lowest and highest diversity?\n\nshannon_H &lt;- diversity(comm_df, \"shannon\", base = exp(1)) \nrichness &lt;- specnumber(comm_df) \nbarplot(shannon_H, main = NULL, ylab = \"Shannon's H\")\n\n\n\nFigure 6.4: Shannon diversity\n\n\n\nTry plotting richness.\nQ151. Do the plots correspond to what you expected?\nQ152. What does specnumber do?\nQ153. Change comm_df to introduce some very evenly distributed species across sites and some with extremes or absences and see how this effects diversity and richness.\nQ154. How could you ‘counter’ any extremes (as in superabundant taxa) in the raw count data that you’ve generated? Try your idea.\nQ155. Which description (diversity or richness) is ‘best’ for describing your multivariate data? How does this compare to NMDS?"
  },
  {
    "objectID": "P6_machine_learning.html#nmds-on-real-data",
    "href": "P6_machine_learning.html#nmds-on-real-data",
    "title": "6  Machine Learning",
    "section": "\n6.3 NMDS on real data",
    "text": "6.3 NMDS on real data\nNow you’ve been introduced to NMDS and diversity indices, with ‘fake’ data, you are in a better position to interpret real data. Using simulated (or at least simple) data to learn new statistical techniques is usually the best approach because it gives you the opportunity to get a better sense for how the algorithms work (and to be sure your code is free from bugs!).\nHave a look at the varespec and varechem datasets included in the vegan package. I’ve reproduced the examples below:\n\ndata(varespec) # ?varespec -- percent cover of 44 spp in lichen pastures \ndata(varechem) # ?varechem -- associated soil characteristics\nord_vare &lt;- metaMDS(varespec^0.25, distance = \"bray\", \n                   trace = FALSE, autotransform = FALSE)\n\n\npar(mfrow=c(1,3), mar=c(4,4,1,1)) \nordiplot(ord_vare, choices = c(1, 2), display = \"sites\", type = \"text\")\nordiplot(ord_vare, choices = c(1, 2), display = \"species\", type = \"text\")\nordiplot(ord_vare, choices = c(1, 2), display = c(\"sites\", \"species\"),\n  type = \"text\")\n# You can superimpose environmental variables onto NMDS-ordinations.\nef &lt;- envfit(ord_vare, varechem) #\nplot(ef, p.max = 0.1, col = \"green\") # overlay environmental variables\nplot(ef, p.max = 0.01, col = \"blue\") # subset based on p\n\n\n\nFigure 6.5: vare- data from vegan showing sites only, species only, and both plus environmental overlays.\n\n\n\nSee how the multivariate analysis has taken all those data, both species and environmental, and ‘communicated’ them in one single figure? You can see numerous relationships (both positive and negative) in this figure, and species-site-environment associations. It also illustrates a potential challenge with multivariate ordinations. It is very easy to get cluttered and overloaded. There is no easy way around this, though there is further help in these packages.\nYou are in charge of the analysis. You can change the emphasis and elements of the message depending on your data transformation, dissimilarity metric, and ordination technique (hence ‘black art’). There is no absolutely ‘correct’ way to go about multivariate stats, so different statisticians will have their favoured approaches and methods.\nNote: Some functions (e.g. metaMDS()) default to autotransform your data if the function thinks it is necessary. This can be useful but, in scientific reports, you must specify what transformations you used. Here you don’t know what the function applies as it depends on the data, but it could be the square or fourth-root and/or Wisconsin transformation (which is a double standardisation). My advice is only to use transformations that you specify."
  },
  {
    "objectID": "P6_machine_learning.html#principal-components-analysis-pca",
    "href": "P6_machine_learning.html#principal-components-analysis-pca",
    "title": "6  Machine Learning",
    "section": "\n6.4 Principal components analysis (PCA)",
    "text": "6.4 Principal components analysis (PCA)\nPCA is a long-established multivariate technique that is often applied to ‘environmental’ data rather than ‘count’ data. Environmental data is, usually, quite different from species count data in that most environmental parameters (e.g. metal concentrations) are present, at least to some degree. This contrasts to species data where many species are often absent (zeros). These zero counts would lead to problems if analysed using PCA, since PCA would ‘think’ that sites that shared lots of ‘absences’ were more similar, which is not necessarily desirable.\nWith environmental data, such as temperature, light, and concentration, zeros tend to be less prevalent. However, environmental data (e.g. that describing your sampling location in space and time) might include all manner of different variables on different scales (e.g., radiant flux in lumens, temperature in C, nutrient/contaminant concentrations in mg/l). What you wouldn’t wish to see is your arbitrary choice of measurement unit (e.g., C or K) having any influence on your analysis such that a variable is given more weight simply because the numbers are larger.\nInstead, we want all of our variables to be treated ‘equally’. You can do all this by setting pcomp(..., scale=TRUE). Scaling means that each measurement is expressed in units of standard deviation (a Z-score!!). Usually it is desirable to center the data as well by subtracting the mean. Centering and scaling means that each of the environmental variables is of ‘equal importance’ regardless of the magnitude of the raw values.\nA basic, and very friendly, introduction to PCA is given in Chapter 4 of Clarke et al. 2014.\nThe data set we’ll look at here is from SAMS Professor Tom Wilding’s PhD thesis. He set up an experiment to examine the relative leaching of trace metals from concrete, granite, and a control (artificial seawater). Concrete contains cement which is enriched in vanadium and molybdenum, and these elements could leach out in dangerous amounts. Granite, the main constituent of this concrete, might also leach some trace elements. He suspended concrete and granite powder in artificial water, constantly agitated it, and measured the leachate concentrations over 100 days Wilding and Sayer 2002.\n\nleach_df &lt;- read_excel(\"data/practical_5.xlsx\", sheet = \"Leaching\") |&gt;\n  mutate(Treat_abbr = factor(Treat, # abbreviate for cleaner plotting\n                             levels = c(\"concrete\", \"control\", \"granite\"),\n                             labels = c(\"conc\", \"ctrl\", \"gran\")),\n         Treat_day = paste(Treat_abbr, Day, sep=\"_\"), # treat + days in exprmnt\n         Conc = signif(Conc)) |&gt;\n  select(Treat_day, Element, Conc) # remove columns that aren't of use,\n#summary(leach_df)\n\n# not dominated by zeros; try other values (e.g. &lt;10)\ntable(leach_df$Conc == 0) \n\n\nFALSE  TRUE \n  143     4 \n\nmean(leach_df$Conc == 0) # recall that R treats T/F as 1/0 \n\n[1] 0.02721088\n\n\n\npar(mfrow=c(1,1))\nhist(leach_df$Conc, main = NULL) \n\n\n\nFigure 6.6: Histogram of leaching data (raw) illustrating the wide range of values and skew.\n\n\n\nNext, we need to re-organise the data into a wider format so that each element is a column and each row is a sample.\n\nleach_df_wide &lt;- leach_df |&gt;\n  pivot_wider(names_from=\"Element\", values_from=\"Conc\")\n\nThe column Treat_day is coded as trt_day, where trt is the 4 letter code indicating treatment type and day is the number of days elapsed in the experiment (one of 1, 4, 7, 17, 32, 50 or 100). So gran_32 means the granite treatment sampled at day 32.\nWe can calculate the principal components using prcomp(), subsetting the dataframe to give only the columns with element concentrations (i.e., removing Treat_day, which is the first column). We’ll also set the arguments for centering and scaling to TRUE.\n\nPCA_leach &lt;- prcomp(leach_df_wide[, -1], center = TRUE, scale = TRUE) \nscreeplot(PCA_leach, main = NULL, ylab = \"Relative variation explained\")\n\n\n\nFigure 6.7: Scree plot showing the variance explained by each principal component.\n\n\n\n\n# take a look at PCA_leach\nPCA_leach\nstr(PCA_leach)\n\nYou can see from the scree plot that the amount variance explained declines with principal component as expected and that there is very little variation left after 3 principal components. That is, nearly all of the variation in the dataset is captured by PC1, PC2, and PC3. In this case, PCA has essentially solved the ‘curse of dimensionality’ by successfully reducing 7 dimensional data to about three.\nData transformations are critical to PCA analysis, as they are with NMDS. In most PCAs you center and standardize your data so that each column is on the same scale.\nWe can plot our results using biplot() which has some helpful defaults including labels for the samples (Treat_day) and the correlation strength of each element with PC1 and PC2:\n\nbiplot(PCA_leach, xlabs = leach_df_wide$Treat_day, cex = 0.75)\n\n\n\nFigure 6.8: PCA of the complete dataset, illustrating the common challenge of overplotting.\n\n\n\nTo plot more than 2 dimensions you could use a 3D plot, but these are frankly difficult to interpret since it is reduced back to 2D on a page or computer screen. Another option is to use colour for the 3rd axis. Here’s an example using ggplot2. Adding arrows showing the loadings (see below) is possible, but more work.\n\nPCA_leach$x |&gt;\n  as_tibble() |&gt;\n  mutate(Treat_day = leach_df_wide$Treat_day) |&gt;\n  ggplot(aes(PC1, PC2, colour = PC3, label = Treat_day)) + \n  geom_label(size = 3, fill = NA) +\n  scale_colour_gradient2(mid = \"cornsilk2\") +\n  theme_bw()\n\n\n\nFigure 6.9: PCA of the complete dataset using ggplot2.\n\n\n\nThe ordination plots the relative positions (in terms of similarity) of the samples. There are numerous label overlaps making the interpretation of the ordination difficult. If you were producing this for publication you would need to sort this out.\nQ156. Which elements are positively associated with granite and concrete, particularly after longer periods of leaching?\nAs is typical, overlapping points make interpretation more difficult. There are elegant solutions to this (in terms of labelling) but for now, we’ll split the data and analyse it separately.\nWe’ll need more data wrangling to split it efficiently and we’ll use grepl(), which identifies a pattern within a character using a regular expression (a.k.a., regex), returning a TRUE or FALSE for each element in the character vector. Here, we’ll filter the dataframe to only include rows where Treat_day contains conc or gran. Remember the ‘or’ operator |?\n\n#?grepl\n#cbind(leach_df_wide$Treat_day, grepl(\"conc|gran\", leach_df_wide$Treat_day))\nleach_df_trts &lt;- leach_df_wide |&gt;\n  filter(grepl(\"conc|gran\", Treat_day))\nPCA_trts &lt;- prcomp(leach_df_trts[, -1], scale = TRUE, center = TRUE)\nbiplot(PCA_trts, xlabs = leach_df_trts$Treat_day)\n\n\n\nFigure 6.10: PCA of the concrete and granite, illustrating temporal and treatment differences.\n\n\n\nQ157. Repeat the analysis, but set scale = FALSE. Which element now seems to dominate the analysis? Explain what you see.\nA PCA is normally reported with the proportion of the variation explained by each of the principal components (and the cumulative proportion). If the cumulative proportion for the 1st two PCs is high, then your 2D (PC1 and PC2) ordination is a good representation of the similarities between samples. In that sense a high cumulative proportion is analogous to a low stress (for NMDS).\nLet’s have a look at a summary of the principal components.\n\nPCA_leach_summary &lt;- summary(PCA_leach) \n#PCA_leach_summary\n#str(PCA_leach_summary)\nPCA_leach_summary$importance[, 1:4] # extract only PC1-4\n\n                            PC1      PC2       PC3       PC4\nStandard deviation     1.988091 1.450775 0.7782675 0.5599535\nProportion of Variance 0.564640 0.300680 0.0865300 0.0447900\nCumulative Proportion  0.564640 0.865320 0.9518500 0.9966400\n\n\nAs you can see, PC1, PC2, and PC3 capture more than 95% of the variance in our data. Adding PC4 brings that up above 99%.\nFinally, we can look at factor loadings. This is a measure of how each feature (metal concentration in this case) relates to the principal components. In PCA, the principal components are sequentially ‘less’ influential since they describe increasingly smaller amounts of variation. By centering and standardising your response variables, you can assess the relative importance of each in driving the patterns you observe in your ordination. The magnitude is what we’re interested in rather than the sign.\nIn an object created by prcomp(), the loadings are stored as .$rotation.\n\n# ?prcomp\n# str(PCA_leach)\nsignif(PCA_leach$rotation[, 1:5], 3) # factor loadings for PC1-5\n\n      PC1     PC2     PC3    PC4      PC5\nBa -0.367  0.2090  0.6510 -0.619 -0.00405\nFe -0.380 -0.0440 -0.7370 -0.553  0.01420\nMn -0.146  0.6500 -0.0963  0.211  0.67300\nMo -0.430 -0.3360  0.0795  0.283 -0.09810\nRb -0.462 -0.2440  0.0327  0.305  0.21600\nSr -0.495 -0.0788  0.0663  0.168  0.01510\nU  -0.238  0.5940 -0.1110  0.254 -0.70000\n\n\nHere you can see that Sr, Rb, Mo are relatively ‘important’ in driving the multivariate pattern you’ve observed (i.e., high absolute values on PC1) while Mn and U have high values for PC2 (you could say that PC2 accounts for Mn and U)."
  },
  {
    "objectID": "P6_machine_learning.html#conclusions",
    "href": "P6_machine_learning.html#conclusions",
    "title": "6  Machine Learning",
    "section": "\n6.5 Conclusions",
    "text": "6.5 Conclusions\nI will let you write your own summary and conclusions from this practical as this will help you consolidate your understanding. You may wish to consider the following: the nature of multivariate vs. univariate data, types and characteristics of multivariate data (e.g. counts vs. environmental data), the analytical methods introduced here for dealing with data of different types, the purpose and effect of transformations and standardisations and where they are used, the use of summary metrics (e.g. diversity metrics) and their advantages and disadvantages. You should also be familiar with the interpretation of simple ordinations (both PCA and NMDS) and the concept of stress and ‘proportion explained’ (for NMDS and PCA respectively)."
  },
  {
    "objectID": "P7_appendix.html#probability",
    "href": "P7_appendix.html#probability",
    "title": "7  Appendix",
    "section": "7.1 Probability",
    "text": "7.1 Probability\n\n7.1.1 Probability for equally likely outcomes\nThe probability of event \\(A\\) occurring is \\(P(A) = \\frac{x}{n}\\), where \\(n\\) is the number of trials and \\(x\\) is the number of trials during which \\(A\\) occurred.\n\n\n7.1.2 Multiplication and addition\nThe general case of the multiplication rule is that, for two events \\(A\\) and \\(B\\):\n\\(P(A \\& B) = P(A) * P(B|A)\\), where \\(P(B|A)\\) is the probability that \\(B\\) occurs given than \\(A\\) has already occurred.\nThe Addition Law states that, for two events \\(A\\) and \\(B\\):\n\\(P(A\\ or\\ B\\ or\\ A\\&B) = P(A) + P(B) - P(A\\&B)\\)\nand\n\\(P(A\\ or\\ B\\ but\\ not\\ A\\&B) = P(A) + P(B) - 2*P(A\\&B)\\)\nand when \\(A\\) and \\(B\\) are mutually exclusive, then:\n\\(P(A\\ or\\ B) = P(A) + P(B)\\)\n\n\n7.1.3 Bayes’ theorem\n\\(P(A|B)= \\frac{P(B|A) * P(A)}{P(B)}\\), where \\(P(B)= P(A) * P(B|A) + P(A’) * P(B|A’)\\)."
  },
  {
    "objectID": "P7_appendix.html#univariate-statistics",
    "href": "P7_appendix.html#univariate-statistics",
    "title": "7  Appendix",
    "section": "7.2 Univariate statistics",
    "text": "7.2 Univariate statistics\n\n7.2.1 Mean\nmean()\nThe mean is calculated as \\(\\bar{y} = \\frac{\\sum{y}}{n}\\).\nFor measures of central tendency and dispersion, we use Greek letters to refer to population values and Latin letters to refer to samples:\n\n\n\n\nPopulation\nSample\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{y}\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\\(s^2\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s\\)\n\n\n\n\n\n7.2.2 Median, quartiles and adjacent values\nmedian(), quantile(), IQR()\nWith data ordered by rank, the median value is the middle value or the \\((\\frac{n+1}{2})^{th}\\) value, the lower quartile, \\(Q1\\), is the \\((\\frac{n+1}{4})^{th}\\) value, the upper quartile, \\(Q3\\), is the \\((\\frac{3*(n+1)}{2})^{th}\\) value. The inter-quartile range is \\(IQR = Q3 – Q1\\). The upper adjacent value is the upper value that is less than \\(Q3 + (1.5 * IQR)\\). The lower adjacent value is the lower value that is more than the \\(Q1 – (1.5 * IQR)\\). Outliers are defined as values that lie outside the range \\(Q1 - 1.5*IQR\\) or \\(Q3 + 1.5*IQR\\).\n\n\n7.2.3 The sum of squares\nThe sum of squares is given by \\(SS = \\sum{y^2} - \\frac{(\\sum{y})^2}{n}\\) where \\(n\\) is the number of observations.\n\n\n7.2.4 Measures of dispersion\nvar(), sd(), length()\nWhen dealing with populations the following formulas are used:\nVariance: \\(\\sigma^2 = \\frac{SS}{n}\\)\nStandard deviation: \\(\\sigma = \\sqrt{\\frac{SS}{n}} = \\sqrt{\\sigma^2}\\)\nWhen dealing with samples the following formulas are used:\nVariance: \\(s^2 = \\frac{SS}{n-1}\\)\nStandard deviation: \\(s = \\sqrt{\\frac{SS}{n-1}}\\)"
  },
  {
    "objectID": "P7_appendix.html#the-binomial-distribution",
    "href": "P7_appendix.html#the-binomial-distribution",
    "title": "7  Appendix",
    "section": "7.3 The Binomial distribution",
    "text": "7.3 The Binomial distribution\ndbinom(), pbinom(), qbinom(), rbinom()\nThe binomial distribution describes the expected distribution for two mutually exclusive outcomes. The formula is given by:\n\\(P(x) = \\frac{n!}{x!(n-x)!} p^x q^{n-x}\\)\nwhere \\(P(x)\\) is the probability of \\(x\\) ‘successes’ occurring, \\(n\\) is the number of trials, \\(p\\) is the probability of \\(x\\) in a single trial, and \\(q\\) is the probability of not \\(x\\) in a single trial with \\(p+q=1\\).\nA binomially distributed variable has mean = \\(n*p\\) and variance = \\(n*p*q\\)."
  },
  {
    "objectID": "P7_appendix.html#the-poisson-distribution",
    "href": "P7_appendix.html#the-poisson-distribution",
    "title": "7  Appendix",
    "section": "7.4 The Poisson distribution",
    "text": "7.4 The Poisson distribution\ndpois(), ppois(), qpois(), rpois()\nThe Poisson distribution for a sample is defined as follows:\n\\(P(x) = \\frac{\\bar{y}^x e^{-\\bar{y}}}{x!}\\)\nwhere \\(\\bar{y}\\) is the sample mean and \\(x\\) is the value (count) of interest."
  },
  {
    "objectID": "P7_appendix.html#z-scores",
    "href": "P7_appendix.html#z-scores",
    "title": "7  Appendix",
    "section": "7.5 Z scores",
    "text": "7.5 Z scores\nscale()\nThe standard normal distribution is \\(Norm(\\mu=0, \\sigma=1)\\). Z scores are used to convert any normal distribution to the standard normal distribution:\n\\(z = \\frac{y-\\mu}{\\sigma}\\)\nwhere \\(y\\) is the value, \\(\\mu\\) is the mean of the population and \\(\\sigma\\) is the standard deviation of the population."
  },
  {
    "objectID": "P7_appendix.html#samples-taken-from-a-population",
    "href": "P7_appendix.html#samples-taken-from-a-population",
    "title": "7  Appendix",
    "section": "7.6 Samples taken from a population",
    "text": "7.6 Samples taken from a population\nsd(), sqrt(), length(), mean(), sqrt()\n\n7.6.1 Standard error of the mean\nThe standard error is the standard deviation of sample means:\n\\(SEM = \\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(n\\) is the sample size. If we are testing a sample taken from a population of known population mean and population standard deviation we use the formula:\n\\(z = \\frac{\\bar{y}-\\mu}{\\sigma_{\\bar{y}}}\\)\nwhere \\(\\bar{y}\\) is a particular sample mean, \\(\\mu\\) is the population mean, and \\(\\sigma_{\\bar{y}}\\) is the standard error of the mean."
  },
  {
    "objectID": "P7_appendix.html#the-t-distribution",
    "href": "P7_appendix.html#the-t-distribution",
    "title": "7  Appendix",
    "section": "7.7 The t distribution",
    "text": "7.7 The t distribution\ndt(), pt(), qt(), rt()\nThe t-distribution describes the distribution of \\(T\\) statistics expected under the null hypothesis.\n\n7.7.1 The one sample t-test\nt.test()\nThe T statistic is calculated as:\n\\(T = \\frac{\\bar{y}-\\mu}{s_{\\bar{y}}}\\)\nWhere \\(\\bar{y}\\) is the sample mean, \\(\\mu\\) is the hypothesized population mean, and \\(s_{\\bar{y}}\\) is the standard error such that \\(s_{\\bar{y}} = \\frac{s}{\\sqrt{n}}\\) with sample standard deviation \\(s\\).\nUse t.test(), or find the p-value associated with your T with pt(T, df-1).\n\n\n7.7.2 Confidence intervals for sample means\nt.test(), qt(), mean(), sd(), length(), sqrt()\nThe 95% CI for a mean is calculated as:\n\\(\\bar{y} \\pm t_{\\alpha/2, df=n-1} * s_{\\bar{y}}\\)"
  },
  {
    "objectID": "P7_appendix.html#anova",
    "href": "P7_appendix.html#anova",
    "title": "7  Appendix",
    "section": "7.8 ANOVA",
    "text": "7.8 ANOVA\nanova(lm()), aov()\nAn ANOVA table contains the following:\n\n\n\n\n\n\n\n\n\n\nSource of variation\nSum of Squares\nDegrees of Freedom\nMean Square\nF\n\n\n\n\nBetween groups\n\\({SS}_{groups}\\)\n\\({df}_{groups}\\)\n\\(\\frac{{SS}_{groups}}{{df}_{groups}}\\)\n\\(\\frac{{MS}_{groups}}{{MS}_{error}}\\)\n\n\nError\n\\({SS}_{error}\\)\n\\({df}_{error}\\)\n\\(\\frac{{SS}_{error}}{{df}_{error}}\\)\n\n\n\nTotal\n\\({SS}_{total}\\)\n\\({df}_{total}\\)\n\n\n\n\n\nwith:\n\\({SS}_{groups} = \\sum{n_j (\\bar{y_j} - \\bar{\\bar{y}})^2}\\)\n\\({SS}_{error} = \\sum{(y_{ij} - \\bar{y_j})^2}\\)\n\\({SS}_{total} = {SS}_{groups} + {SS}_{error} = \\sum{(y_{ij} - \\bar{\\bar{y}})^2}\\)\nwhere \\(y_{ij}\\) is the \\(i^{th}\\) observation in group \\(j\\), \\(\\bar{y_j}\\) is the mean for group \\(j\\), and \\(\\bar{\\bar{y}}\\) is the grand mean. With \\(N\\) total observations, \\(n_j\\) observations per group, and \\(k\\) groups, the degrees of freedom are:\n\\({df}_{groups} = k - 1\\)\n\\({df}_{error} = N - k = k (n_j-1)\\)\n\\({df}_{total} = {df}_{groups} + {df}_{error} = N-1\\)\nThe critical value (CV) for samples of equal size is given as:\n\\(CV = q \\sqrt{\\frac{{MS}_{error}}{n}}\\)\nwhere \\(q\\) either comes from a table online (the studentized range distribution), or from qtukey(1-alpha, k, df_error)."
  },
  {
    "objectID": "P7_appendix.html#regression-correlation",
    "href": "P7_appendix.html#regression-correlation",
    "title": "7  Appendix",
    "section": "7.9 Regression & correlation",
    "text": "7.9 Regression & correlation\nlm(), cor(), confint(), predict()\nWe do not bother calculating regression coefficients by hand."
  }
]