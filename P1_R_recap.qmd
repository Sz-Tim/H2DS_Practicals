# Practical 1: R recap

**TODO: cut a bunch**

Statistics can be divided into two broad categories - descriptive statistics and inferential statistics. In this practical session we will focus on descriptive stats and how to appropriately display and summarise data. You should already be aware of several techniques for displaying data (e.g., bar charts, histograms and scatter plots). When and how to use these different techniques is one focus of today's practical.

There are two purposes for visualizing data.

First, the best way to get a 'gut' feel for your dataset is to look at it graphically. Examining data graphically enables you to identify any outliers (suspicious observations which could be errors). It will also help you to select the most appropriate inferential statistical model (more on this through the course).

Second, visualizations are used to impart information as clearly as possible to 'the reader', drawing attention to the most interesting aspects of your data. Graphics that are confusing, either through a lack of detail (e.g. no labels) or that contain too much information will fail in this central objective.

As you create graphics, keep in mind that they may be viewed on different machines or printed in grey scale. Importantly, some colour combinations may be difficult for colour-blind or visually impaired readers. Colour scales such as those available from [ColorBrewer](https://colorbrewer2.org) or [viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) are designed with this in mind.

------------------------------------------------------------------------

## Basic data exploration

### Object structure

We will mostly work with dataframes. A dataframe is a 2D rectangular structure with columns and rows. In a tidy dataset, each row represents an 'observation' and each column represents a 'variable'. R (and often packages) contains several built-in dataframes.

The dataframe `cars` gives the max speeds and stopping distances for cars built in the early 20th century. We are going to use `cars` to demonstrate a few basic concepts in relation to R programming and statistical analysis.

```{r}
#| eval: FALSE
# functions for basic details of objects
str(cars)
class(cars)
names(cars)
head(cars)
```

```{r}
head(cars, 2)
tail(cars, 2)
# what are the last 10 rows?
```

### Subsetting, renaming, and rearranging

There are several ways to access subsets of a dataframe:

-   Use `.$columnName` or `.[["columnName"]]` to extract a single column
-   Use `.[rows,columns]` to extract a block

```{r}
#| eval: FALSE
cars$speed # whole column
cars[["speed"]] # whole column
```

```{r}
cars[1, 1] # row 1, column 1
cars[1:5, 1] # rows 1-5, column 1
cars[1:3, ] # leaving the 'columns' space blank returns all columns
```

We can also change column names. For illustration, let's make a copy of the dataframe to do that.

```{r}
cars2 <- cars 
names(cars2)
names(cars2)[1] <- "speed_mph"
names(cars2)
names(cars2) <- c("speed_mph", "dist_ft") # change both column names
```

Rearranging and duplicating columns is also easy.

```{r}
head(cars2, 2)  
cars2 <- cars2[, 2:1] # rearrange columns 
head(cars2, 2) 
cars3 <- cars2[, c(2, 1, 1)] # duplicate a column
head(cars3, 2)
cars3 <- cars3[, 1:2] # remove the duplicated column
head(cars3, 2)
cars3$dist_x_speed <- cars3$dist_ft * cars3$speed_mph # create a new column
head(cars3, 2)
rm(cars3) # remove the dataframe 'cars3'
```

You can also subset based on criteria. Say we only want rows where the speed is $>$ 20 mph:

```{r}
fast_i <- which(cars2$speed_mph > 20) 
str(fast_i)
cars_fast <- cars2[fast_i, ] # or: cars2[which(cars2$speed_mph > 20), ]
class(cars_fast) 
ncol(cars_fast)  # and how many *rows* are there?
head(cars_fast, 2)
```

### NAs and summary

When you import data, you should check for missing values. These are represented as `NA`.

We can check each element of a vector using `is.na()`, which will return `TRUE` if an element *is* `NA`, and `FALSE` if an element *is not* `NA`.

```{r}
#| eval: FALSE
#| echo: TRUE
is.na(cars2$speed_mph)
```

R converts a logical vector (i.e., `TRUE`/`FALSE`) to numeric (i.e., `1`/`0`) automatically. This is handy, but dangerous if you don't realize it.

```{r}
sum(is.na(cars2$speed_mph))
carsNA <- cars2
carsNA[c(2, 4, 5, 10), 1] <- NA
sum(is.na(carsNA$dist_ft))
```

Another very useful check is `summary()`:

```{r}
summary(cars)
```

```{r}
#| eval: FALSE
summary(carsNA)
```

Once you have assured yourself that your dataframe looks sensible, that it contains the data you expect, and that you know what the data-types are, you can start to explore and summarise your data.

There are many graphical methods for data exploration. The appropriate method depends on the nature of the data and what you wish to communicate to the reader.

------------------------------------------------------------------------

## Graphical methods for displaying data

Always keep in mind that the primary reason for data visualization is to impart information concisely and accurately to your reader.

Graphics must be clear, concise and easy to understand. Brightspace contains some examples of bad graphics ('Learning resources\>Lecture support material\>Introduction (Lectures 1-3)\>Graphics').

```{=tex}
\begin{figure}
\centerline{\includegraphics[width=0.5\textwidth]{figs/bad_fig_1.png}}
\caption{An example of a terrible graphic, as published in a Scottish government report.}
\label{fig:bad1}
\end{figure}
```
In addition to poor design choices for effective communication (Fig. \@ref(fig:bad1)), graphics can also be deliberately misleading (Fig. \@ref(fig:bad2)).

```{=tex}
\begin{figure}
\centerline{\includegraphics[width=0.5\textwidth]{figs/bad_fig_2.png}}
\caption{A misleading graphic. What type of plot is this and how is it misleading?}
\label{fig:bad2}
\end{figure}
```
### Scatter plot

The scatter plot is used to plot two continuous variables against each other. It is commonly used for analyses like correlation or linear regression.

```{r}
#| fig.dim: c(6,2.5)
#| out.width: "100%"
#| out.height: "100%" 
#| echo: FALSE
par(mfrow=c(1,3)) # set the plot window to show 1 row, 3 columns
# plot(response ~ predictor, data=dataframe)
plot(dist_ft ~ speed_mph,
  data=cars2, xlab="Speed (mph)", ylab="Distance (ft)",
  main="Default symbol")
plot(dist_ft ~ speed_mph,
  data=cars2, xlab="Speed (mph)", ylab="Distance (ft)",
  pch=2, main="Setting 'pch=2'")
# you can check out more symbols and their respective numbers using this plot:
plot(1:20, pch=1:20, main="'pch' symbols 1 to 20")
```

```{r}
#| echo: FALSE
par(mfrow=c(1,1))
```

*Q1. Search the `plot` help page for 'title', then add an appropriate title to your plot.*

*Q2. `?points` opens the help page for points. Search the help page for 'pch' and change the symbol of your plot*

*Q3. With the `cars` dataset, plot stopping distance by speed for only those cars with a speed greater than 20.*

```{r}
#| fig.dim: c(3.5,4)
#| out.width: "40%"
#| out.height: "40%"
plot(dist ~ speed, data=cars[cars$speed > 20, ], 
     xlab="Speed (mph)", ylab="Distance (ft)", pch=2)
```

### Boxplots

Box plots are used to summarise a continuous variable by levels of a factor. We will use the `mtcars` dataset to illustrate this.

Explore the dataframe using the strategies covered above. Which variables are categorical? Which are continuous?

```{r}
head(mtcars, 2)
```

```{r}
#| label: mplcyl-plot
#| fig.cap: "Boxplot showing miles per litre vs. number of carburetors"
#| fig.dim: c(4,4)
#| echo: FALSE
mtcars2 <- mtcars
mtcars2$mpl <- mtcars2$mpg / 4.5
boxplot(mpl ~ cyl, data=mtcars2, xlab="Cylinders", ylab="Miles per litre")
```

```{r}
#| eval: FALSE
?boxplot
# See examples at bottom of the help page
# Produce a box plot with axes labels and a title
```

Reproduce the plot shown in Fig. \@ref(fig:mplcyl-plot) (assume 1 gallon = 4.5 L). Remember you can learn about these data with `?mtcars`. You will need to generate a new variable (miles per litre) and label your box plot appropriately. You can limit the extent of the y-axis by adding the argument `ylim=c(a, b)` where `a` and `b` are the limits you want (e.g., `ylim=c(0, 100)`).

*Q4. Use `?boxplot` to investigate what the box and whiskers in the box plot actually represent. Check you can reproduce the upper and lower adjacent values manually (see Chapter* \@ref(appendix)*)*

Note that box plots are not the most visually intuitive. Packages like `ggplot2` (and extensions) make alternatives like those in Fig. \@ref(fig:mplcyl-ggplot) simple to produce. We will cover some of these later.

```{r}
#| label: mplcyl-ggplot
#| fig.cap: "Alternatives to boxplots"
#| fig.dim: c(8,4)
#| echo: FALSE
#| message: FALSE
#| out.width: "80%"
library(tidyverse); library(ggdist); library(cowplot)
theme_set(theme_classic())
mtcars2$cyl <- factor(mtcars2$cyl)
mtcars_mns <- mtcars2 |>
  group_by(cyl) |>
  summarise(mpl_mn=mean(mpl),
            mpl_se=sd(mpl)/sqrt(n()))
p_a <- ggplot(mtcars2, aes(cyl, mpl)) + 
  geom_jitter(alpha=0.5, width=0.1, size=0.75) + 
  geom_pointrange(data=mtcars_mns, aes(y=mpl_mn, ymin=mpl_mn-2*mpl_se, ymax=mpl_mn+2*mpl_se)) +
  labs(subtitle="Strip plot with means and standard errors")
p_b <- ggplot(mtcars2, aes(mpl, cyl)) + 
  geom_dots(side="bottom", scale=0.5) +
  stat_halfeye(scale=0.5, .width=c(0.5, 0.95)) + 
  labs(subtitle="Rain cloud plot with median and middle 50, 95%")
plot_grid(p_a, p_b, align="hv", axis="tblr", nrow=1)
```

### Line plots

Line plots are most often seen in time-series plots with time on the x-axis and the response on the y-axis. Line plots typically involve joining points with a line, which indicates that you have made assumptions about the value of the response variable between successive measurements.

We will examine these plots using the dataset `lynx`, which consists of the number of Canadian lynx pelts sold per year between 1821 - 1934. It is a 'classic' dataset as it shows a cyclical 'boom-and-bust' lynx population (demonstrating predator-prey interactions).

First, we will create a variable `Year`.

```{r}
str(lynx)
```

```{r}
#| results: "hide"
class(lynx) # ts = time-series
lynx2 <- as.data.frame(lynx) 
class(lynx2) 
head(lynx2, 2) 
lynx2$Year <- seq(from=1821, to=1934, by=1)
```

In R, we use *functions* to perform actions on *objects*. Functions have arguments, taking the form `functionName(arg1=..., arg2=...)`. If you do not name the arguments, the function will assume that you are listing the arguments in order. See the help file for a function with `?` to see the argument order (e.g., `?seq`).

*Q5. Using `seq()`, write a piece of code which generates odd numbers between 1 and 20 with and without specifying `by`.*

```{r}
#| results: "hide"
# change the name of the 1st column to 'Trappings'
names(lynx2)[1] <- "Trappings"
str(lynx2)
lynx2$Trappings <- as.numeric(lynx2$Trappings) # Time-Series is complicated.
str(lynx2)
```

Use `?plot` to investigate options for plotting. Find the `type=` argument for plotting both the points and a connecting line. This might be the best option in this case. Why?

*Q6. Using the R plot function, produce a line plot of the Trappings data, as per Figure* \@ref(fig:lynx-plot)*.*

```{r}
#| label: lynx-plot
#| fig.cap: "The number of lynx trapped in Canada (1820-1934)"
#| echo: FALSE
#| fig.align: 'center'
plot(Trappings ~ Year, data=lynx2,
     main="Lynx trapping in Canada (1820-1934)", type="l")
```

Change the plot to show only the years up to 1900, then plot the Trappings on the log scale.

### Histograms

Histograms are used to illustrate the distribution of **continuous** data. In histograms the bars are adjacent (no gap) and this indicates that there is a continuum (i.e. that the data are not discrete).

```{r}
#| label: lynx-hist-default
#| fig.cap: "Lynx pelts per year with default settings."
#| fig.dim: c(4,3)
# this gives very different information.
hist(lynx2$Trappings, main="Lynx trapping", xlab="Trapped lynx per year")
```

Which range of values was most common across years?

Be aware that histograms can be quite sensitive to the bins that you use.

```{r}
#| label: lynx-hist-b1
#| fig.cap: "Lynx pelts per year with breaks=5 (left) and a vector of breaks (right)."
#| fig.dim: c(6,3)
#| out.width: "80%"
#| out.height: "80%"
par(mfrow=c(1,2)) # panels for the plotting window
# R takes the number of breaks as a suggestion
hist(lynx2$Trappings, main="Lynx trapping", xlab="Trapped lynx per year",
     breaks=5)
# this forces R to plot according to the defined breaks
hist(lynx2$Trappings,
     main="Lynx trapping", xlab="Trapped lynx per year",
     breaks=c(0, 500, 1000, 2000, 5000, 10000))
```

```{r}
#| label: lynx-hist-panels
#| fig.dim: c(6,4)
#| out.width: "80%"
#| out.height: "80%"
par(mfrow=c(2, 2)) # plot panels (2 rows x 2 columns)
par(mar=rep(2, 4)) # change the plot margins
hist(lynx2$Trappings, main="bin width: 100", xlab="Trapped lynx per year", 
     breaks=seq(from=0, to=10000, by=100))
hist(lynx2$Trappings, main="bin width: 500", xlab="Trapped lynx per year", 
     breaks=seq(from=0, to=10000, by=500))
hist(lynx2$Trappings, main="bin width: 1000", xlab="Trapped lynx per year", 
     breaks=seq(from=0, to=10000, by=1000))
hist(lynx2$Trappings, main="bin width: 2000", xlab="Trapped lynx per year", 
     breaks=seq(from=0, to=10000, by=2000))
```

```{r}
par(mfrow=c(1, 1)) # reset the par setting.
```

Which of these plots is the most useful? There is no definitive answer to this, but the first is very busy and the last fails to show relevant detail near 0. Setting the bin width to 500 or 1000 communicates the patterns in the data most clearly.

As a general guideline, 5-15 breaks usually work well in a histogram.

### Bar graphs

When you create a `data.frame` it defaults to naming the rows 1...n, where n is the number of rows. You may occasionally come across a `data.frame` with row names. Converting between data types may lose this information. Consequently, **it is better practice to store relevant information in a column**.

Bar graphs are used to plot counts of categorical or discrete variables. We'll be using the `islands` dataset (which has data stored as row names).

Working with data involves a lot of time spent tidying the datasets: cleaning, checking, and reshaping into useful formats. We will cover a more modern set of methods for this later in the course using the *tidyverse* package. For now, we'll stay with base R. First, we need to tidy the `islands` data.

```{r}
#| results: "hide"
str(islands) 
class(islands) # this is a named numeric vector
head(islands)

# convert to a dataframe
islands.df <- as.data.frame(islands) 
head(islands.df, 2)
```

```{r}
# put the row names into a new column
islands.df$LandMass <- row.names(islands.df) 
head(islands.df, 2)

# set row names to the row number
row.names(islands.df) <- 1:nrow(islands.df) 
names(islands.df)[1] <- "Area" 
head(islands.df, 2) 

# reorder by area
islands.df <- islands.df[order(islands.df$Area, decreasing=TRUE), ]
head(islands.df, 3)
```

We can use the function `barplot()` to plot the vector of island areas.

```{r}
#| label: island-1
#| fig.cap: "Island areas with barplot defaults"
#| fig.dim: c(6,2.5)
#| out.width: "80%"
#| out.height: "80%"
par(mar=c(4, 0, 0, 0)) # change the margin sizes
barplot(islands.df$Area)
```

The whole dataset includes a lot of very small areas, so let's cut it down to just the 10 largest. Since the dataset is already sorted, we can take rows `1:10`.

```{r}
#| label: island-2
#| fig.cap: "Top 10 island areas"
#| fig.dim: c(4,4)
barplot(islands.df$Area[1:10])
```

And the next step is to add some names to the x-axis...

```{r}
#| label: island-3
#| fig.cap: "Top 10 island areas with names"
#| fig.dim: c(4,4)
barplot(islands.df$Area[1:10], names=islands.df$LandMass[1:10])
```

Which of course are unreadable. The `las` argument (`?par`) controls how the axis labels relate to the axis line, so we can try adjusting that...

```{r}
#| label: island-4
#| fig.cap: "Top 10 island areas with names rotated"
#| fig.dim: c(4,4)
barplot(islands.df$Area[1:10], names=islands.df$LandMass[1:10], las=3)
```

Maybe we just need to make the bars horizontal. To do this, we should adjust the margins again with `par(mar=...))`, set `horiz=TRUE`, and `las=1`, and use `[10:1]` so the largest is on top.

```{r}
#| label: island-5
#| fig.cap: "Finally! Did you know Antarctica is bigger than Europe?"
#| fig.dim: c(4,3)
par(mar=c(4, 10, 0, 0))
barplot(islands.df$Area[10:1], names=islands.df$LandMass[10:1], 
        horiz=TRUE, las=1, xlab="Area (km2)")
```

As you may have noticed, visualization is an iterative process with lots of trial and error until you find a plot that communicates the message within the data well. There are several packages (e.g., `ggplot2`) that make these sort of adjustments and explorations less opaque than all of the options in `par()`.

------------------------------------------------------------------------

## Summary statistics

You will often need to summarise your data before you present it. Data summaries are usually contained in tables and they can sometimes replace graphics (e.g., where the data is relatively simple or where individual precise values are important). There are many types of summary statistics. Here we are concerned with central tendency and variability.

*Q8. What are the three main measures of central tendency?*

*Q9. What are three measures of variability?*

Measures of central tendency and variability each have pros and cons and you need to be able to apply the most appropriate method to your data. Another summary statistic that you might include is sample size. R is very good at producing summary statistics, and there are myriad ways to produce them. We'll return to the `cars2` dataset.

```{r}
summary(cars2) 
```

```{r}
#| eval: FALSE
summary(cars2[cars2$speed_mph > 20, ]) 
```

```{r}
#| eval: FALSE
# There are several ways to access a column in a dataframe
summary(cars2$speed_mph) 
summary(cars2[, 2])
summary(cars2[, "speed_mph"])
summary(cars2[, c("speed_mph", "dist_ft")])
```

Often you'll wish to summarise your data across levels of a certain factor. For example, levels of a certain treatment that you are applying. More complex summaries can be made using the `dplyr` package. We'll go into more detail later on some of the very powerful ways this package (and its friends in the *tidyverse*) can be used.

First, you'll need to install it. The *tidyverse* is a collection of packages. Install all of them with `install.packages("tidyverse")` (see Section \@ref(R_intro)).

We'll use the built-in dataset `InsectSprays`. Viewing your raw data can be an important check as well. You can open a spreadsheet-style viewer in R using `View(YourDataFrame)`.

```{r}
#| message: FALSE
#| warning: FALSE
library(tidyverse) 
```

```{r}
str(InsectSprays)
glimpse(InsectSprays) # glimpse() is loaded with tidyverse
```

```{r}
#| eval: FALSE
# spray is the categorical predictor; count is the response
View(InsectSprays)
```

To do more complex summaries, we're going to string together a series of functions. This can be done in a nested format (e.g., `fun1(fun2(fun3(dataset)))`), but this gets unwieldy very quickly.

So, let's use the *pipe* operator `|>`. This takes the output from one function and feeds it as the first input of the next (e.g., `dataset |> fun3() |> fun2() |> fun1()`), making code much more legible. Many functions in the *tidyverse* are built for piping.

```{r}
#| eval: FALSE
?`|>`
```

```{r}
#| results: "hide"
# use group_by() with the grouping column name(s)
spray_summaries <- InsectSprays |>
  group_by(spray) |>
  summarise(count_mean=mean(count))
spray_summaries
```

```{r}
# it is very easy to calculate any number of summary statistics
InsectSprays |>
  group_by(spray) |>
  summarise(mean=mean(count) |> round(2),
            median=median(count),
            max=max(count),
            sd=sd(count) |> round(2),
            N=n(),
            N_over_10=sum(count > 10))
```

### Choosing a measure of central tendency

The choice of which measure of central tendency to use depends on the nature of the data and objectives of your research. We will use datasets that you downloaded from Brightspace (Practicals \> data). Remember to put these into the *data* folder in your working directory (or modify the file paths in the code accordingly).

```{r}
library(readxl) # installed with tidyverse, but not loaded in library(tidyverse)
# this will load the 'Scallop %fat' data sheet from the xlsx spreadsheet.
scallop_df <- read_excel("data/practical_1.xlsx", sheet="Scallop %fat")
str(scallop_df)
# avoid spaces and symbols in column names. It's a pain.
names(scallop_df) <- "fat_pct"
```

*Q10. Check the data using the methods above. Does it look OK to you?*

*Q11. Are these data likely to be continuous or discontinuous?*

*Q12. Create a plot to visualize the distribution of these data.*

*Q13. Do you spot any issues?*

```{r}
#| fig.dim: c(4,4)
#| out.width: "30%"
#| out.height: "30%"
hist(scallop_df$fat_pct, main=NULL) # (what does 'main=NULL' do?)
```

You should have spotted a potential outlier. Data entry errors are very common, and a check against the original data sheet shows that the decimal was typed in the wrong place. The following code helps you ID which data entry is in error. We can now search for the 'odd' observation i.e. determine in which row the outlier is located.

```{r}
which(scallop_df$fat_pct > 50) 
scallop_df$fat_pct[35:37] # row 36 is 99, but should be 9.9
scallop_df <- scallop_df[, c(1, 1)] # duplicate column
names(scallop_df) <- c("fat_pct_orig", "fat_pct_corr")
head(scallop_df, 2)
```

```{r}
#| results: "hide"
# there are many ways to 'fix' the outlier in R.
# You need to correct the outlier in row 36 of column 'fat_pct_corr'
scallop_df$fat_pct_corr[36] <- 9.9
which(scallop_df$fat_pct_corr > 90) 
# integer(0) - this means that no elements in fat_pct_corr contain values >90
```

Now summarise `scallop_df` using some of the methods above.

*Q14. Create a histogram for the corrected column. How does it differ from the original column with the error?*

*Q15. Calculate mean, variance, median, interquartile range, minimum, maximum and range for both fat_pct_orig and fat_pct_corr.*

*Q16. Suppose the outlier was even bigger (i.e. you typo was even worse). Adjust your data, multiplying the erroneous data item by 10; copy the '\_orig' column and change row 36 in that column to 999.*

*Q17. Calculate the same summary statistics.*

*Q18. Which measures of central tendency and variability are most 'robust' against this outlier?*

Or look individually instead of calculating many metrics at once with `dplyr` functions:

```{r}
summary(scallop_df$fat_pct_corr)
var(scallop_df$fat_pct_corr)
IQR(scallop_df$fat_pct_corr)
```

R is excellent at generating well formatted tables such as shown in Table \@ref(tab:scallop-table). What is missing from from Table \@ref(tab:scallop-table)?

```{r}
#| label: scallop-table
#| echo: FALSE
scallop_df |>
  pivot_longer(1:2, names_to="Column", values_to="val") |>
  group_by(Column) |>
  summarise(Mean=mean(val) |> signif(3),
            Median=median(val) |> signif(3),
            `Standard deviation`=sd(val) |> signif(3),
            Range=range(val) |> diff() |> signif(3),
            `Interquartile range`=IQR(val) |> signif(3)) |>
  knitr::kable(
    caption='Summary statistics with and without an outlier. Note which summary stats are most influenced by the outlier.',
    booktabs=TRUE, 
    digits=3)
```

*Q19. How would the patterns seen in Table* \@ref(tab:scallop-table) *influence your choice if you were required to summarise data that you thought might contain data that you weren't sure about? The three measure of central tendency are influenced to different extents by the 'shape' of the data they are used to describe.*

```{r}
hake_df <- read_excel("data/practical_1.xlsx", sheet="Hake")
str(hake_df) # once again, column names made for excel rather than R
```

*Q20. What type of variable is `length`?*

*Q21. Select an appropriate graphical method and display these data.*

*Q22. In your own time, use the `dplyr` functions to summarise the hake data by year.*

```{r}
hake_df$Year <- as.factor(hake_df$Year) # Treat as categorical, not numeric
names(hake_df) <- c("Year", "Length") # simplify the column names
```

```{r}
#| label: hake-table
#| echo: FALSE
hake_df |>
  group_by(Year) |>
  summarise(`Mean length (cm)`=mean(Length)) |>
  knitr::kable(
    caption='Summary of hake data.',
    booktabs=TRUE,
    digits=1)
```

Try to re-create Table \@ref(tab:hake-table).

The following 'settling velocity' data relates to the settling velocity of salmon faecal material. Shona Magill generated these data.

```{r}
library(readxl)
fishPoo_df <- read_excel("data/practical_1.xlsx", sheet="Settling velocity")
str(fishPoo_df)
```

*Q23. Produce a histogram of the settling velocity. Is it left or right skewed?*

*Q24. Which measures of central tendency and variability are most appropriate?*

*Q25. Sketch the distribution and indicate the relative positions of the mean and median.*

*Q26. Generate a new column of the log-transformed settling velocity data and plot these data.*

*Q27. What measures of central tendency and variability could be applied to the log-transformed data? Selecting the preferable measure of central tendency and variability in a dataset is not necessarily straightforward.*

Table 1.3 gives some indication of what issues you might consider.

```{r}
#| label: metric-table
#| echo: FALSE
tibble(a=c("Continuous, unimodal, symmetric", "Continuous, skewed", 
           "Continuous, multimodal", "Discontinuous"),
       b=c("Mean", "Median", 
           "None; state modes", "None; data-dependent"),
       c=c("Variance or sd", "Interquartile range", 
           "None; summarise by group", "Range?")) |>
  mutate(across(where(is.character), ~str_wrap(.x, 20))) |>
  rename(`Data distribution`=a, 
         `Central tendency metric`=b,
         `Variability metric`=c) |>
  knitr::kable(
    caption="Appropriate measures of central tendency and variability according to the underlying data distribution.",
    booktabs=T
  ) 
```

------------------------------------------------------------------------

## Conclusions

Visualizing and summarising data are the critical first steps in the data analysis and reporting workflow. We use graphical methods to firstly explore our own data. Once we have made sense of it we select the most appropriate method to convey that understanding to our readers. We may help that communication by summarising data in the most appropriate way taking into account the distribution of the data and the presence of outliers.
