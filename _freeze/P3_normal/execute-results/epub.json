{
  "hash": "0f6a388661ca86a8618782fa0d8d49d9",
  "result": {
    "engine": "knitr",
    "markdown": "# Normal distribution {#sec-P3}\n\nStatistical inference is the process by which we infer from a sample statistic to the population. We need to infer from samples to populations because it is usually impossible to measure the entire population. Inference is the process of estimating population parameters from a sample.\n\nIn order to infer from samples to populations we need to understand how the statistics we generate from our samples are likely to ‘behave’. To do this we use theoretical distributions.\n\n::: Q.\nWhich two theoretical distributions have we already covered?\n:::\n\nThe normal (a.k.a., Gaussian) distribution is a theoretical distribution that is central to inferential statistics. If your data (or, more accurately, statistics derived from you data) are reasonably approximated by the normal distribution then you will be able to use a wide range of techniques to deal with it.\n\nIn this practical we will examine the normal distribution, calculate Z scores, and interpret those Z scores. We’ll assess whether data are reasonably assumed to be normally distributed, transforming the data where they are not. We’ll apply the CLT and evaluate how well other distributions are approximated by the normal distribution.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nset.seed(2025)\n```\n:::\n\n\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n## Using the normal distribution\n\nThe length of a catch of herring was measured. Five hundred individuals were studied. We will consider this group to be the entire population of interest. The population parameters are: $\\mu = 37.6 cm$ and $\\sigma = 1.20 cm$.\n\n::: Q.\nWhat are the theoretical limits of the normal distribution?\n:::\n\n::: Q.\nDo you think normality a fair assumption for these data?\n:::\n\n::: Q.\nAssume herring length is approximately normally distributed and write down the model which describes the fish length distribution: $y_i \\sim Norm(\\mu, \\sigma)$.\n:::\n\nIf we know the population parameters, we can calculate Z scores for individuals (or groups of individuals) from that population and calculate how unusual they are. For the moment, we are interested in determining what proportion of fish from this population are expected to be \\< 38 cm.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmu <- 37.6\nsigma <- 1.2\ny <- 38\n\ny_lt38_df <- tibble(len=seq(mu-3*sigma, mu+3*sigma, length.out=1e3),\n                     density=dnorm(len, mu, sigma),\n                     shade=len < 38)\nggplot(y_lt38_df, aes(len, ymin=0, ymax=density, fill=shade)) + \n  geom_ribbon(colour=\"grey30\") + \n  scale_fill_manual(values=c(\"white\", \"red3\"), guide=\"none\") +\n  labs(x=\"Herring length (cm)\", y=\"Probability density\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Normal distribution of herring lengths.](P3_normal_files/figure-epub/fig-normal_herring-1.png){#fig-normal_herring}\n:::\n:::\n\n\n\n\n\n\n\n\n### Using R to calculate areas under the normal curve\n\nTo solve this problem using R we use the cumulative probability. Observations at the extreme low end are extremely unlikely, but the probability of observing data increases and reaches a maximum at the mean, after which it declines again. The normal distribution is symmetrical.\n\n::: Q.\nWhat shape is the cumulative probability curve of a normally distributed variable?\n:::\n\nThe R functions for the normal distribution take the same form as those for the binomial and Poisson distributions. To calculate the probability is of observing a fish less than 38 cm, you need to specify the model and select the appropriate distribution function. Use `?dnorm` or `?pnorm` and look at your options (@fig-dpq_examp).\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(cowplot)\nnorm_df <- tibble(x=seq(-4, 4, length.out=1e3),\n                  x2=seq(0, 1, length.out=1e3),\n                  lo=0) |>\n  mutate(d=dnorm(x), \n         q=qnorm(x2),\n         p=pnorm(x),\n         above_m1=x > -1) \nlabels_df <- tribble(~x, ~y, ~label,\n                     -1, dnorm(-1), \"dnorm(-1) = 0.242  \",\n                     -1, 0, \" qnorm(0.158) = -1\")\np1 <- ggplot(norm_df, aes(x)) + \n  geom_ribbon(aes(ymin=lo, ymax=d, fill=above_m1)) +\n  geom_line(aes(y=d)) + \n  geom_point(data=labels_df, aes(y=y), size=2) +\n  geom_text(data=labels_df[1,], aes(y=y, label=label), hjust=1, size=3) + \n  geom_text(data=labels_df[2,], aes(y=y, label=label), size=3,\n            hjust=0, vjust=0, nudge_y=0.01) + \n  scale_fill_manual(\"\", values=c(\"red3\", \"white\"), \n                    labels=c(\"pnorm(-1) = 0.158\", \"\")) +\n  labs(x=\"Value\", y=\"Density\") +\n  theme_classic() +\n  theme(legend.position=c(0.8, 0.8),\n        legend.text=element_text(size=8),\n        legend.key.size=unit(0.3, \"cm\"))\np2 <- ggplot(norm_df, aes(x, d)) + geom_line() + \n  labs(x=\"Value\", y=\"dnorm(Value, 0, 1)\") + \n  theme_classic() \np3 <- ggplot(norm_df, aes(x, p)) + geom_line() + \n  labs(x=\"Value\", y=\"pnorm(Value, 0, 1)\") + \n  theme_classic() \np4 <- ggplot(norm_df, aes(x2, q)) + geom_line() + \n  labs(x=\"Probability\", y=\"qnorm(Probability, 0, 1)\") + \n  theme_classic() \n\nplot_grid(p1, p2, p3, p4, align=\"hv\", axis=\"tblr\")\n```\n\n::: {.cell-output-display}\n![Functions for distributions in R illustrated with a standard normal.](P3_normal_files/figure-epub/fig-dpq_examp-1.png){#fig-dpq_examp}\n:::\n:::\n\n\n\n\n\n\n\n\nTo calculate $P(y_i < 38 | \\mu = 37.6, \\sigma = 1.2)$, we use `pnorm()`, which gives the cumulative probability from `-Inf` to the value we choose.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cumulative probability: which bit of the curve does this relate to?\npnorm(q = 38, mean = 37.6, sd = 1.2) \n```\n:::\n\n\n\n\n\n\n\n\n::: Q.\nWhat is the probability of a randomly selected fish from this population being less than 35 cm?\n:::\n\n::: Q.\nWhat proportion of individuals are greater than 39 cm?\n:::\n\nWe can plot any normal distribution we like. Play with the values for `mu` and `sigma` below, adjusting the values for `from` and `to` as needed to see the distribution.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- 37.6  # population mean\nsigma <- 1.2  # population sd\ncurve(dnorm(x, mean = mu, sd = sigma), from = 30, to = 45,\n      main = \"Normal density\", ylab = \"Density\", xlab = \"Fish length, cm\")\n```\n:::\n\n\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n## Normal model adequacy\n\nIn a population of long-eared wrasse, we calculate $\\mu$ and $\\sigma$ and find $y \\sim Norm(15.2 cm, 25.1 cm)$.\n\n::: Q.\nWhat is the mean length and standard deviation of long-eared wrasse?\n:::\n\n::: Q.\nCalculate the proportion of fish that are expected to be less than zero cm in length.\n:::\n\n::: Q.\nWhat do your results indicate about the adequacy of the normal model to describe the length distribution of long-eared wrasse?\n:::\n\n::: Q.\nWould your conclusions change if $\\sigma$ were smaller? For example: $y \\sim Norm(15.2, 2.51)$.\n:::\n\n------------------------------------------------------------------------\n\n## Quantiles\n\nA quantile is a value that divides a frequency distribution (i.e. a set of numbers) into equally represented groups (i.e., the same number of observations per group). For example, there are three values (Q1, Q2, Q3) that split a normal distribution into four groups (negative infinity to Q1, Q1 to Q2 (median), Q2 to Q3, and Q3 to positive infinity). Q3 - Q1 is the middle 50% of the data: the interquartile range.\n\nThere are 99 quantiles, called percentiles, that split your data into 100 groups. The 2.5 percentile is the value that splits your data into two groups corresponding to 2.5% along the distribution (from negative infinity for the normal distribution). Just as we can ask what proportion of a distribution is above or below a set value, we can also ask between which values will a given percentage of my data lie (e.g. what values correspond to the middle 95%?).\n\nTo solve this problem using R we use the quantile function `qnorm()`, speciying the cumulative probability as an argument. To find the value that splits the upper 2.5%:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(p = 0.975, mean = 37.6, sd = 1.2) # p is the cumulative probability \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39.95196\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- seq(from = 0.1, to = 0.9, by = 0.2) # vectorize for multiple values\nrbind(probs, quantiles=qnorm(p = probs, mean = 37.6, sd = 1.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              [,1]     [,2] [,3]     [,4]     [,5]\nprobs      0.10000  0.30000  0.5  0.70000  0.90000\nquantiles 36.06214 36.97072 37.6 38.22928 39.13786\n```\n\n\n:::\n\n```{.r .cell-code}\nqnorm(p = c(0.025, 0.975), mean = 37.6, sd = 1.2) # middle 95%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35.24804 39.95196\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThese values (`c(0.025, 0.975)`) identify proportions of the cumulative curve. That is, they identify the bottom 2.5% and the bottom 97.5% of the curve. Values between these two parameters constitute the central 95% of your data.\n\nWe quote our mean and interval like this:\n\n> The mean fish length and 95% interval was 37.6 cm (35.2 – 40.0 cm).\n\nRemember to use to same degree of precision (significant figures) for confidence intervals as was used to gather the data (or as specified in the question, defaulting to three).\n\n::: Q.\nFind the values that capture the middle 90% of the herring data, where $y \\sim Norm(37.6, 1.2)$.\n:::\n\n::: Q.\nWhat value would you expect to correspond to `p=0.5`?\n:::\n\n::: Q.\nCheck your answer above using `qnorm()`.\n:::\n\nTo visualize regions of the normal distribution, let's use *ggplot*.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- 37.6\nsigma <- 1.2\nlb <- 36 # lower boundary \nub <- 40 # upper boundary\n\nnorm_df <- tibble(z=seq(-4, 4, length.out=1000),\n                  x=z*sigma + mu,\n                  densNorm=dnorm(x, mu, sigma))\nggplot(norm_df, aes(x, densNorm)) + \n  geom_line() + \n  geom_ribbon(data=norm_df |> filter(x > lb & x < ub),\n              aes(ymin=0, ymax=densNorm), fill=\"steelblue\") +\n  labs(x=\"Herring length (cm)\", \n       y=\"Probability density\",\n       subtitle=paste0(\"P(\", lb, \"< y <\", ub, \") = \", \n                       signif(pnorm(ub, mu, sigma) - pnorm(lb, mu, sigma), digits=3))) +\n  theme_classic() # changes how the plot looks: ?theme\n```\n\n::: {.cell-output-display}\n![The normal distribution illustrating probability of a random herring from your population having a length between 36 and 40 cm.](P3_normal_files/figure-epub/fig-herring2-1.png){#fig-herring2}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nChange the parameters in code the above to check that the values you generated for the 95% interval correspond when plugged into `lb` and `ub` in the code.\n:::\n\n::: Q.\nWhat is the difference between $< x$ and $\\leq x$ when applied to continuous data?\n:::\n\n::: Q.\nDoes this also apply to discontinuous data?\n:::\n\nHow would you expect these intervals to change when the population standard deviation $\\sigma$ changes?\n\n::: Q.\nWith everything else equal, try doubling, quadrupling, and halving the standard deviation on the herring data then re-running the same code.\n:::\n\n::: Q.\nDoes the change in the 95% interval match your expectations?\n:::\n\nA z score gives the number of standard deviations away from the mean for any value. The z score is the value from the standard normal distribution ($\\mu = 0, \\sigma = 1$) that corresponds with the same quantile of the original distribution. The values for any normal distribution can be converted to Z scores by subtracting $\\mu$ and dividing by $\\sigma$, such that $z_i = \\frac{y_i - \\mu}{\\sigma}$\n\n::: Q.\nWith $y \\sim Norm(37.6, 1.2)$, what is the z score for an individual of length 34?\n:::\n\n------------------------------------------------------------------------\n\n## Testing for normality\n\nMany statistical tests assume that data are reasonably approximated by a normal distribution and have homogeneous variance. R can be used to formally test the assumption that data are normally distributed, though you should have some idea of whether this is likely through consideration of the data source. This applies particularly where you have a small sample size which makes evaluating the distribution challenging.\n\nThe data you collect will be part of a population. The normality check assesses the viability of the assumption that the data you collected were drawn from a population that was normally distributed. Note that populations are, in practice, *never* actually normally distributed. Your test is to assess how *reasonable* the assumption of normality is.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMS <- read_xlsx(\"data/H2DS_practicalData.xlsx\", sheet = \"Mood shrimp\")\n# check the data using head(), str() etc.\n```\n:::\n\n\n\n\n\n\n\n\nWe'll learn more advanced methods in @sec-P5 for the analyses we cover, but we can plot how well our observations follow the expectations of a normal distribution with `qqnorm()` and add a line of perfect fit with `qqline()`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nqqnorm(MS$Shrimp1)\nqqline(MS$Shrimp1, col = 2) # the data fall near the line\nqqnorm(MS$Shrimp2)\nqqline(MS$Shrimp2, col = 2) # the data deviate widely from the line\n```\n\n::: {.cell-output-display}\n![Normal (QQ) plots. The left indicates that the normality assumption might be reasonable, not so on the right.](P3_normal_files/figure-epub/qq-examp-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\nThe axes are automatically scaled so that perfectly normally distributed data would fall on a diagonal line. Any deviation from the red straight line indicates a lack of normality. What we need to assess is how serious any deviation is and whether it is sufficient to indicate that the assumption of normality is not ‘reasonable’. This is a subjective decision, and two different statisticians may tell you different answers about the same data.\n\nAs sample size decreases, it becomes increasingly difficult to see if your data are reasonably approximated by a normal distribution. There are many formal statistical methods for assessing normality, but as we already know, it is impossible for a population to be distributed perfectly normally and this means such tests are largely redundant. You must assess the assumptions of your model, but you should be aware that all data fails the assumptions. The question is whether the assumptions are *reasonably* well met such that the model results can be useful. There is sadly no hard rule as to what constitutes ‘reasonable’!\n\n::: Q.\nMake histograms of both `Shrimp1` and `Shrimp2`. Comment on their apparent distributions.\n:::\n\n------------------------------------------------------------------------\n\n## Data transformations {#transformations}\n\nThe assumption that our sample data are drawn from a normally distributed population is central to the use of many important inferential statistical techniques. However, frequently it is *not* reasonable to assume that data are approximately normally distributed.\n\nA common issue is that our measurements are logically bounded. For example, chemical concentrations (e.g., zinc in sediments) cannot be negative. Similarly, you cannot have negative lengths, time, mass, etc. Proportions must be between 0 and 1. Where data is collected ‘near’ a logical boundary they are often not normally distributed, since the normal distribution predicts ‘tails’ which are impossible.\n\nOne solution is to use a mathematical transformation to convert your data to something that is reasonably approximated by a normal distribution even if it is not well approximated in the original measurement units. Often transformations will also correct unequal variances (see @sec-P5) in addition to non-normality so they are very useful. The appropriate transformation depends on the data.\n\nCommon transformations include:\n\n-   Log (`log(x)`, inverse: `exp(x)`): When the distribution is skewed right and all values are \\>0. Often ‘cures’ heteroscedasticity. Commonly used for observations spanning orders of magnitude such as body size.\\\n-   Square-root (`sqrt(x)`, inverse: `x^2`): When the measurements are areas (e.g., leaf areas). Often used to ‘down-weight’ common species (e.g., @sec-P6), which is unrelated to model assumptions. Values must be \\>0.\\\n-   Arcsine (`asin(x)`, inverse: `sin(x)^2`): When the measurements are proportions. Tends to stretch out the tails (e.g., near 0 or 1 for proportions) and squash the middle (e.g., near 0.5).\\\n-   Logit (`boot::logit(x)`, inverse: `boot::inv.logit(x)`): Used for proportions excluding 0 and 1. Also commonly used in models with a binary (Bernoulli) response variable (e.g., survival probability predicted by temperature, where the response variable is 'alive'/'dead').\n-   Reciprocal (`1/x`, inverse: `1/x`): When the distribution is skewed right. Generally 'stronger' than a log transformation and often has logically interpretable units ($m~s^{-1}$, $s~m^{-1}$).\\\n\nIdentifying the correct transformation can be led by an underlying comprehension of the nature of the data. However, this often doesn’t work so expect some trial and error.\n\nWe can visualize the relationship between data on the original scale and the transformed values. For values bounded by 0:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npositive_df <- tibble(orig=seq(0.01, 10, length.out=1e3)) |>\n  mutate(sqrt=sqrt(orig),\n         ln=log(orig),\n         reciprocal=1/orig,\n         squared=orig^2) |>\n  pivot_longer(cols=2:5, names_to=\"transformation\", values_to=\"new_value\")\n\nggplot(positive_df, aes(orig, new_value)) + \n  geom_line() + \n  facet_wrap(~transformation, scales=\"free\", nrow=1) + \n  labs(main=\"Positive values\", x=\"Original value\", y=\"Transformed value\")\n```\n\n::: {.cell-output-display}\n![Effect of common transformations on positive data.](P3_normal_files/figure-epub/fig-trans_positiveData-1.png){#fig-trans_positiveData}\n:::\n:::\n\n\n\n\n\n\n\n\nAnd for proportions:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nproportion_df <- tibble(orig=seq(0.01, 0.99, length.out=1e3)) |>\n  mutate(sqrt=sqrt(orig),\n         ln=log(orig),\n         asin=asin(orig),\n         logit=boot::logit(orig)) |>\n  pivot_longer(cols=2:5, names_to=\"transformation\", values_to=\"new_value\")\n\nggplot(proportion_df, aes(orig, new_value)) + \n  geom_line() + \n  facet_wrap(~transformation, scales=\"free\", nrow=1) + \n  labs(main=\"Proportions\", x=\"Original value\", y=\"Transformed value\")\n```\n\n::: {.cell-output-display}\n![Effect of common transformations on proportions.](P3_normal_files/figure-epub/fig-trans_proportionalData-1.png){#fig-trans_proportionalData}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nUsing the Radon concentration worksheet, plot the data. Do they look normally distributed?\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get the Radon data into R; the units are parts per billion\nreadxl::excel_sheets(\"data/H2DS_practicalData.xlsx\")\n```\n:::\n\n\n\n\n\n\n\n\n::: Q.\nIn your own time, use R to determine the log, square root, and reciprocals (and combinations of all of them: at least one converts the data to approximate normality).\n:::\n\n------------------------------------------------------------------------\n\n## The Central Limit Theorem\n\nThe central limit theorem (CLT) is about how *sample means* are distributed.\n\nThe CLT states that **the means of normally distributed data will, themselves, be normally distributed**.\n\nIn addition, the theorem states that **the means of data which are NOT normally distributed will be normally distributed if the sample size is sufficiently large**. In this practical we are going to demonstrate this theorem using random data generated from various probability distributions.\n\n### The distribution of means from non-normal data\n\nThe following code chunk:\n\n1.  Generates a non-normally distributed dataset (`obs_data`)\n2.  Repeatedly samples from it `num_samples` times, each with `sample_size` observations\n3.  Calculates and stores the sample mean for each `num_samples` repeat\n4.  Plots histograms and QQ-plots for the raw data and for the sample means\n\nRun the following code and then experiment with the `sample_size`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Non-normal data: a mixture of several distributions\nobs_data <- c(rnorm(2000, 200, 20), \n              rnorm(1500, 100, 50),\n              rnorm(1000, 400, 80), \n              rnorm(800, 300, 100))\nxlims <- range(obs_data)\n\n# Define size of each sample and the number of sampling repeats\nsample_size <- 30\nnum_samples <- 3000\n\n# Sample num_samples times from obs_data, with n = sample_size for each sample\nsample_means <- numeric(length=num_samples) # initialize an empty vector\nfor(i in 1:num_samples) {\n  sample_means[i] <- sample(x=obs_data, size=sample_size, replace=T) |> mean()\n}\n\n# plot observed distribution\npar(mfrow = c(2, 2),\n    mar = c(5, 2, 2, 2)) \nhist(obs_data,\n     main = \"Raw data: obs_data\",\n     sub = paste0(\"mean: \", signif(mean(obs_data), 3), \n                  \", sd: \", signif(sd(obs_data), 3)),\n     breaks = 30, xlab = \"Observations\", xlim = xlims\n) \nqqnorm(obs_data, main = \"Raw data QQ\")\nqqline(obs_data)\n\n# plot distribution of sample means\nhist(sample_means,\n     main = paste0(\"Sample means, N: \", sample_size),\n     sub = paste0(\"mean: \", signif(mean(sample_means), 3), \n                  \", sd: \", signif(sd(sample_means), 3)),\n     breaks = 30, xlab = \"Sample means\", xlim = xlims\n) \nqqnorm(sample_means, main=\"Sample mean QQ\")\nqqline(sample_means)\n```\n\n::: {.cell-output-display}\n![The central limit theorem in action.](P3_normal_files/figure-epub/fig-clt_examp-1.png){#fig-clt_examp}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nWhat do you notice about the location of the mean as a function of `sample_size` ($n$)?\n:::\n\n::: Q.\nWhat do you notice about the spread around the mean of sample means as a function of `sample_size`? Why did the pattern you have observed occur?\n:::\n\nThe CLT states that `sample_means` will be normally distributed if (a) the underlying data are normally distributed, OR (b) the `sample_size` is large enough. With R, we can see this in action.\n\nNote also the relationship between the sample size and the range of values for the sample mean. How does the standard deviation of `sample_means` change with changes in `sample_size`?.\n\n------------------------------------------------------------------------\n\n## The standard error of the mean\n\nThe standard error of the mean is the standard deviation of sample means, of a given sample size, taken from a population. It describes the dispersion of sample means ($\\bar{y}$'s) we would expect if we were to repeatedly sample the same population over and over again. It is the standard deviation of the distribution shown in the lower histogram in @fig-clt_examp : `sd(sample_means)`.\n\nUsually we only have a single sample rather than 10,000 (=`num_samples`) as above. In that case, we estimate it from a single sample as ${SE}_{\\bar{y}} = \\frac{sd(y)}{\\sqrt{n}}$, where $y$ is a vector of $n$ observations.\n\nWe can explore this similarly to the simulated sampling we did for the CLT.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a normally distributed population with mean mu and sd sigma\nmu <- 10\nsigma <- 2\nsim_obs <- rnorm(10000, mu, sigma)\n\n# Define sample size and number\nsample_size <- c(3, 10, 30, 100) # size of each sample\nnum_samples <- 3000 # number of samples (=repeats) for each sample size\n\n# Create a dataframe to store results\nsample_mean_df <- tibble(N=rep(sample_size, each = num_samples),\n                         id=rep(1:num_samples, times = length(sample_size))) |>\n  rowwise() |> # enforces new sample() call for each row\n  mutate(sample_mean=mean(sample(x=sim_obs, size=N))) |>\n  ungroup()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean_df |>\n  mutate(N=factor(N, levels=unique(N), labels=paste(\"n:\", unique(N)))) |>\n  ggplot(aes(sample_mean, colour=N)) +\n  geom_vline(xintercept=mu, linetype=3) +\n  geom_density(linewidth=0.9) +\n  scale_colour_viridis_d(\"Size of each sample\", option=\"mako\", end=0.85) +\n  labs(x=paste(\"Means of\", num_samples, \"simulated samples\")) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Simulated sample means. Each curve shows the sampling distribution for means of samples with size n. The standard deviation of each curve is the standard error of the mean.](P3_normal_files/figure-epub/fig-SEM-1.png){#fig-SEM}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nCalculate the theoretical standard error of the mean for each `sample_size` given `sigma` (i.e., from the equation) and compare this with the standard error from the simulated sample means in `sample_mean_df`.\n:::\n\nHint:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean_df |>\n  group_by(N) |>\n  summarise() # what goes here?\n```\n:::\n\n\n\n\n\n\n\n\n::: {.callout-tip collapse=\"true\"}\n## View solution\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean_df |>\n  group_by(N) |>\n  summarise(se_bySim=sd(sample_mean)) |>\n  ungroup() |>\n  mutate(se_byFormula=sigma/sqrt(N))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n      N se_bySim se_byFormula\n  <dbl>    <dbl>        <dbl>\n1     3    1.16         1.15 \n2    10    0.631        0.632\n3    30    0.371        0.365\n4   100    0.194        0.2  \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: Q.\nGiven what you know about the CLT, how does this change with non-normally distributed data? Try creating `sim_obs` using `rpois()`.\n:::\n\n::: Q.\nChange `sigma` and `sample_size` and check that the standard error estimates are in line with their true values (i.e., as determined by using the standard error formula).\n:::\n\n------------------------------------------------------------------------\n\n## Normal approximations\n\nIn @sec-P2 you saw that the Poisson distribution could be used to approximate the binomial distribution. In a conceptually similar way, the normal distribution can be used to approximate both the Poisson distribution and the binomial distribution under certain conditions.\n\nWhile the Poisson and binomial distributions are discrete and the normal distribution continuous, in practice all of our observations are discontinuous, subject to our measurement precision. A recorded mass of 437 g really indicates a mass between 436.5 and 437.5 g. The assumption that a variable is continuous is typically reasonable if there at least \\~30 possible values between the smallest and largest observation. This constraint similarly applies to the approximation of binomial and Poisson distributions with the normal distribution.\n\n::: callout-note\nAll models are wrong, but some are useful! Normality is always approximate in practice.\n:::\n\n### The Normal approximation of the Poisson distribution\n\nRecall that the Poisson model takes only one parameter, $\\lambda$ = mean = variance. So if we have a variable $y \\sim Pois(10)$, the mean (e.g., density per quadrat) is 10 and so is the variance. We now have the two parameters that define the normal distribution.\n\n::: Q.\nFor $y \\sim Pois(10)$ write the equivalent normal distribution: $y \\sim Norm(...)$.\n:::\n\nA normal approximation may be reasonable if $\\lambda \\geq 30$. So $y \\sim Pois(10)$ should not be approximated by the normal distribution while $y \\sim Pois(30)$ could be. As $\\lambda$ gets larger, the Poisson distribution becomes more and more computationally demanding compared to the normal distribution. Many standard statistical analyses also assume normal distributions.\n\nHowever, when feasible, it is generally preferable to use the natural distribution for your data (e.g., a Poisson distribution for counts) through the appropriate GLM. The normal distribution is beneficial in some cases, but this is increasingly less so with modern methods.\n\n::: Q.\nCalculate $P(y_i \\leq 35)$ from $y \\sim Pois(40)$ and compare it to the same probability under the normal approximation of this distribution.\n:::\n\n::: Q.\nEvaluate $P(y_i<3\\ |\\ \\lambda=5)$ using the Poisson model and its normal approximation.\n:::\n\n### The Normal approximation of the binomial distribution\n\nWhen $n$ is large, the binomial distribution tends toward a normal distribution, particularly if $p$ is near 0.5. Roughly speaking, if $np > 5$ and $n(1-p) > 5$, then the normal distribution may be a reasonable approximation. The mean of the binomial distribution is $np$ and the variance is $npq$.\n\nWe might be interested in predicting the number of male and female offspring in turtle clutches. Assume the proportion that are male is 0.5 and clutch size is 40. We've observed several clutches of eggs on a particular island with only 10 males. We might wonder how unlikely this was by chance, assuming $P(male)=0.5$.\n\n::: Q.\nWhat is the probability of observing 10 or fewer males in this scenario?\n:::\n\n::: Q.\nPlot the probability (i.e. from 0 – 40 males) as a bar graph and comment on its shape.\n:::\n\n::: Q.\nCalculate the mean and variance of this population.\n:::\n\n::: Q.\nAre $np$ and $n(1-p)$ both \\> 5?\n:::\n\n::: Q.\nSpecify the normal distribution that approximates this binomial distribution.\n:::\n\n::: Q.\nWhat is $P(y_i \\leq 10)$ for the normal approximation?\n:::\n\n------------------------------------------------------------------------\n\n## Conclusions\n\nThe normal distribution is central to statistics. A huge variety of observations are reasonably approximated by the normal distribution (or can be made to be normally distributed through transformation).\n\nThe normal distribution can be used to assess the likelihood of a given observation, if we know the population mean and standard deviation from which it came. Furthermore, given our knowledge of the population parameters ($\\mu$, $\\sigma$) we can determine values which define intervals on that population. Frequently scientists determine the values that bound 95% of their data.\n\nThe central limit theorem says that sample means taken from a normally distributed population will themselves be normally distributed and, in addition, means of sufficiently large samples will also be normally distributed even where the original data are not normally distributed (the sample size required depends on the extent of the skew in the original data).\n\nThe normal distribution is a good approximation of the Poisson distribution when lambda is large and the binomial model where the number of trials is large and the probability of success around 0.5 (i.e. the distribution of values is not too skewed).\n",
    "supporting": [
      "P3_normal_files\\figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}