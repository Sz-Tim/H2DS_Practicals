{
  "hash": "9adcf86def70af06ece5c151d9e3a204",
  "result": {
    "engine": "knitr",
    "markdown": "# The t-distribution and confidence intervals {#sec-P4}\n\nThe goal of science is to understand the world (universe!). To do that, we want to know the values of population parameters (e.g., the mean size of barnacles on the back-beach, the variance in fail-rate of a machine component, the maximum satellite signal strength per satellite transect, the mean size of a fisher's catch, ...). However, we usually cannot measure entire populations due to logistical/time/money constraints. Instead, we take a (random) sample, and infer from that sample to our population of interest based on our statistical models of the world. This is statistical inference.\n\nUnfortunately, we can never know how well our sample reflects the population.\n\nFor example, our random sample of barnacles might contain (by chance alone) mostly big ones, or barnacles that varied considerably (or negligibly) in size. The t-distribution is similar to the normal distribution, but it accounts for this added uncertainty. This enables us to estimate the probability that a given sample came from a population with any given mean (with caveats). The t-distribution also enables us to put confidence intervals around the mean of our sample, and gives us some idea of the values of the mean that are likely.\n\nThe t-distribution models the probability of making a given observation from a population whose parameters are estimated from your sample. The t statistic is calculated in the same way as the z score for samples, but the expected scores follow the t-distribution (instead of the normal distribution) which accounts for sampling uncertainty. For small sample sizes, we are less confident about the population parameter estimates, and the t-distribution consequently becomes shorter than the normal distribution and with fatter tails (@fig-t_vs_norm). The shape of the t-distribution depends on the *degrees of freedom* ($df = \\nu = N-1$). There is more guidance on the t-distribution, and links to other sources on Brightspace.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nset.seed(2025)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nt_Norm.df <- tibble(\n  x = seq(-6, 6, length.out = 100),\n  Norm = dnorm(x),\n  t01 = dt(x, 1),\n  t03 = dt(x, 3),\n  t30 = dt(x, 30)\n) |>\n  pivot_longer(2:5, names_to = \"distr\", values_to = \"Density\") |>\n  mutate(Distribution = factor(distr,\n    levels = c(\"t01\", \"t03\", \"t30\", \"Norm\"),\n    labels = c(\"t(df=1)\", \"t(df=3)\", \"t(df=30)\", \"Normal(0,1)\")\n  ))\n\nggplot(t_Norm.df, aes(x, Density, colour = Distribution, linetype = Distribution)) +\n  geom_line(linewidth = 1) +\n  scale_colour_manual(values = c(RColorBrewer::brewer.pal(5, \"BuGn\")[2:4], \"black\")) +\n  scale_linetype_manual(values = c(1, 1, 1, 3)) +\n  theme_classic() +\n  theme(legend.position = c(0.85, 0.8))\n```\n\n::: {.cell-output-display}\n![Comparison of different t-distributions (note that at df=30, the distribution is nearly identical to the normal distribution).](P4_t_CIs_files/figure-epub/fig-t_vs_norm-1.png){#fig-t_vs_norm}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nWould you feel as confident about basing an estimate of the heights of the lab population on a sample of 2 compared with a sample of 50?\n:::\n\n::: Q.\nHow does sample size affect the reliability of our population parameter estimates?\n:::\n\n## Single sample t-tests\n\nThe single sample t-test is analogous to the calculation of z scores. It enables us to determine how unlikely our sample mean is, given any hypothesized mean. However, before using any parametric tests such as t-tests, we need to assure ourselves that the model assumptions are reasonably met.\n\nImagine we are fisheries inspectors and have sampled the cod landed by a fishing boat. We know that the mean size of the landed cod should be greater than 36.6 cm. We need to assess how likely it is that our sampled cod come from a population where $\\mu \\geq 36.6 cm$. We are testing the hypothesis that there is one 'population' of legally landed cod, and these cod are a part of that population. We use the t-distribution to assess the probability that our sample was drawn from a legally-landed cod population. If this probability is low then we might speculate that the cod are, in fact, drawn from a different population (i.e., that the boat is using illegal gear).\n\nWe do not know the mean $\\mu$ or standard deviation $\\sigma$ of the population and hence cannot use a normal distribution to model the likelihood of observing any particular value.\n\nFirst, we should clearly state our hypotheses:\n\n**H~0~ (the null hypothesis):** The population mean of the cod on this boat is greater than or equal to 36.6 cm ($\\mu \\geq 36.6 cm$).\n\n**H~1~ (the alternative hypothesis):** The population mean of the cod on this boat is less than 36.6 cm ($\\mu < 36.6 cm$).\n\nWe use a t-test to calculate the probability of drawing our sample from a population where the mean is 36.6 cm or greater.\n\n::: Q.\nGiven the hypothesis, is this one or two tailed test?\n:::\n\nWe collect a sample of 20 fish (found in the worksheet ‘Cod lengths’). The sample size is $<$ 30 so we can't assume that the means will be normally distributed under the CLT. We can check the normality assumption by plotting the data using a ‘normality’ plot or ‘QQ-plot’ (@fig-cod_qq).\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncod_df <- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"Cod lengths\")\nstr(cod_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [20 × 1] (S3: tbl_df/tbl/data.frame)\n $ CodLength_cm: num [1:20] 34.1 35.6 36.1 34.6 38.3 35 36.8 38 34.4 35.5 ...\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(cod_df$CodLength_cm, main = NULL)\nqqline(cod_df$CodLength_cm)\n```\n\n::: {.cell-output-display}\n![QQ-plot for the sample of cod.](P4_t_CIs_files/figure-epub/fig-cod_qq-1.png){#fig-cod_qq}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nDo you think the normality assumption is reasonable?\n:::\n\n::: Q.\nWhat parameter are we actually trying to understand / model? How does the distribution of this parameter change with sample size (think CLT)?\n:::\n\nWe wish to assess how likely our sample is to have been drawn from a population where $\\mu$ is at least 36.6 cm. If our sample mean $\\bar{y}$ is less than the ‘legal’ mean and it is ‘unlikely’ to have been drawn from a legal population, we might wonder if the mean of the landed cod on this boat is \\<36.6 cm and recommend legal action.\n\n::: Q.\nCalculate the sample mean, standard deviation, and standard error of the mean (@sec-appendix).\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## View solution\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncod_df |>\n  summarise(mean=mean(CodLength_cm),\n            sd=sd(CodLength_cm),\n            N=n(),\n            SEM=sd(CodLength_cm)/sqrt(N))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n   mean    sd     N   SEM\n  <dbl> <dbl> <int> <dbl>\n1  35.8  1.55    20 0.347\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: Q.\nNow manually calculate the t-statistic for this sample and determine the probability of observing your data assuming that $\\mu = 36.6 cm$.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## View solution\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nybar <- mean(cod_df$CodLength_cm)\nSEM <- sd(cod_df$CodLength_cm) / sqrt(length(cod_df$CodLength_cm))\nT_stat <- (ybar - 36.6) / SEM\nround(T_stat, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.35\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: Q.\nHow does this value compare to the expectation under the null hypothesis? Use `pt()`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## View solution\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npt(T_stat, df=19) |> round(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01482\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: Q.\nCheck you answer against that given by the `t.test()` function. See `?t.test`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## View solution\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nt.test(cod_df$CodLength_cm, mu = 36.6, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  cod_df$CodLength_cm\nt = -2.3515, df = 19, p-value = 0.01482\nalternative hypothesis: true mean is less than 36.6\n95 percent confidence interval:\n     -Inf 36.38429\nsample estimates:\nmean of x \n   35.785 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nHopefully your manually calculated t-statistic and the one generated by R match. The p-value given by R is exact: there is a probability of 0.014819 that a sample of 20 cod with mean of 35.785 cm would be drawn from a legally landed cod population where the true mean was 36.6 cm or more (assuming model assumptions are met).\n\n::: Q.\nWhat is your next step as the regulatory agent?\n:::\n\nEvaluating evidence is a central part of statistical analysis. In this example, your conclusion about the population mean $\\mu$ based on your sample determines whether legal action is taken. When a binary decision is necessary, it is best to set the decision threshold before collecting data. This is called the $\\alpha$ value. It is often set to 0.05, but this is a subjective choice. If $P < \\alpha$, the null hypothesis is rejected; the sample is considered too unlikely if $\\mu \\geq 36.6cm$. In this scenario, we have two clear competing hypotheses, so we are in the realm of ‘Neyman-Pearson’s’ decision theory (not Fisher’s hypothesis significance testing approach).\n\n::: Q.\nGiven the P value, do you reject the null hypothesis?\n:::\n\n::: Q.\nIf you had set $\\alpha$ at 0.01 would you reject the null hypothesis?\n:::\n\n::: Q.\nIf you set $\\alpha$ at 0.01 rather than 0.05, what type of error are you reducing and what type of error are you increasing? Which wrong conclusion becomes more likely and which becomes less likely?\n:::\n\nNow let's use simulation to explore variability among samples. We will repeatedly sample from $y \\sim Norm(100, 10)$.\n\n::: Q.\nWhat is the standard deviation in this model?\n:::\n\n::: Q.\nWhat does the symbol ‘\\~’ mean?\n:::\n\nWe will repeat our sampling `num_samples` times. Each sample will be `sample_size` values drawn from a normally distributed population with mean `mu` and standard deviation `sigma`. For each sample, we will calculate the sample mean $\\bar{y}$ and sample standard deviation $s$. Note that in this case, we *know* the population parameters.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set simulation details and population parameters\nnum_samples <- 5\nsample_size <- 3\nmu <- 100\nsigma <- 10\n\n# initialize plot\nset.seed(1)\nxlims <- c(mu-4.5*sigma, mu+3*sigma)\nplot(NA, NA, xlim=xlims, ylim=c(0,num_samples),\n     xlab=\"Observations\", ylab=\"Sample number\")\nabline(v=mu, lty=3)\npoints(mu, 0, pch=4, col=\"steelblue\")\nsegments(mu-sigma, 0, mu+sigma, 0, col=\"steelblue\")\ntext(x=xlims[1], y=0, adj=c(0,0.5), col=\"steelblue\", \n     labels=paste0(\"mu: \", mu, \", sigma: \", sigma))\n\n# repeatedly sample\nfor (i in 1:num_samples) {\n  sample_i <- rnorm(n = sample_size, mean = mu, sd = sigma)\n  sample_mean <- signif(mean(sample_i), 3)\n  sample_sd <- signif(sd(sample_i), 3)\n  # add to plot\n  points(sample_i, rep(i, sample_size))\n  points(sample_mean, i, pch=4)\n  segments(sample_mean-sample_sd, i, sample_mean+sample_sd, i)\n  text(x=xlims[1], y=i, adj=c(0,0.5),\n       paste0(\"ybar: \", sample_mean, \", s: \", sample_sd))\n}\n```\n\n::: {.cell-output-display}\n![Repeated samples from a normal distribution. Open points are observations in the sample, with 'x' giving the mean and lines showing 1 sd.](P4_t_CIs_files/figure-epub/fig-repeatSamples-1.png){#fig-repeatSamples}\n:::\n:::\n\n\n\n\n\n\n\n\nNote: these are random samples, so the values will be different each time you run the code. However, R uses pseudo-random number generation. Use `set.seed()` for fully reproducible code.\n\n::: Q.\nIs there a discrepancy between the population parameters and the sample statistics? Does the discrepency seem greater for the mean or the standard deviation?\n:::\n\nThe discrepancy ($\\mu$ vs. $\\bar{y}$, $\\sigma$ vs. $s$) is called *sampling error*. In most situations, we do not know $\\mu$ and $\\sigma$, but must estimate them from our sample.\n\nIf the sample is very large (and representative) then the estimate of the population parameters is likely very good. However, as the sample size is reduced, the reliability of the estimate decreases. Try adjusting `sample_size` in the code above and see how it affects the results.\n\nThe t-distribution is the distribution of values you get when you subtract sample means from the population mean and standardize by the sample standard error (i.e., $\\frac{\\bar{y} - \\mu}{SE_{\\bar{y}}}$).\n\n::: Q.\nHow does this distribution relate to the T-statistic calculated above? The P-value?\n:::\n\nWe can extend the simulated sampling above to calculate a T-statistic for each sample in addition to $\\bar{y}$ and $s$. We will set `num_samples` very large to better represent the distribution of sample T-statistics. The red histogram in @fig-t_stat_hist shows the distribution of these sample T-statistics. The solid curve is the theoretical t-distribution (df=`sample_size - 1`) and the dotted line is a standard normal distribution.\n\n::: Q.\nExplore different values for `sample_size` below to see how the shapes change.\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- 10 # population mean\nsigma <- 10 # population sd\nnum_samples <- 1e5 # number of samples\nsample_size <- 3 # size of each sample\nT_sample <- numeric(num_samples) # sample t statistics\n\n# for each repeat: draw a sample, calculate SE and T, and store T in T_sample\nfor(i in 1:num_samples) {\n  sample_i <- rnorm(sample_size, mu, sigma)\n  sample_SEM <- sd(sample_i) / sqrt(sample_size)\n  T_sample[i] <- (mean(sample_i) - mu) / (sample_SEM)\n}\n\ncurve(dnorm(x, 0, 1), from = -6, to = 6, lty = 2, \n      xlab = \"Simulated T-statistics\", ylab = \"Density\",\n      main = paste(\"T-statistics of\", \n                   format(num_samples, big.mark=\",\", scientific=F), \n                   \"samples, each with N =\", sample_size))\nhist(T_sample, freq = F, add = T, col = rgb(1, 0, 0, 0.25), \n     breaks = seq(floor(min(T_sample)), ceiling(max(T_sample)), by=0.2))\ncurve(dnorm(x, 0, 1), from = -6, to = 6, add = T, lty = 2)\ncurve(dt(x, sample_size - 1), from = -6, to = 6, add = T)\nlegend(\"topright\", lty = c(2, 1, 1), col = c(1, 1, 2), bty = \"n\",\n       c(\"Normal(0,1)\", paste0(\"t(df=\", sample_size-1, \")\"), \"t-stat (sim)\"))\n```\n\n::: {.cell-output-display}\n![Histogram of 100,000 T-statistics calculated from 100,000 samples, along with the corresponding theoretical t-distribution (solid line) and a standard normal (dotted line).](P4_t_CIs_files/figure-epub/fig-t_stat_hist-1.png){#fig-t_stat_hist}\n:::\n:::\n\n\n\n\n\n\n\n\nNotice how the histogram and the solid lines are nearly identical? These simulations illustrate that the t-distribution *is* the distribution of t-statistics for a given sample size.\n\n::: callout-note\nWhen we perform a t-test, we compare the T-statistic from our sample to this distribution: the distribution of T-statistics we would expect under the hypothesized $\\mu$. The P-value gives the probability of our T-statistic (or one more extreme) under the hypothesized $\\mu$. If it is very small, it is very unlikely that our sample would occur with that $\\mu$, and we might conclude the value of $\\mu$ is something different.\n:::\n\n## Confidence Intervals\n\nSay you are interested in knowing the mean of a population (e.g. barnacle mass on the back beach). You cannot afford to determine the mass of each barnacle, so you take a random sample. You don’t know how ‘good’ (i.e. representative) your sample is. It might have included lots of small barnacles, or big ones, or a wide- or narrow-range of sizes. You can never know (unless you sample everything). When you calculate the mean of this sample you don’t know how close it is to the population mean, but you do know the probability associated with that estimate. Confidence intervals capture this uncertainty, and you use the t-distribution to determine them.\n\nWe’ll invent a population of barnacle diameters, called `barnacle_diam`. We'll create a histogram of that population and superimpose values on that. Again, these are random numbers so your values will be slightly different from mine.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# barnacle population meta-details\npop_size <- 100000  # number of barnacles in the population\nbarnacle_mu <- 200\nbarnacle_sigma <- 25\n\n# the full population:\nbarnacle_diam <- rnorm(pop_size, mean = barnacle_mu, sd = barnacle_sigma)\nmu <- mean(barnacle_diam)\nsigma <- sd(barnacle_diam)\n\n# histogram of the population with middle 95% shown\nhist(barnacle_diam, main = NULL)\nQ95 <- quantile(barnacle_diam, c(0.025, 0.975))\nabline(v = Q95, col = \"green\", lwd = 3)\ntext(x=Q95[2], y=pop_size/7, \n     labels=paste0(\"mu: \", round(mu, 1), \"\\nsigma:\", round(sigma, 1)))\n```\n\n::: {.cell-output-display}\n![Histogram of a simulated barnacle population with 2.5\\% and 97.5\\% quantiles.](P4_t_CIs_files/figure-epub/fig-ci_examp1-1.png){#fig-ci_examp1}\n:::\n:::\n\n\n\n\n\n\n\n\nNow we can take samples from that population. This is reality: you take samples from populations where you don’t know the population mean and standard deviation. Let's take 4 samples, each with size `sample_size`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(barnacle_diam, main = NULL)\nsample_size <- 5\nnum_samples <- 4\nabline(v = mu, col = \"blue\", lwd = 4)\n\nfor (i in 1:num_samples) {\n  sample_i <- sample(barnacle_diam, size = sample_size)\n  print(sample_i)\n  abline(v = mean(sample_i), col = \"red\", lwd = 0.5)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252.2560 209.5652 194.1364 180.0893 199.9900\n[1] 181.4844 212.2622 212.0780 205.8330 209.9440\n[1] 214.3341 150.0059 206.0774 194.1674 191.1762\n[1] 168.1111 194.2603 180.3917 178.7273 193.4117\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Histogram of the barnacle population showing location of 4 sample means (red lines), each with N = 5. The blue line shows the true population mean mu.](P4_t_CIs_files/figure-epub/fig-ci_examp2-1.png){#fig-ci_examp2}\n:::\n:::\n\n\n\n\n\n\n\n\nOur sample means inevitably differ from the true population mean ($\\mu$), even if only a bit. Likewise, the sample standard deviations will differ from the true population standard deviation ($\\sigma$).\n\nIf we repeat this sampling enough times, we can generate a distribution of sample standard deviations (@fig-sd_N_hist). This distribution is not normal, but is instead related to the chi-square distribution (don’t worry too much about this). The point is that if your sample size is small, your estimate of the standard deviation is often very poor.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow=c(2,2))\n# sim_df will hold the sample sizes N, and the median and mean sample sd's \nnum_samples <- 1e4\nsim_df <- data.frame(N=c(2, 4, 10, 30),\n                     sd_median=NA, sd_mean=NA,\n                     sd_q025=NA, sd_q25=NA, sd_q75=NA, sd_q975=NA,\n                     mn_median=NA, mn_mean=NA,\n                     mn_q025=NA, mn_q25=NA, mn_q75=NA, mn_q975=NA) \n\n# for each sample size N, draw a sample and store its mean and sd\n# repeat this num_samples times\n# plot a histogram of the sample sd's, then store the mean and median\nfor (i in 1:nrow(sim_df)) {\n  samp_sd_i <- numeric(num_samples) \n  samp_mn_i <- numeric(num_samples) \n  for (j in 1:num_samples) { \n    sample_ij <- sample(barnacle_diam, size = sim_df$N[i])\n    samp_sd_i[j] <- sd(sample_ij)\n    samp_mn_i[j] <- mean(sample_ij)\n  }\n  hist(samp_sd_i, main = paste(num_samples, \"sample SDs for N =\", sim_df$N[i]), \n       breaks = 20, xlim = c(0, 100))\n  abline(v = sigma, col = \"blue\", lwd = 2)\n  sim_df[i, 2:13] <- c(median(samp_sd_i), mean(samp_sd_i),\n                       quantile(samp_sd_i, probs = c(0.025, 0.25, 0.75, 0.975)),\n                       median(samp_mn_i), mean(samp_mn_i),\n                       quantile(samp_mn_i, probs = c(0.025, 0.25, 0.75, 0.975)))\n}\n```\n\n::: {.cell-output-display}\n![Histograms of sample standard deviations from repeated samples of the same barnacle population. The true population standard deviation is shown in blue.](P4_t_CIs_files/figure-epub/fig-sd_N_hist-1.png){#fig-sd_N_hist}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow=c(1,2))\nplot(sim_df$N, sim_df$sd_median,\n  xlim = c(0, 30), ylim = range(c(sim_df[,2:7], sigma)),\n  type = \"b\", xlab = \"Sample size\", ylab = \"Standard deviation\"\n)\nsegments(sim_df$N, sim_df$sd_q25, sim_df$N, sim_df$sd_q75, lwd=2)\nsegments(sim_df$N, sim_df$sd_q025, sim_df$N, sim_df$sd_q975)\nlines(sim_df$N, sim_df$sd_mean, type = \"b\", col = \"dodgerblue\")\nabline(h = sigma, lty = 2)\nlegend(\"topright\", c(\"Mean sample SD\", \"Median sample SD\", \"True SD\"),\n  col = c(\"black\", \"dodgerblue\", \"black\"),\n  lty = c(1, 1, 2), pch = c(1, 1, NA), bty = \"n\"\n)\nplot(sim_df$N, sim_df$mn_median,\n  xlim = c(0, 30), ylim = range(c(sim_df[,8:13], mu)),\n  type = \"b\", xlab = \"Sample size\", ylab = \"Mean\"\n)\nsegments(sim_df$N, sim_df$mn_q25, sim_df$N, sim_df$mn_q75, lwd=2)\nsegments(sim_df$N, sim_df$mn_q025, sim_df$N, sim_df$mn_q975)\nlines(sim_df$N, sim_df$mn_mean, type = \"b\", col = \"dodgerblue\")\nabline(h = mu, lty = 2)\nlegend(\"topright\", c(\"Mean sample mean\", \"Median sample mean\", \"True mean\"),\n  col = c(\"black\", \"dodgerblue\", \"black\"),\n  lty = c(1, 1, 2), pch = c(1, 1, NA), bty = \"n\"\n)\n```\n\n::: {.cell-output-display}\n![Mean (black), median (blue), and 50\\% and 95\\% quantiles (vertical lines) for (left) sample standard deviations at each sample size compared to the true population standard deviation (dotted line) or for the (right) sample means.](P4_t_CIs_files/figure-epub/fig-sd_N_lines-1.png){#fig-sd_N_lines}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nDoes the difference between the mean (black) and median (blue) in @fig-sd_N_lines match your expectations based on the shape of the distributions in @fig-sd_N_hist?\n:::\n\nThe t-distribution allows for the fact that the standard deviation of small samples is usually less than that of the population as seen in @fig-sd_N_hist and @fig-sd_N_lines.\n\nThe take home message here is that when we sample from a population with unknown $\\mu$ and $\\sigma$, we won’t know how ‘accurate’ the sample is, but we do know how your random samples ‘behave’ - they are modelled using the t-distribution. From this knowledge, we can build a 95% confidence interval which is described as an interval which, if repeated for many samples, would include $\\mu$ within its boundaries in 95% of those samples. Read that again. You don’t have knowledge of the true value of the mean or sd (as you did for Z score calculations) and the t-distribution accounts for this uncertainty.\n\n::: callout-note\nA 95% confidence interval is the interval that, when calculated on infinite repeated samples, contains the population mean for 95% of those samples (and thus misses the population mean in 5% of the samples).\n:::\n\nWe can modify @fig-ci_examp2 by adding 95% confidence intervals for each sample from our barnacle population.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_samples <- 5\nsample_size <- 3\n\n# plot population\nhist(barnacle_diam, \n     xlim = c(barnacle_mu-6*barnacle_sigma, barnacle_mu+6*barnacle_sigma), \n     ylim = c(0, length(barnacle_diam)/6),\n     main = NULL, \n     col = \"grey90\", border = \"grey50\", xlab = \"Barnacle diameter\")\nabline(v = mu, col = \"blue\", lwd = 2)\ny_pos <- seq(0, length(barnacle_diam)/6, length.out=num_samples)\n\n# draw samples, calculate mean and 95% CIs, and plot them\nfor (i in 1:num_samples) {\n  sample_i <- sample(barnacle_diam, size = sample_size)\n  points(x = mean(sample_i), y = y_pos[i], col = \"red\", pch = 16, cex = 0.75)\n  sample_ci <- c(\n    mean(sample_i) + qt(0.025, (sample_size - 1)) * (sd(sample_i) / sqrt(sample_size)),\n    mean(sample_i) + qt(0.975, (sample_size - 1)) * (sd(sample_i) / sqrt(sample_size))\n  )\n  arrows(sample_ci[1], y_pos[i], sample_ci[2], y_pos[i], \n         col = \"red\", code = 3, angle = 90, length=0.05)\n}\n```\n\n::: {.cell-output-display}\n![Histogram illustrating the barnacle population with population mean (blue) and sample means with 95\\% CIs (red) repeated across 5 samples.](P4_t_CIs_files/figure-epub/fig-ci_examp3-1.png){#fig-ci_examp3}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nKeep repeating the above code until you get an example where your 95% CI misses the true value of the mean.\n:::\n\n::: Q.\nTry different values for `sample_size`. How does this influence the width of your CIs?\n:::\n\n::: Q.\nWhat proportion of your 95% CIs would you expect to include the true value of the mean? Does that change for different values of `sample_size`?\n:::\n\n::: Q.\nFind the relevant bit of the code and determine 99% CIs, then 79% CIs. Note that you may need to adjust the x-axis limits which are set to 6 $\\sigma$ on either side of $\\mu$ in `hist()`).\n:::\n\nIn the above code, we calculated CIs manually using `qt()`. We can instead simply use `t.test()` and specify whatever confidence level we like:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbarnacle_sample <- sample(barnacle_diam, size=10)\nexamp_ttest <- t.test(barnacle_sample, conf.level=0.89) # 89% CIs\nexamp_ttest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  barnacle_sample\nt = 35.493, df = 9, p-value = 5.533e-11\nalternative hypothesis: true mean is not equal to 0\n89 percent confidence interval:\n 196.7245 217.4111\nsample estimates:\nmean of x \n 207.0678 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Or just the confidence intervals:\nexamp_ttest$conf.int\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 196.7245 217.4111\nattr(,\"conf.level\")\n[1] 0.89\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nLet’s explore the influence of sample size on the width of the confidence interval a little more.\n\nFor sample sizes of 2, 5, and 10, we will collect a sample and calculate the 95% CIs with `t.test()`. As above, we will repeat our sampling a few times to assess the variability among samples of the same size.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nnum_samples <- 10\nsample_sizes <- c(2, 5, 10)\n\nCI_df <- expand_grid(N=sample_sizes,\n                     sample_id=1:num_samples) |>\n  rowwise() |>\n  mutate(obs=list(sample(barnacle_diam, N)),\n         mn=mean(obs),\n         ci_lo=t.test(obs)$conf.int[1],\n         ci_hi=t.test(obs)$conf.int[2]) |>\n  ungroup() |>\n  mutate(N=factor(N, levels=unique(N), labels=paste(\"N =\", sample_sizes)))\nggplot(CI_df, aes(mn, sample_id, xmin=ci_lo, xmax=ci_hi, colour=N)) +\n  geom_vline(xintercept=barnacle_mu, colour=\"cadetblue\") + \n  geom_point(size=1.5) + \n  geom_errorbarh(height=0.15) + \n  facet_wrap(~N, ncol=1, scales=\"free_y\", strip.position=\"left\") + \n  labs(x=\"Sample mean and 95% confidence interval\") +\n  theme_bw() + \n  theme(axis.text.y=element_blank(), \n        axis.ticks.y=element_blank(),\n        axis.title.y=element_blank(),\n        legend.position=\"none\",\n        panel.grid.minor=element_blank())\n```\n\n::: {.cell-output-display}\n![Confidence interval size vs. sample size.](P4_t_CIs_files/figure-epub/fig-ci_v_N-1.png){#fig-ci_v_N}\n:::\n:::\n\n\n\n\n\n\n\n\n::: Q.\nWhat do you notice about the average width of CIs in @fig-ci_v_N as sample size changes? What about the variability in width?\n:::\n\n::: Q.\nWhat proportion of 95% CIs would you expect to include $\\mu$ for N=2? For N=100?\n:::\n\n## Comparing means (two-sample t tests)\n\nThe two-sample t-test is a widely used inferential statistical test. The two-sample t-test is a special case of analysis of variance (ANOVA) where there are only two groups. The results are identical and so we only mention its existence. It is good to be aware of the two-sample t-test because it is so commonly used, but you will be comparing means using ANOVA in @sec-P5.\n\n## Non-parametric Tests\n\nParametric tests are so named because they estimate population parameters. Non-parametric tests are often used to compare samples where the data are non-continuous or fail the assumptions of parametric general linear models, typically converting data to ranks for analysis rather than using the actual values. Non-parametric test include ‘classics’ such as the ‘Mood’ and ‘Wilcoxon’ tests. However, we make you aware of the GLM family which will usually supply you with a much more elegant solution to model data that doesn’t fit the simple linear model. You should be aware of the existence of ‘non-parametric’ tests because they are prevalent in the literature. Remind yourself of the disadvantages of non-parametric tests.\n\n## Conclusions\n\nThe t-test is a ‘classic’ statistical test which doesn’t assume knowledge of population parameters. The strength of the t-test (its ability to quantify differences between samples) is proportional to the sample size. The larger the sample size, the better the estimate of the population parameters and the more precisely we are to be able to detect differences between the means.\n\nThe central limit theorem tells us that the means of non-normally distributed data will be normally distributed if the sample size is sufficiently large. If your sample size is $>$ 30 it is likely that the means of that sample will be normally distributed regardless of the distribution of the original data.\n\nParametric tests including the t-test are quite ‘robust’ against deviations from normality, particularly as sample sizes increase. However, parametric test are less robust against heteroscedasticity, regardless of sample size. Always check this assumption and be prepared to transform the data if the assumption of homoscedasticity is not tenable (more of this in @sec-P5).\n\nThe t-test is in the ‘general linear model’ family (which is a subset of the generalized linear modelling family). General linear models are usually used to model continuous data where residual error is approximately normal. If you have count data, you should start with a different member of the GLM family before trying transformations to acheive approximate normality. Non-parametric tests are frequently adopted when data do not conform to the assumptions of normality but they are invariably used for NHST with all the inherent problems with that approach.\n\nA final reminder with regard to many statistical tests, including all in the GLM family: they make the assumption that data are independent. You must always ensure that your experimental design lends itself to making independent observations *in relation to the question you are asking*. This is the most critical and fundamental of the assumptions of parametric and non-parametric tests. Non-independence (e.g. measuring the same urchin over time) can be modelled using more complex ‘mixed’ models. Application of mixed modelling is beyond this course but you should be aware of the limitations of the techniques that you are learning and know where to go next.\n",
    "supporting": [
      "P4_t_CIs_files\\figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}