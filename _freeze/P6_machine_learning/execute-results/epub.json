{
  "hash": "ee38fb4a74105d6b6dbe1ce8e772e257",
  "result": {
    "engine": "knitr",
    "markdown": "# Machine Learning {#sec-P6}\n\n## Multivariate analysis\n\nSo far, every model we've used has had a single response variable. **Multivariate analyses** are techniques that allow the analysis of multiple response variables, such as counts of each species within a community. Statistical routines for multivariate analysis are relatively new and have co-evolved with computational capacity. Multivariate data are typically stored as a matrix of samples (rows) v ‘features’ (columns). Features may include things like faunal counts, chemical concentrations, or environmental conditions.\n\nMultivariate analysis typically includes three stages:\n\n1)  Data transformation or standardisation\n2)  Calculation of a dissimilarity matrix\n3)  Ordination (display) of the dissimilarity matrix\n\nEach of these steps has numerous options, with advantages and disadvantages to each. Multivariate analysis are less inferential than most univariate approaches and implementation can feel subjective. Statisticians are still arguing about the best way to approach multivariate analyses.\n\nMultivariate data occur in a number of common situations, including species inventories, multiple data streams from the same station (e.g., a glider with CTD), and in bioinformatics (e.g., metabarcoding). Many are critical resources in understanding relative change through time.\n\nMore detail on the the material we cover here is accessible in Clarke et al. (2014) on Brightspace, entitled *Change in marine communities: An approach to statistical analysis and interpretation.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(vegan)\nlibrary(plotly)\nset.seed(2025)\n```\n:::\n\n\n## Non-Metric Multidimensional Scaling (NMDS)\n\nFor the NMDS workflow, we’ll work through the above steps (`data |> transform() |> dissimilarity() |> ordination()`) first with two simulated datasets and then with a more complex (real) dataset included in the package *vegan*.\n\nRecall that the `^` operator raises each element in a vector to a power. For example, for the vector `obs_values <- c(1, 10, 35)`, we can calculate a fourth-root transformation of the whole vector with `obs_values^(1/4)`. Remember from previous courses that this is identical to `sqrt(sqrt(obs_values))`.\n\n### 1: Data transformation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# these toy data duplicate those in the multivariate lecture.\nworm_df <- data.frame(\n  row.names = c(\"CE1\", \"CE2\", \"Ref1\", \"Ref2\"),\n  Capitella = c(1000, 1500, 10, 50),\n  Malacoeros = c(500, 2000, 50, 25),\n  Mysella = c(1, 0, 25, 30),\n  Nucella = c(1, 0, 20, 15)\n)\nworm_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Capitella Malacoeros Mysella Nucella\nCE1       1000        500       1       1\nCE2       1500       2000       0       0\nRef1        10         50      25      20\nRef2        50         25      30      15\n```\n\n\n:::\n\n```{.r .cell-code}\n# alt-log transformation: ifelse(x==0, 0, log(x))\nworm_df_log <- decostand(worm_df, method = \"log\", logbase = 10)\nworm_df_4rt <- worm_df^0.25\n```\n:::\n\n\nTake a look at `worm_df`, `worm_df_log`, and `worm_df_4rt` to be sure they make sense.\n\n### 2: Dissimilarity matrix\n\nA dissimilarity matrix summarises the dissimilarity between each pair of samples. There are many methods of summarising the dissimilarity (or distance), particularly when that dissimilarity occurs across multiple dimensions (here, genera).\n\nThe `vegdist()` function can generate distance matrices using many different methods. See `?vegdist` for more information. We'll use Bray-Curtis for the raw and 4^th^-root transformed data, and alternative Gower for the alt-log transformed data. See the help page and Brightspace material for more information on these. Feel free to investigate other combinations of data transformation and dissimilarity matrices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvegdist(worm_df, method = \"bray\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           CE1       CE2      Ref1\nCE2  0.4002399                    \nRef1 0.9228376 0.9667129          \nRef2 0.9050555 0.9585635 0.3333333\n```\n\n\n:::\n\n```{.r .cell-code}\nvegdist(worm_df_4rt, method = \"bray\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            CE1        CE2       Ref1\nCE2  0.18044721                      \nRef1 0.39098221 0.59100112           \nRef2 0.36024122 0.55728021 0.08642723\n```\n\n\n:::\n\n```{.r .cell-code}\nvegdist(worm_df_log, method = \"altGower\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           CE1       CE2      Ref1\nCE2  0.6945378                    \nRef1 1.4247425 2.1192803          \nRef2 1.3138181 2.0083559 0.3010300\n```\n\n\n:::\n:::\n\n\n::: Q.\nBased on the dissimilarity matrices, which sites are more similar (smaller numbers) and which are more dissimilar (bigger numbers)? Does this align with an ‘eyeballing’ of the data? How has the data transformation changed the resultant dissimilarity matrix?\n:::\n\n### 3: Ordination\n\nNext we want to plot the dissimilarities to visualize which sites are more similar or dissimilar to one another. We will use functions from *vegan*. Remember that you can run `?packageName` to learn more about any R package, typically with helpful examples and vignettes of the most useful applications of the package.\n\nWe have numerous options in relation to displaying the dissimilarity matrices. We'll explore non-metric multiple dimensional scaling, abbreviated to NMDS, nMDS, nmMDS, or just MDS.\n\nAs you might suspect, there are R functions which combine the three steps, though often it is preferable to separate them. The `metaMDS()` function calculates a dissimilarity matrix (as we did above) and produces an R object with all the information needed for plotting. It expects that samples are in rows and species (features) are in columns. We can also specify the distance metric, the number of axes to project to, whether to autotransform the data, and many other options. See `?metaMDS` for the default values and other available arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nord_raw <- metaMDS(worm_df, distance = \"bray\", k = 2,\n                   autotransform = FALSE, trace = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in metaMDS(worm_df, distance = \"bray\", k = 2, autotransform = FALSE, :\nstress is (nearly) zero: you may have insufficient data\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n```\n\n\n:::\n\n```{.r .cell-code}\nordiplot(ord_raw, choices = c(1, 2), display = \"sites\", \n         type = \"text\", main = \"NMDS: raw data, Bray-Curtis\")\n```\n\n::: {.cell-output-display}\n![Simple pattern in example worm data.](P6_machine_learning_files/figure-epub/fig-worm_1-1.png){#fig-worm_1}\n:::\n:::\n\n\nNote that `metaMDS()` involves some randomization, and your plot will change each time you run the code. For fully reproducible code, use `set.seed()`.\n\nCompare this with the 4^th^-root transformed data using the same distance metric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nord_4rt <- metaMDS(worm_df_4rt, distance = \"bray\", k = 2, \n                   autotransform = FALSE, trace = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in metaMDS(worm_df_4rt, distance = \"bray\", k = 2, autotransform =\nFALSE, : stress is (nearly) zero: you may have insufficient data\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n```\n\n\n:::\n:::\n\n\n::: Q.\nPlot `ord_4rt`. How has the 4th root changed your data interpretation?\\*\n:::\n\n::: Q.\nInterpret the ordinations, cross referencing to the raw and transformed data. Are the patterns that you see in the data apparent on the ordination?\n:::\n\nYou can include on your plot the species ‘locations’ (as determined by their correlation with the axes). This shows where the main associations are occurring.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# you can also plot the 'species' on the ordination\nordiplot(ord_4rt, choices = c(1, 2), display = c(\"sites\", \"species\"), \n         type = \"text\", main = \"NMDS, 4th-rt trans., Bray-Curtis\")\n```\n\n::: {.cell-output-display}\n![Species superimposed onto ordination, indicating species-site associations.](P6_machine_learning_files/figure-epub/fig-worm_2-1.png){#fig-worm_2}\n:::\n:::\n\n\n::: Q.\nRe-plot the untransformed data, but this time include the species. How has the transformation changed your interpretation of the data?\n:::\n\nLet’s have a look at another simulated dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomm_df <- data.frame(\n  row.names = c(\"Dunst\", \"Creran\", \"Lismore\", \"Charl\"),\n  SpA = c(1, 20, 30, 40),\n  SpB = c(11, 22, 50, 1),\n  SpC = c(500, 40, 30, 20),\n  SpD = c(10, 25, 35, 50),\n  SpE = c(4, 3, 2, 1),\n  SpF = c(40, 250, 1, 9)\n)\ncomm_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        SpA SpB SpC SpD SpE SpF\nDunst     1  11 500  10   4  40\nCreran   20  22  40  25   3 250\nLismore  30  50  30  35   2   1\nCharl    40   1  20  50   1   9\n```\n\n\n:::\n\n```{.r .cell-code}\nord_comm <- metaMDS(comm_df, distance = \"bray\", k = 2, \n                    autotransform = FALSE, trace = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in metaMDS(comm_df, distance = \"bray\", k = 2, autotransform = FALSE, :\nstress is (nearly) zero: you may have insufficient data\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n```\n\n\n:::\n\n```{.r .cell-code}\nordiplot(ord_comm, choices = c(1, 2), \n         display = c(\"sites\", \"species\"), type = \"text\")\n```\n\n::: {.cell-output-display}\n![Ordination for a simulated dataset showing sites and species.](P6_machine_learning_files/figure-epub/fig-comm_ord-1.png){#fig-comm_ord}\n:::\n:::\n\n\n::: Q.\nHave a look at these raw data. What are the main trends? Which sites are more similar?\n:::\n\nThese data are still much simpler than most ‘real’ data sets but it is still difficult to summarise the similarities and differences between stations. However, multivariate analyses help you in this process.\n\n::: Q.\nNow fourth-root transform these data, generate the new dissimilarity matrix and plot it.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## View solution\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncomm_4rt_df <- comm_df^0.25\nord_comm_4rt <- metaMDS(comm_4rt_df, distance = \"bray\", k = 2, \n                        autotransform = FALSE, trace = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in metaMDS(comm_4rt_df, distance = \"bray\", k = 2, autotransform =\nFALSE, : stress is (nearly) zero: you may have insufficient data\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nordiplot(ord_comm_4rt, choices = c(1, 2), \n         display = c(\"sites\", \"species\"), type = \"text\")\n```\n\n::: {.cell-output-display}\n![Ordination for the 4th root transformed data.](P6_machine_learning_files/figure-epub/fig-comm_ord_4rt-1.png){#fig-comm_ord_4rt}\n:::\n:::\n\n:::\n\n\n::: Q.\nWhat has the transformation done to your interpretation of differences between the Sites and Site x Species associations?\n:::\n\n\n------------------------------------------------------------------------\n\n## Diversity indices\n\nPrior to the development of the multivariate techniques you’ll be using today, univariate indices were derived from multivariate data. A classic example of such a univariate measure in ecology is the Shannon-Wiener diversity index (a.k.a., Shannon's H). This index balances the *number* of species in a sample and the *relative abundance* of each species (where ‘species’ can once again be any sort of feature). Univariate measures of ‘evenness’ can also be derived from multivariate data and, when reporting species data, you may also wish to include species richness, which is just the number of species present regardless of their abundances.\n\nWe can use the `comm_df` dataset we invented to explore some diversity concepts.\n\n::: Q.\nLooking at the `comm_df` data (by eye) and given the description above, which of the sites is associated with the lowest and highest diversity?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshannon_H <- diversity(comm_df, \"shannon\", base = exp(1)) \nrichness <- specnumber(comm_df) \nbarplot(shannon_H, main = NULL, ylab = \"Shannon's H\")\n```\n\n::: {.cell-output-display}\n![Shannon diversity](P6_machine_learning_files/figure-epub/fig-H_examp-1.png){#fig-H_examp}\n:::\n:::\n\n\nTry plotting `richness`.\n\n::: Q.\nDo the plots correspond to what you expected?\n:::\n\n::: Q.\nWhat does `specnumber()` do?\n:::\n\n::: Q.\nHow could you ‘counter’ any extremes (as in superabundant taxa) in the raw count data that you’ve generated? Try your idea.\n:::\n\n::: Q.\nWhich description (diversity or richness) is ‘best’ for describing your multivariate data? How does this compare to NMDS?\n:::\n\n## NMDS on real data\n\nUsing simulated (or at least simple) data to learn new statistical techniques is usually the best approach because it gives you the opportunity to get a better sense for how the algorithms work (and to be sure your code is free from bugs!). Now we will progress to real observations.\n\nHave a look at the `varespec` and `varechem` datasets included in the *vegan* package. I’ve reproduced the examples below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(varespec) # ?varespec -- percent cover of 44 spp in lichen pastures \ndata(varechem) # ?varechem -- associated soil characteristics\nord_vare <- metaMDS(varespec^0.25, distance = \"bray\", \n                   trace = FALSE, autotransform = FALSE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3), mar=c(4,4,1,1)) \nordiplot(ord_vare, choices = c(1, 2), display = \"sites\", type = \"text\")\nordiplot(ord_vare, choices = c(1, 2), display = \"species\", type = \"text\")\nordiplot(ord_vare, choices = c(1, 2), display = c(\"sites\", \"species\"),\n  type = \"text\")\n# You can superimpose environmental variables onto NMDS-ordinations.\nef <- envfit(ord_vare, varechem) #\nplot(ef, p.max = 0.1, col = \"green\") # overlay environmental variables\nplot(ef, p.max = 0.01, col = \"blue\") # subset based on p\n```\n\n::: {.cell-output-display}\n![vare- data from vegan showing sites only, species only, and both plus environmental overlays.](P6_machine_learning_files/figure-epub/fig-vare_ord-1.png){#fig-vare_ord}\n:::\n:::\n\n\nSee how the multivariate analysis has taken all those data, both species and environmental, and ‘communicated’ them in one single figure? You can see numerous relationships (both positive and negative) in this figure, and species-site-environment associations. It also illustrates a potential challenge with multivariate ordinations. It quickly gets cluttered and overloaded. There is no easy way around this, though there is further help in these packages.\n\nYou are in charge of the analysis. You can change the emphasis and elements of the message depending on your data transformation, dissimilarity metric, and ordination technique. There is no absolutely ‘correct’ way to go about multivariate stats, so different statisticians will have their favoured approaches and methods.\n\nNote: Some functions (e.g. `metaMDS()`) default to autotransform your data if the function thinks it is necessary. This can be useful but, in scientific reports, you must specify what transformations you used. Here you don’t know what the function applies as it depends on the data, but it could be any of several or a combination. My advice is only to use transformations that you specify.\n\n------------------------------------------------------------------------\n\n## Principal components analysis (PCA)\n\nPCA is a long-established multivariate technique that is often applied to ‘environmental’ data rather than ‘count’ data. Environmental data is, usually, quite different from species count data in that most environmental parameters (e.g. metal concentrations) are present, at least to some degree. This contrasts to species data where many species are often absent (zeros). These zero counts would lead to problems if analysed using PCA, since PCA would consider sites with many shared absences to be more similar, which is often not desirable.\n\nIn PCA, it is important that all variables are of a roughly similar magnitude. Environmental data might include all manner of different variables on different scales (e.g., radiant flux in lumens, temperature in C, nutrient/contaminant concentrations in mg/l, coordinates in easting/northing). If used in their raw form, the variables with larger values would have a greater influence on the outcome of the PCA.\n\nInstead, we want all of our variables to be treated ‘equally’. You can do all this by setting `pcomp(..., scale=TRUE)`. Scaling means that each measurement is expressed in units of standard deviation (a Z-score!!). Usually it is desirable to center the data as well by subtracting the mean. Centering and scaling means that each of the environmental variables is of ‘equal importance’ regardless of the magnitude of the raw values.\n\nA basic, and very friendly, introduction to PCA is given in Chapter 4 of Clarke et al. (2014).\n\nThe data set we’ll use is from SAMS Professor Tom Wilding's PhD thesis. He set up an experiment to examine the relative leaching of trace metals from concrete, granite, and a control (artificial seawater). Concrete contains cement which is enriched in vanadium and molybdenum, and these elements could leach out in dangerous amounts. Granite, the main constituent of this concrete, might also leach some trace elements. He suspended concrete and granite powder in artificial water, constantly agitated it, and measured the leachate concentrations over 100 days [Wilding and Sayer 2002](https://doi.org/10.1006/jmsc.2002.1267).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleach_df <- read_excel(\"data/H2DS_practicalData.xlsx\", sheet = \"Leaching\") |>\n  mutate(Treat_abbr = factor(Treat, # abbreviate for cleaner plotting\n                             levels = c(\"concrete\", \"control\", \"granite\"),\n                             labels = c(\"conc\", \"ctrl\", \"gran\")),\n         Treat_day = paste(Treat_abbr, Day, sep=\"_\"), # treat + days in exprmnt\n         Conc = signif(Conc)) |>\n  select(Treat_day, Element, Conc) # remove columns that aren't of use,\n#summary(leach_df)\n\n# not dominated by zeros; try other values (e.g. <10)\ntable(leach_df$Conc == 0) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFALSE  TRUE \n  143     4 \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(leach_df$Conc == 0) # recall that R treats T/F as 1/0 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02721088\n```\n\n\n:::\n:::\n\n\n::: Q.\nDo some exploratory analysis of this dataset. How are the concentrations distributed? Do similar treatments show qualitatively similar distributions? Elements?\n:::\n\nNext, we need to re-organise the data into a wider format so that each element is a column and each row is a sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleach_df_wide <- leach_df |>\n  pivot_wider(names_from=\"Element\", values_from=\"Conc\")\n```\n:::\n\n\nThe column `Treat_day` is coded as `trt_day`, where `trt` is the 4 letter code indicating treatment type and `day` is the number of days elapsed in the experiment (one of 1, 4, 7, 17, 32, 50 or 100). So `gran_32` means the granite treatment sampled at day 32.\n\nWe can calculate the principal components using `prcomp()`, subsetting the dataframe to give only the columns with element concentrations (i.e., removing `Treat_day`, which is the first column). We'll also set the arguments for centering and scaling to `TRUE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPCA_leach <- prcomp(leach_df_wide[, -1], center = TRUE, scale = TRUE) \nscreeplot(PCA_leach, main = NULL, ylab = \"Relative variation explained\")\n```\n\n::: {.cell-output-display}\n![Scree plot showing the variance explained by each principal component.](P6_machine_learning_files/figure-epub/fig-leach_scree-1.png){#fig-leach_scree}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# take a look at PCA_leach\nPCA_leach\nstr(PCA_leach)\n```\n:::\n\n\nYou can see from the scree plot that the amount of variance explained declines with principal component as expected, and that there is very little variation left after 3 principal components. That is, nearly all of the variation in the dataset is captured by PC1, PC2, and PC3. In this case, PCA has essentially solved the ‘curse of dimensionality’ by successfully reducing 7D data to about 3D.\n\nData transformations are critical to PCA analysis, as they are with NMDS. In most PCAs the data are standardized so that each column is on the same scale.\n\nWe can plot our results using `biplot()` which has some helpful defaults including labels for the samples (`Treat_day`) and the correlation strength of each element with PC1 and PC2:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(PCA_leach, xlabs = leach_df_wide$Treat_day, cex = 0.75)\n```\n\n::: {.cell-output-display}\n![PCA of the complete dataset, illustrating the common challenge of overplotting.](P6_machine_learning_files/figure-epub/fig-leach_biplot1-1.png){#fig-leach_biplot1}\n:::\n:::\n\n\nTo plot more than 2 dimensions you could use a static 3D plot, but these are often difficult to interpret since it is reduced *back* to 2D on a page. Another option is to use colour for the 3^rd^ axis, or, for digital distribution, a package like *plotly* can produce an interactive 3D plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nif(knitr::pandoc_to() == \"html\") {\n  \narrow_df <- tibble(element=row.names(PCA_leach$rotation),\n                   PC1=PCA_leach$rotation[,1]*6,\n                   PC2=PCA_leach$rotation[,2]*6,\n                   PC3=PCA_leach$rotation[,3]*6)\n\nPCA_leach$x |>\n  as_tibble() |>\n  mutate(Treat_day = leach_df_wide$Treat_day) |>\n  separate_wider_delim(Treat_day, delim=\"_\", names=c(\"Treatment\", \"Days\")) |>\n  mutate(Days=as.numeric(Days)) |>\n  plot_ly(type=\"scatter3d\", x=~PC1, y=~PC2, z=~PC3, symbol=~Treatment, color=~Days, mode=\"markers\",\n            symbols=c(\"circle-open\", \"diamond-open\", \"cross\")) |>\n  add_text(data=arrow_df, x=~PC1, y=~PC2, z=~PC3, text=~element, color=I(\"black\"), symbol=NULL)\n}\n```\n:::\n\n\n\nThe ordination plots the relative positions (in terms of similarity) of the samples. If there are numerous label overlaps, this makes the interpretation of the ordination difficult. If you were producing this for publication you would need to sort this out to present the reader with a figure that communicates the message effectively.\n\n::: Q.\nWhich elements are positively associated with granite and concrete, particularly after longer periods of leaching?\n:::\n\nAs is typical, overlapping points make interpretation more difficult. There are elegant solutions to this (in terms of labelling) but for now, we’ll split the data and analyse it separately.\n\nWe’ll need more data wrangling to split it efficiently and we’ll use `grepl()`, which identifies a pattern within a character using a regular expression (a.k.a., regex), returning a `TRUE` or `FALSE` for each element in the character vector. Here, we'll filter the dataframe to only include rows where `Treat_day` contains `conc` or `gran`. Remember the 'or' operator `|`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#?grepl\n#cbind(leach_df_wide$Treat_day, grepl(\"conc|gran\", leach_df_wide$Treat_day))\nleach_df_trts <- leach_df_wide |>\n  filter(grepl(\"conc|gran\", Treat_day))\nPCA_trts <- prcomp(leach_df_trts[, -1], scale = TRUE, center = TRUE)\nbiplot(PCA_trts, xlabs = leach_df_trts$Treat_day)\n```\n\n::: {.cell-output-display}\n![PCA of the concrete and granite, illustrating temporal and treatment differences.](P6_machine_learning_files/figure-epub/fig-leach_biplot2-1.png){#fig-leach_biplot2}\n:::\n:::\n\n\n::: Q.\nRepeat the analysis, but set `scale = FALSE`. Which element now seems to dominate the analysis? Explain what you see.\n:::\n\nA PCA is normally reported with the proportion of the variation explained by each of the principal components (and/or the cumulative proportion). If the cumulative proportion for the 1st two components is high, then the 2D (PC1 and PC2) ordination is a good representation of the similarities between samples. In that sense a high cumulative proportion is analogous to a low stress for NMDS.\n\nLet’s have a look at a summary of the principal components.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPCA_leach_summary <- summary(PCA_leach) \n#PCA_leach_summary\n#str(PCA_leach_summary)\nPCA_leach_summary$importance[, 1:4] # extract only PC1-4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            PC1      PC2       PC3       PC4\nStandard deviation     1.988091 1.450775 0.7782675 0.5599535\nProportion of Variance 0.564640 0.300680 0.0865300 0.0447900\nCumulative Proportion  0.564640 0.865320 0.9518500 0.9966400\n```\n\n\n:::\n:::\n\n\nAs you can see, PC1, PC2, and PC3 capture more than 95% of the variance in our data. Adding PC4 brings that up above 99%.\n\nFinally, we can look at factor loadings. This is a measure of how each feature (metal concentration in this case) relates to the principal components. In PCA, the principal components are sequentially ‘less’ influential since they describe increasingly smaller amounts of variation. By centering and standardising the variables, you can assess the relative importance of each in driving the patterns in the ordination. The magnitude is what we're interested in rather than the sign.\n\nIn an object created by `prcomp()`, the loadings are stored as `.$rotation`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ?prcomp\n# str(PCA_leach)\nsignif(PCA_leach$rotation[, 1:5], 3) # factor loadings for PC1-5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      PC1     PC2     PC3    PC4      PC5\nBa -0.367  0.2090  0.6510 -0.619 -0.00405\nFe -0.380 -0.0440 -0.7370 -0.553  0.01420\nMn -0.146  0.6500 -0.0963  0.211  0.67300\nMo -0.430 -0.3360  0.0795  0.283 -0.09810\nRb -0.462 -0.2440  0.0327  0.305  0.21600\nSr -0.495 -0.0788  0.0663  0.168  0.01510\nU  -0.238  0.5940 -0.1110  0.254 -0.70000\n```\n\n\n:::\n:::\n\n\nHere you can see that Sr, Rb, Mo are relatively ‘important’ in driving the multivariate pattern you’ve observed (i.e., high absolute values on PC1) while Mn and U have high values for PC2 (you could say that PC2 accounts for Mn and U).\n\n------------------------------------------------------------------------\n\n## Conclusions\n\nMultivariate analyses are extremely useful in a variety of contexts. The form of analysis is often more qualitative and less inferential compared to univariate analyses we've covered, but this is an active field of research and there are well-developed methods and R packages for more complex analyses (e.g., [`ade4`](https://adeverse.github.io/ade4/).  \n",
    "supporting": [
      "P6_machine_learning_files\\figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}